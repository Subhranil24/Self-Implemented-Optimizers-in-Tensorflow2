{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZoGxquWFTah"
   },
   "source": [
    "# **This implementation executes the results of the MNIST part for the part 2 of the lab. The results of this implementation is compared with the results of the mentioned paper, and interesting findings are discussed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6193,
     "status": "ok",
     "timestamp": 1568243095851,
     "user": {
      "displayName": "Subhranil Bagchi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDr67UoIklBCVjijdnam_x9mznvsmjGw-moT-Mp=s64",
      "userId": "08663459646864926551"
     },
     "user_tz": -330
    },
    "id": "08AjindrJDUE",
    "outputId": "e0023cb1-712b-46e8-ab42-b0538a57cf98"
   },
   "outputs": [],
   "source": [
    "# Run this cell only if tensorflow 2.0 needs to be installed, otherwise please ignore this cell.\n",
    "\n",
    "#!pip install tensorflow-gpu==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ugIgW1KEF5Dk"
   },
   "source": [
    "**The MNIST Dataset is loaded, shuffled and normalized.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDXsdP20Kgd0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Getting the dataset and normalizing.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train/255.0; x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_t8SVw-GWYr"
   },
   "source": [
    "**After normalization of the input data, it is time to define the architecture of the model. In the paper it was mentioned that the architecture consisted of 2 hidden layers of 1000 units each, alongside drops. However, the dropout's probability was not mentioned; thus, for my experiments I have considered the dropout probabilities to be 0.5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDTSjbphLnZx"
   },
   "outputs": [],
   "source": [
    "# Calling necessary modules essential for the implementation of the model.\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# The return function, when called, will return object of the model.\n",
    "def return_Model():\n",
    "  \n",
    "  # Extending the model class.\n",
    "  class Model_MNIST(Model):\n",
    "    def __init__(self):\n",
    "      super(Model_MNIST, self).__init__()\n",
    "      # Define layers.\n",
    "      self.flatten = Flatten()\n",
    "      self.hidden1 = Dense(1000, activation='relu')\n",
    "      self.hidden2 = Dense(1000, activation='relu')\n",
    "      self.dropout = Dropout(0.5)\n",
    "      self.final = Dense(10, activation='softmax')\n",
    "\n",
    "    # Assigning the forward pass.\n",
    "    def call(self, x):\n",
    "      x = self.dropout(x)\n",
    "      x = self.flatten(x)\n",
    "      x = self.hidden1(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.hidden2(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.final(x)\n",
    "      return x\n",
    "  \n",
    "  # Creating the object of the model and returning the same.\n",
    "  model_mnist = Model_MNIST()\n",
    "  return(model_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxZ3_s7jIjcr"
   },
   "source": [
    "**After the defining the network architecture, it is time to create our own optimizers. For making the code more efficient the abtract optimizer class have been extended. The benefit of this method lies with the fact that since the custom optimizers are extended for the abstract class, it becomes much easier to handle the gradients from the call-graph and weight updates. In the paper five different optimizers were used for comparison, which include SGD with Nesterov's Momentum, AdaGrad, AdaDelta, RMSProp and Adam. However, since it was mentioned that we have to show the comparisons between the optimizers which are implemented in part 1 of the lab, AdaDelta has not been taken into consideration. Thus, all the rest four optimizers have been implemented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jum0xsbEHq1Z"
   },
   "outputs": [],
   "source": [
    "# Calling necessary modules for implemnetation of the a proper custom optimizer.\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.training import optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrWFVG70J_Ai"
   },
   "source": [
    "**The first implemntation is SGD with Nesterov's Momentum optimizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzqTl2_4mI-X"
   },
   "outputs": [],
   "source": [
    "# The SGDNesterov Optimizer class have been defined.\n",
    "class MySGDNesterov(optimizer.Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, use_locking=False, name=\"MySGDNesterov\"):\n",
    "      \n",
    "      super(MySGDNesterov, self).__init__(use_locking, name)\n",
    "      \n",
    "      # Defining the class member function's essential for the optimization\n",
    "      self._lr = learning_rate; self._gamma = gamma;\n",
    "      self._lr_t = None; self._gamma_t = None;\n",
    "\n",
    "    # prepare(), create_slots() and resourse_apply_dense() are member functions that are declared in the base class.\n",
    "    # These functions allow efficient gathering of the weights and the grad values from the accumulate of the running device,\n",
    "    # as well has element update implementation.\n",
    "    \n",
    "    # The prepare function prepares class members to be operable for executing in the call-graph.\n",
    "    def _prepare(self):\n",
    "      self._lr_t = ops.convert_to_tensor(self._lr)\n",
    "      self._gamma_t = ops.convert_to_tensor(self._gamma)\n",
    "\n",
    "    # This function creates slots with running variables in the accumulator, i.e., variables other than the weights whose values are going to change\n",
    "    # in each iteration, and initialize their values as zero.\n",
    "    def _create_slots(self, var_list):\n",
    "      for v in var_list:\n",
    "        self._zeros_slot(v, \"mt\", self._name)\n",
    "\n",
    "    # The resource_apply_dense functions computes the updated values of the weights, and updates the same alongside the running variables\n",
    "    # in thr accumulator.\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "      \n",
    "      # Type casting for data members for calculation.\n",
    "      lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "      gamma_t = math_ops.cast(self._gamma_t, var.dtype.base_dtype)\n",
    "      \n",
    "      # The function get_slot gets the current value of the running variable.\n",
    "      mt_old = self.get_slot(var, \"mt\")\n",
    "      mt = mt_old.assign(gamma_t * mt_old + lr_t * grad)\n",
    "      \n",
    "      # Updates the weights' values.\n",
    "      update_val = (-gamma_t * mt_old) + ((1 + gamma_t) * mt)\n",
    "      var_update = state_ops.assign_sub(var, update_val)\n",
    "      \n",
    "      # Updates the new values of the weights and the running variables to the accumulator.\n",
    "      return control_flow_ops.group(*[var_update,mt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Snmidws2PRBb"
   },
   "source": [
    "**Implementation of Adagrad optimizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9jE8gbq2neF0"
   },
   "outputs": [],
   "source": [
    "class MyAdagrad(optimizer.Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8, use_locking=False, name=\"MyAdagrad\"):\n",
    "      \n",
    "      super(MyAdagrad, self).__init__(use_locking, name)\n",
    "      \n",
    "      self._lr = learning_rate; self._epsilon = epsilon\n",
    "      self._lr_t = None; self._epsilon_t = None\n",
    "      \n",
    "    def _prepare(self):\n",
    "      self._lr_t = ops.convert_to_tensor(self._lr)\n",
    "      self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "      for v in var_list:\n",
    "        self._zeros_slot(v, \"vt\", self._name)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "\n",
    "      lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "      epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "\n",
    "      # Getting the value of the adaptive velocity.\n",
    "      vt = self.get_slot(var, \"vt\")\n",
    "      vt_new = vt.assign(vt + tf.square(grad))\n",
    "\n",
    "      # Final weight update equation and updating the weights.\n",
    "      update_val = (lr_t * grad) / tf.sqrt(vt_new + epsilon_t)\n",
    "      var_update = state_ops.assign_sub(var, update_val)\n",
    "\n",
    "      return control_flow_ops.group(*[var_update,vt_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ob8yjWecQWXO"
   },
   "source": [
    "**Implementation of RMSProp optimizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfjMMVxUr_IP"
   },
   "outputs": [],
   "source": [
    "class MyRMSProp(optimizer.Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, beta = 0.95, epsilon=1e-4, use_locking=False, name=\"MyRMSProp\"):\n",
    "      \n",
    "      super(MyRMSProp, self).__init__(use_locking, name)\n",
    "\n",
    "      self._lr = learning_rate; self._beta = beta; self._epsilon = epsilon\n",
    "      self._lr_t = None; self._beta_t = None; self._epsilon_t = None\n",
    "      \n",
    "    def _prepare(self):\n",
    "      self._lr_t = ops.convert_to_tensor(self._lr)\n",
    "      self._beta_t = ops.convert_to_tensor(self._beta)\n",
    "      self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "      for v in var_list:\n",
    "        self._zeros_slot(v, \"vt\", self._name)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "\n",
    "      lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "      beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n",
    "      epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "\n",
    "      # Getting the adaptive velocity and also taking the beta factor into account.\n",
    "      vt = self.get_slot(var, \"vt\")\n",
    "      vt_new = vt.assign((beta_t * vt) + ((1 - beta_t) * tf.square(grad)))\n",
    "\n",
    "      # Final weight update equation and updating the weights.\n",
    "      update_val = (lr_t * grad) / tf.sqrt(vt_new + epsilon_t)\n",
    "\n",
    "      var_update = state_ops.assign_sub(var, update_val)\n",
    "      return control_flow_ops.group(*[var_update,vt_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qm_ZGlhDRQAX"
   },
   "source": [
    "**Implementation of Adam optimizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tkg1YavCNVhO"
   },
   "outputs": [],
   "source": [
    "class MyAdam(optimizer.Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, beta = 0.999, epsilon=1e-8, use_locking=False, name=\"MyAdam\"):\n",
    "      \n",
    "      super(MyAdam, self).__init__(use_locking, name)\n",
    "      \n",
    "      self._lr = learning_rate; self._gamma = gamma; self._beta = beta; self._epsilon = epsilon\n",
    "      self._lr_t = None; self._gamma_t = None; self._beta_t = None; self._epsilon_t = None\n",
    "      \n",
    "    def _prepare(self):\n",
    "      self._lr_t = ops.convert_to_tensor(self._lr)\n",
    "      self._gamma_t = ops.convert_to_tensor(self._gamma)\n",
    "      self._beta_t = ops.convert_to_tensor(self._beta)\n",
    "      self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "      for v in var_list:\n",
    "        self._zeros_slot(v, \"mt\", self._name)\n",
    "        self._zeros_slot(v, \"vt\", self._name)\n",
    "        self._zeros_slot(v, \"t\", self._name)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "\n",
    "      lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "      gamma_t = math_ops.cast(self._gamma_t, var.dtype.base_dtype)\n",
    "      beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n",
    "      epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "\n",
    "      # Getting the updated t value required for correction.\n",
    "      t = self.get_slot(var, \"t\")\n",
    "      t_new = t + 1\n",
    "      \n",
    "      # Getting the moment value and calculating the corrected updated value.\n",
    "      mt = self.get_slot(var, \"mt\")\n",
    "      mt_new = mt.assign((gamma_t * mt) + ((1 - gamma_t) * grad))\n",
    "      corr_mt_new = mt_new / (1-tf.pow(gamma_t,t_new))\n",
    "\n",
    "      # Getting the velocity value and calculating the corrected updated value.\n",
    "      vt = self.get_slot(var, \"vt\")\n",
    "      vt_new = vt.assign((beta_t * vt) + ((1 - beta_t) * tf.square(grad)))\n",
    "      corr_vt_new = vt_new / (1-tf.pow(beta_t,t_new))\n",
    "\n",
    "      # Updating weights based on the final update equation.\n",
    "      update_val = (lr_t * corr_mt_new) / tf.sqrt(corr_vt_new + epsilon_t)\n",
    "\n",
    "      var_update = state_ops.assign_sub(var, update_val)\n",
    "      return control_flow_ops.group(*[var_update,mt_new,vt_new,t_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2uQDbMJSEzR"
   },
   "source": [
    "**After defining the optimizers I have defined the function calculate_grad(), upon which the final call-graphs will be created. The creation of the call-graph makes the execution much much faster. The purpose of the function is to calculate the gradient, and inside the function the optimizer is called to backpropagate the error and update the weights. This function is wrapped with 'tf.function()', so that, on different instance of the call graph.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03pfmlYnNb0l"
   },
   "outputs": [],
   "source": [
    "def calculate_grad(model,loss_criterion, x, y, optimizerObj, global_step):\n",
    "  # tf.GradientTape() calculates the gradient of the los with respect to the weights.\n",
    "  with tf.GradientTape() as tape:\n",
    "    # For the prediction, loss is being calculated.\n",
    "    predictions = model(x)\n",
    "    loss_value = loss_criterion(y,predictions)\n",
    "  # Gradient for the loss is also calculated.\n",
    "  gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "  # The optimizer is called to optimize the weights with respect to the gradients.\n",
    "  optimizerObj.apply_gradients(zip(gradients, model.trainable_variables),global_step)\n",
    "  return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0CskLdfRUXeZ"
   },
   "source": [
    "**Finally, we define the train_model(), this function is called each time when an optimizer's performance needs to be checked. This functions trains the model based on the chosen optimizer, returns the losses and accuracies per epoch for the training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2dqxV3knOe_B"
   },
   "outputs": [],
   "source": [
    "def train_model(model,optimizerObj,train_dataset,loss_criterion,global_step,n_epochs,batch_size):\n",
    "\n",
    "  loss_per_epoch = []\n",
    "  accuracy_per_epoch = []\n",
    "\n",
    "  # Wrapping the calculate_grad() to generate the call-graph.\n",
    "  graph_calculate_grad = tf.function(calculate_grad)\n",
    "\n",
    "  # Computation for each epoch.\n",
    "  for epoch in range(n_epochs):\n",
    "    \n",
    "    # These two are helper functions to calcuate the running mean of loss, and accuracy.\n",
    "    epoch_loss = tf.keras.metrics.Mean(name='epoch_loss')\n",
    "    epoch_accuracy =  tf.keras.metrics.SparseCategoricalAccuracy(name='epoch_accuracy')\n",
    "\n",
    "    # Computation for each mini-batch.\n",
    "    for x, y in train_dataset:\n",
    "      \n",
    "      # The call-graph for is called. The loss is calculated, the gradients of losses with respect to the weights\n",
    "      # are generated, and the optimizer optimizes the weights.\n",
    "      #loss_amount = calculate_grad(model,loss_criterion,x,y,optimizerObj,global_step)\n",
    "      loss_amount = graph_calculate_grad(model,loss_criterion,x,y,optimizerObj,global_step)\n",
    "      # Prediction for accuracy.\n",
    "      predictions = model(x)\n",
    "\n",
    "      # Calculate the loss and accuracy per epoch.\n",
    "      epoch_loss(loss_amount)\n",
    "      epoch_accuracy(y,predictions)\n",
    "\n",
    "    loss_per_epoch.append(epoch_loss.result())\n",
    "    accuracy_per_epoch.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"Epoch {:03d}: Loss: {:.5f}, Accuracy: {:.3%}\".format(epoch+1, epoch_loss.result(), epoch_accuracy.result()))\n",
    "\n",
    "  # The losses and accuracies are returned.\n",
    "  return(loss_per_epoch,accuracy_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSnAmeVLWAwL"
   },
   "source": [
    "**Since, neural networks depends on multiple random initializations, a single trial is not expressive enough for estimating the performance. Thus, I have conducted a set of 5 trials, and losses & accuracies are averages over all the trials. These values were used to plot the graphs.**\n",
    "\n",
    "**Note: Plot for accuries were not the part of the results in the paper, however, it still shown as a part of my experiment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "0NByW4_AYyw2",
    "outputId": "69831449-f33e-49f3-ea1f-0c6b731b2c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n",
      "SGD Nesterov\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5268>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5268>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b8695278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 1.10826, Accuracy: 76.027%\n",
      "Epoch 002: Loss: 0.44239, Accuracy: 88.607%\n",
      "Epoch 003: Loss: 0.35252, Accuracy: 90.377%\n",
      "Epoch 004: Loss: 0.31194, Accuracy: 91.328%\n",
      "Epoch 005: Loss: 0.28579, Accuracy: 92.020%\n",
      "Epoch 006: Loss: 0.26620, Accuracy: 92.533%\n",
      "Epoch 007: Loss: 0.25029, Accuracy: 92.962%\n",
      "Epoch 008: Loss: 0.23674, Accuracy: 93.437%\n",
      "Epoch 009: Loss: 0.22486, Accuracy: 93.760%\n",
      "Epoch 010: Loss: 0.21427, Accuracy: 94.067%\n",
      "Epoch 011: Loss: 0.20468, Accuracy: 94.338%\n",
      "Epoch 012: Loss: 0.19593, Accuracy: 94.573%\n",
      "Epoch 013: Loss: 0.18788, Accuracy: 94.747%\n",
      "Epoch 014: Loss: 0.18043, Accuracy: 94.973%\n",
      "Epoch 015: Loss: 0.17353, Accuracy: 95.160%\n",
      "Epoch 016: Loss: 0.16709, Accuracy: 95.345%\n",
      "Epoch 017: Loss: 0.16106, Accuracy: 95.510%\n",
      "Epoch 018: Loss: 0.15539, Accuracy: 95.673%\n",
      "Epoch 019: Loss: 0.15006, Accuracy: 95.818%\n",
      "Epoch 020: Loss: 0.14503, Accuracy: 95.972%\n",
      "Epoch 021: Loss: 0.14029, Accuracy: 96.097%\n",
      "Epoch 022: Loss: 0.13580, Accuracy: 96.245%\n",
      "Epoch 023: Loss: 0.13155, Accuracy: 96.362%\n",
      "Epoch 024: Loss: 0.12752, Accuracy: 96.477%\n",
      "Epoch 025: Loss: 0.12369, Accuracy: 96.578%\n",
      "Epoch 026: Loss: 0.12005, Accuracy: 96.667%\n",
      "Epoch 027: Loss: 0.11659, Accuracy: 96.787%\n",
      "Epoch 028: Loss: 0.11328, Accuracy: 96.895%\n",
      "Epoch 029: Loss: 0.11013, Accuracy: 97.005%\n",
      "Epoch 030: Loss: 0.10712, Accuracy: 97.088%\n",
      "Epoch 031: Loss: 0.10424, Accuracy: 97.165%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032: Loss: 0.10148, Accuracy: 97.233%\n",
      "Epoch 033: Loss: 0.09883, Accuracy: 97.297%\n",
      "Epoch 034: Loss: 0.09628, Accuracy: 97.395%\n",
      "Epoch 035: Loss: 0.09384, Accuracy: 97.482%\n",
      "Epoch 036: Loss: 0.09149, Accuracy: 97.547%\n",
      "Epoch 037: Loss: 0.08923, Accuracy: 97.625%\n",
      "Epoch 038: Loss: 0.08705, Accuracy: 97.687%\n",
      "Epoch 039: Loss: 0.08495, Accuracy: 97.743%\n",
      "Epoch 040: Loss: 0.08293, Accuracy: 97.788%\n",
      "Epoch 041: Loss: 0.08097, Accuracy: 97.857%\n",
      "Epoch 042: Loss: 0.07908, Accuracy: 97.922%\n",
      "Epoch 043: Loss: 0.07726, Accuracy: 97.985%\n",
      "Epoch 044: Loss: 0.07549, Accuracy: 98.018%\n",
      "Epoch 045: Loss: 0.07378, Accuracy: 98.070%\n",
      "Epoch 046: Loss: 0.07213, Accuracy: 98.123%\n",
      "Epoch 047: Loss: 0.07053, Accuracy: 98.175%\n",
      "Epoch 048: Loss: 0.06898, Accuracy: 98.225%\n",
      "Epoch 049: Loss: 0.06748, Accuracy: 98.252%\n",
      "Epoch 050: Loss: 0.06602, Accuracy: 98.297%\n",
      "Epoch 051: Loss: 0.06460, Accuracy: 98.340%\n",
      "Epoch 052: Loss: 0.06323, Accuracy: 98.380%\n",
      "Epoch 053: Loss: 0.06190, Accuracy: 98.415%\n",
      "Epoch 054: Loss: 0.06061, Accuracy: 98.457%\n",
      "Epoch 055: Loss: 0.05935, Accuracy: 98.485%\n",
      "Epoch 056: Loss: 0.05813, Accuracy: 98.555%\n",
      "Epoch 057: Loss: 0.05694, Accuracy: 98.588%\n",
      "Epoch 058: Loss: 0.05578, Accuracy: 98.618%\n",
      "Epoch 059: Loss: 0.05466, Accuracy: 98.648%\n",
      "Epoch 060: Loss: 0.05357, Accuracy: 98.658%\n",
      "Epoch 061: Loss: 0.05250, Accuracy: 98.680%\n",
      "Epoch 062: Loss: 0.05146, Accuracy: 98.703%\n",
      "Epoch 063: Loss: 0.05046, Accuracy: 98.740%\n",
      "Epoch 064: Loss: 0.04947, Accuracy: 98.773%\n",
      "Epoch 065: Loss: 0.04851, Accuracy: 98.803%\n",
      "Epoch 066: Loss: 0.04758, Accuracy: 98.837%\n",
      "Epoch 067: Loss: 0.04667, Accuracy: 98.860%\n",
      "Epoch 068: Loss: 0.04578, Accuracy: 98.887%\n",
      "Epoch 069: Loss: 0.04492, Accuracy: 98.915%\n",
      "Epoch 070: Loss: 0.04407, Accuracy: 98.948%\n",
      "Epoch 071: Loss: 0.04324, Accuracy: 98.972%\n",
      "Epoch 072: Loss: 0.04244, Accuracy: 98.992%\n",
      "Epoch 073: Loss: 0.04165, Accuracy: 99.015%\n",
      "Epoch 074: Loss: 0.04088, Accuracy: 99.038%\n",
      "Epoch 075: Loss: 0.04013, Accuracy: 99.073%\n",
      "Epoch 076: Loss: 0.03940, Accuracy: 99.087%\n",
      "Epoch 077: Loss: 0.03868, Accuracy: 99.107%\n",
      "Epoch 078: Loss: 0.03798, Accuracy: 99.118%\n",
      "Epoch 079: Loss: 0.03729, Accuracy: 99.137%\n",
      "Epoch 080: Loss: 0.03662, Accuracy: 99.160%\n",
      "Epoch 081: Loss: 0.03597, Accuracy: 99.188%\n",
      "Epoch 082: Loss: 0.03533, Accuracy: 99.218%\n",
      "Epoch 083: Loss: 0.03470, Accuracy: 99.250%\n",
      "Epoch 084: Loss: 0.03409, Accuracy: 99.258%\n",
      "Epoch 085: Loss: 0.03349, Accuracy: 99.278%\n",
      "Epoch 086: Loss: 0.03290, Accuracy: 99.287%\n",
      "Epoch 087: Loss: 0.03233, Accuracy: 99.305%\n",
      "Epoch 088: Loss: 0.03176, Accuracy: 99.320%\n",
      "Epoch 089: Loss: 0.03121, Accuracy: 99.343%\n",
      "Epoch 090: Loss: 0.03067, Accuracy: 99.355%\n",
      "Epoch 091: Loss: 0.03015, Accuracy: 99.372%\n",
      "Epoch 092: Loss: 0.02963, Accuracy: 99.382%\n",
      "Epoch 093: Loss: 0.02912, Accuracy: 99.393%\n",
      "Epoch 094: Loss: 0.02863, Accuracy: 99.417%\n",
      "Epoch 095: Loss: 0.02814, Accuracy: 99.432%\n",
      "Epoch 096: Loss: 0.02767, Accuracy: 99.455%\n",
      "Epoch 097: Loss: 0.02720, Accuracy: 99.472%\n",
      "Epoch 098: Loss: 0.02674, Accuracy: 99.480%\n",
      "Epoch 099: Loss: 0.02630, Accuracy: 99.492%\n",
      "Epoch 100: Loss: 0.02586, Accuracy: 99.498%\n",
      "Epoch 101: Loss: 0.02543, Accuracy: 99.508%\n",
      "Epoch 102: Loss: 0.02501, Accuracy: 99.527%\n",
      "Epoch 103: Loss: 0.02459, Accuracy: 99.545%\n",
      "Epoch 104: Loss: 0.02419, Accuracy: 99.552%\n",
      "Epoch 105: Loss: 0.02380, Accuracy: 99.562%\n",
      "Epoch 106: Loss: 0.02341, Accuracy: 99.578%\n",
      "Epoch 107: Loss: 0.02303, Accuracy: 99.585%\n",
      "Epoch 108: Loss: 0.02266, Accuracy: 99.597%\n",
      "Epoch 109: Loss: 0.02229, Accuracy: 99.612%\n",
      "Epoch 110: Loss: 0.02194, Accuracy: 99.618%\n",
      "Epoch 111: Loss: 0.02159, Accuracy: 99.628%\n",
      "Epoch 112: Loss: 0.02124, Accuracy: 99.643%\n",
      "Epoch 113: Loss: 0.02091, Accuracy: 99.653%\n",
      "Epoch 114: Loss: 0.02058, Accuracy: 99.657%\n",
      "Epoch 115: Loss: 0.02025, Accuracy: 99.672%\n",
      "Epoch 116: Loss: 0.01994, Accuracy: 99.683%\n",
      "Epoch 117: Loss: 0.01963, Accuracy: 99.695%\n",
      "Epoch 118: Loss: 0.01932, Accuracy: 99.705%\n",
      "Epoch 119: Loss: 0.01903, Accuracy: 99.710%\n",
      "Epoch 120: Loss: 0.01873, Accuracy: 99.712%\n",
      "Epoch 121: Loss: 0.01845, Accuracy: 99.717%\n",
      "Epoch 122: Loss: 0.01816, Accuracy: 99.722%\n",
      "Epoch 123: Loss: 0.01789, Accuracy: 99.727%\n",
      "Epoch 124: Loss: 0.01762, Accuracy: 99.747%\n",
      "Epoch 125: Loss: 0.01736, Accuracy: 99.755%\n",
      "Epoch 126: Loss: 0.01710, Accuracy: 99.757%\n",
      "Epoch 127: Loss: 0.01684, Accuracy: 99.765%\n",
      "Epoch 128: Loss: 0.01659, Accuracy: 99.775%\n",
      "Epoch 129: Loss: 0.01635, Accuracy: 99.778%\n",
      "Epoch 130: Loss: 0.01611, Accuracy: 99.787%\n",
      "Epoch 131: Loss: 0.01587, Accuracy: 99.797%\n",
      "Epoch 132: Loss: 0.01564, Accuracy: 99.803%\n",
      "Epoch 133: Loss: 0.01541, Accuracy: 99.807%\n",
      "Epoch 134: Loss: 0.01519, Accuracy: 99.815%\n",
      "Epoch 135: Loss: 0.01497, Accuracy: 99.822%\n",
      "Epoch 136: Loss: 0.01476, Accuracy: 99.828%\n",
      "Epoch 137: Loss: 0.01455, Accuracy: 99.835%\n",
      "Epoch 138: Loss: 0.01434, Accuracy: 99.837%\n",
      "Epoch 139: Loss: 0.01414, Accuracy: 99.843%\n",
      "Epoch 140: Loss: 0.01394, Accuracy: 99.852%\n",
      "Epoch 141: Loss: 0.01375, Accuracy: 99.855%\n",
      "Epoch 142: Loss: 0.01356, Accuracy: 99.858%\n",
      "Epoch 143: Loss: 0.01337, Accuracy: 99.862%\n",
      "Epoch 144: Loss: 0.01319, Accuracy: 99.863%\n",
      "Epoch 145: Loss: 0.01301, Accuracy: 99.863%\n",
      "Epoch 146: Loss: 0.01283, Accuracy: 99.870%\n",
      "Epoch 147: Loss: 0.01266, Accuracy: 99.875%\n",
      "Epoch 148: Loss: 0.01249, Accuracy: 99.883%\n",
      "Epoch 149: Loss: 0.01232, Accuracy: 99.888%\n",
      "Epoch 150: Loss: 0.01215, Accuracy: 99.892%\n",
      "Epoch 151: Loss: 0.01199, Accuracy: 99.895%\n",
      "Epoch 152: Loss: 0.01183, Accuracy: 99.895%\n",
      "Epoch 153: Loss: 0.01168, Accuracy: 99.900%\n",
      "Epoch 154: Loss: 0.01152, Accuracy: 99.903%\n",
      "Epoch 155: Loss: 0.01137, Accuracy: 99.908%\n",
      "Epoch 156: Loss: 0.01122, Accuracy: 99.917%\n",
      "Epoch 157: Loss: 0.01108, Accuracy: 99.917%\n",
      "Epoch 158: Loss: 0.01094, Accuracy: 99.917%\n",
      "Epoch 159: Loss: 0.01080, Accuracy: 99.917%\n",
      "Epoch 160: Loss: 0.01066, Accuracy: 99.920%\n",
      "Epoch 161: Loss: 0.01052, Accuracy: 99.920%\n",
      "Epoch 162: Loss: 0.01039, Accuracy: 99.920%\n",
      "Epoch 163: Loss: 0.01026, Accuracy: 99.922%\n",
      "Epoch 164: Loss: 0.01013, Accuracy: 99.923%\n",
      "Epoch 165: Loss: 0.01000, Accuracy: 99.923%\n",
      "Epoch 166: Loss: 0.00988, Accuracy: 99.925%\n",
      "Epoch 167: Loss: 0.00976, Accuracy: 99.927%\n",
      "Epoch 168: Loss: 0.00964, Accuracy: 99.928%\n",
      "Epoch 169: Loss: 0.00952, Accuracy: 99.933%\n",
      "Epoch 170: Loss: 0.00940, Accuracy: 99.933%\n",
      "Epoch 171: Loss: 0.00929, Accuracy: 99.933%\n",
      "Epoch 172: Loss: 0.00918, Accuracy: 99.933%\n",
      "Epoch 173: Loss: 0.00907, Accuracy: 99.935%\n",
      "Epoch 174: Loss: 0.00896, Accuracy: 99.937%\n",
      "Epoch 175: Loss: 0.00885, Accuracy: 99.940%\n",
      "Epoch 176: Loss: 0.00875, Accuracy: 99.942%\n",
      "Epoch 177: Loss: 0.00864, Accuracy: 99.942%\n",
      "Epoch 178: Loss: 0.00854, Accuracy: 99.943%\n",
      "Epoch 179: Loss: 0.00844, Accuracy: 99.945%\n",
      "Epoch 180: Loss: 0.00834, Accuracy: 99.947%\n",
      "Epoch 181: Loss: 0.00825, Accuracy: 99.947%\n",
      "Epoch 182: Loss: 0.00815, Accuracy: 99.948%\n",
      "Epoch 183: Loss: 0.00806, Accuracy: 99.950%\n",
      "Epoch 184: Loss: 0.00797, Accuracy: 99.950%\n",
      "Epoch 185: Loss: 0.00788, Accuracy: 99.955%\n",
      "Epoch 186: Loss: 0.00779, Accuracy: 99.957%\n",
      "Epoch 187: Loss: 0.00770, Accuracy: 99.957%\n",
      "Epoch 188: Loss: 0.00761, Accuracy: 99.958%\n",
      "Epoch 189: Loss: 0.00753, Accuracy: 99.960%\n",
      "Epoch 190: Loss: 0.00744, Accuracy: 99.960%\n",
      "Epoch 191: Loss: 0.00736, Accuracy: 99.960%\n",
      "Epoch 192: Loss: 0.00728, Accuracy: 99.962%\n",
      "Epoch 193: Loss: 0.00720, Accuracy: 99.962%\n",
      "Epoch 194: Loss: 0.00712, Accuracy: 99.963%\n",
      "Epoch 195: Loss: 0.00705, Accuracy: 99.963%\n",
      "Epoch 196: Loss: 0.00697, Accuracy: 99.963%\n",
      "Epoch 197: Loss: 0.00690, Accuracy: 99.967%\n",
      "Epoch 198: Loss: 0.00682, Accuracy: 99.967%\n",
      "Epoch 199: Loss: 0.00675, Accuracy: 99.967%\n",
      "Epoch 200: Loss: 0.00668, Accuracy: 99.968%\n",
      "Adagrad\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9b70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9b70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.29579, Accuracy: 94.088%\n",
      "Epoch 002: Loss: 0.17606, Accuracy: 95.822%\n",
      "Epoch 003: Loss: 0.14707, Accuracy: 96.468%\n",
      "Epoch 004: Loss: 0.12960, Accuracy: 96.913%\n",
      "Epoch 005: Loss: 0.11724, Accuracy: 97.240%\n",
      "Epoch 006: Loss: 0.10779, Accuracy: 97.478%\n",
      "Epoch 007: Loss: 0.10018, Accuracy: 97.645%\n",
      "Epoch 008: Loss: 0.09385, Accuracy: 97.803%\n",
      "Epoch 009: Loss: 0.08845, Accuracy: 97.938%\n",
      "Epoch 010: Loss: 0.08376, Accuracy: 98.042%\n",
      "Epoch 011: Loss: 0.07963, Accuracy: 98.168%\n",
      "Epoch 012: Loss: 0.07594, Accuracy: 98.235%\n",
      "Epoch 013: Loss: 0.07263, Accuracy: 98.312%\n",
      "Epoch 014: Loss: 0.06964, Accuracy: 98.380%\n",
      "Epoch 015: Loss: 0.06691, Accuracy: 98.447%\n",
      "Epoch 016: Loss: 0.06440, Accuracy: 98.513%\n",
      "Epoch 017: Loss: 0.06208, Accuracy: 98.580%\n",
      "Epoch 018: Loss: 0.05993, Accuracy: 98.648%\n",
      "Epoch 019: Loss: 0.05793, Accuracy: 98.703%\n",
      "Epoch 020: Loss: 0.05607, Accuracy: 98.730%\n",
      "Epoch 021: Loss: 0.05433, Accuracy: 98.778%\n",
      "Epoch 022: Loss: 0.05269, Accuracy: 98.822%\n",
      "Epoch 023: Loss: 0.05115, Accuracy: 98.873%\n",
      "Epoch 024: Loss: 0.04969, Accuracy: 98.902%\n",
      "Epoch 025: Loss: 0.04831, Accuracy: 98.945%\n",
      "Epoch 026: Loss: 0.04701, Accuracy: 98.980%\n",
      "Epoch 027: Loss: 0.04577, Accuracy: 99.007%\n",
      "Epoch 028: Loss: 0.04458, Accuracy: 99.050%\n",
      "Epoch 029: Loss: 0.04346, Accuracy: 99.095%\n",
      "Epoch 030: Loss: 0.04238, Accuracy: 99.122%\n",
      "Epoch 031: Loss: 0.04135, Accuracy: 99.143%\n",
      "Epoch 032: Loss: 0.04037, Accuracy: 99.177%\n",
      "Epoch 033: Loss: 0.03943, Accuracy: 99.220%\n",
      "Epoch 034: Loss: 0.03853, Accuracy: 99.238%\n",
      "Epoch 035: Loss: 0.03766, Accuracy: 99.258%\n",
      "Epoch 036: Loss: 0.03683, Accuracy: 99.285%\n",
      "Epoch 037: Loss: 0.03602, Accuracy: 99.303%\n",
      "Epoch 038: Loss: 0.03525, Accuracy: 99.328%\n",
      "Epoch 039: Loss: 0.03450, Accuracy: 99.350%\n",
      "Epoch 040: Loss: 0.03378, Accuracy: 99.373%\n",
      "Epoch 041: Loss: 0.03309, Accuracy: 99.395%\n",
      "Epoch 042: Loss: 0.03241, Accuracy: 99.408%\n",
      "Epoch 043: Loss: 0.03176, Accuracy: 99.425%\n",
      "Epoch 044: Loss: 0.03113, Accuracy: 99.437%\n",
      "Epoch 045: Loss: 0.03053, Accuracy: 99.448%\n",
      "Epoch 046: Loss: 0.02993, Accuracy: 99.460%\n",
      "Epoch 047: Loss: 0.02936, Accuracy: 99.475%\n",
      "Epoch 048: Loss: 0.02881, Accuracy: 99.488%\n",
      "Epoch 049: Loss: 0.02827, Accuracy: 99.503%\n",
      "Epoch 050: Loss: 0.02775, Accuracy: 99.520%\n",
      "Epoch 051: Loss: 0.02725, Accuracy: 99.542%\n",
      "Epoch 052: Loss: 0.02675, Accuracy: 99.557%\n",
      "Epoch 053: Loss: 0.02628, Accuracy: 99.575%\n",
      "Epoch 054: Loss: 0.02581, Accuracy: 99.577%\n",
      "Epoch 055: Loss: 0.02536, Accuracy: 99.593%\n",
      "Epoch 056: Loss: 0.02492, Accuracy: 99.613%\n",
      "Epoch 057: Loss: 0.02449, Accuracy: 99.630%\n",
      "Epoch 058: Loss: 0.02407, Accuracy: 99.637%\n",
      "Epoch 059: Loss: 0.02367, Accuracy: 99.647%\n",
      "Epoch 060: Loss: 0.02327, Accuracy: 99.652%\n",
      "Epoch 061: Loss: 0.02289, Accuracy: 99.665%\n",
      "Epoch 062: Loss: 0.02251, Accuracy: 99.682%\n",
      "Epoch 063: Loss: 0.02215, Accuracy: 99.695%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064: Loss: 0.02179, Accuracy: 99.700%\n",
      "Epoch 065: Loss: 0.02144, Accuracy: 99.707%\n",
      "Epoch 066: Loss: 0.02110, Accuracy: 99.715%\n",
      "Epoch 067: Loss: 0.02077, Accuracy: 99.717%\n",
      "Epoch 068: Loss: 0.02045, Accuracy: 99.722%\n",
      "Epoch 069: Loss: 0.02013, Accuracy: 99.727%\n",
      "Epoch 070: Loss: 0.01982, Accuracy: 99.733%\n",
      "Epoch 071: Loss: 0.01952, Accuracy: 99.742%\n",
      "Epoch 072: Loss: 0.01922, Accuracy: 99.745%\n",
      "Epoch 073: Loss: 0.01894, Accuracy: 99.750%\n",
      "Epoch 074: Loss: 0.01865, Accuracy: 99.757%\n",
      "Epoch 075: Loss: 0.01838, Accuracy: 99.762%\n",
      "Epoch 076: Loss: 0.01811, Accuracy: 99.770%\n",
      "Epoch 077: Loss: 0.01785, Accuracy: 99.773%\n",
      "Epoch 078: Loss: 0.01759, Accuracy: 99.782%\n",
      "Epoch 079: Loss: 0.01734, Accuracy: 99.793%\n",
      "Epoch 080: Loss: 0.01709, Accuracy: 99.803%\n",
      "Epoch 081: Loss: 0.01685, Accuracy: 99.808%\n",
      "Epoch 082: Loss: 0.01662, Accuracy: 99.812%\n",
      "Epoch 083: Loss: 0.01638, Accuracy: 99.817%\n",
      "Epoch 084: Loss: 0.01616, Accuracy: 99.820%\n",
      "Epoch 085: Loss: 0.01594, Accuracy: 99.827%\n",
      "Epoch 086: Loss: 0.01572, Accuracy: 99.830%\n",
      "Epoch 087: Loss: 0.01550, Accuracy: 99.835%\n",
      "Epoch 088: Loss: 0.01530, Accuracy: 99.842%\n",
      "Epoch 089: Loss: 0.01509, Accuracy: 99.845%\n",
      "Epoch 090: Loss: 0.01489, Accuracy: 99.852%\n",
      "Epoch 091: Loss: 0.01470, Accuracy: 99.855%\n",
      "Epoch 092: Loss: 0.01450, Accuracy: 99.860%\n",
      "Epoch 093: Loss: 0.01431, Accuracy: 99.863%\n",
      "Epoch 094: Loss: 0.01413, Accuracy: 99.870%\n",
      "Epoch 095: Loss: 0.01395, Accuracy: 99.875%\n",
      "Epoch 096: Loss: 0.01377, Accuracy: 99.882%\n",
      "Epoch 097: Loss: 0.01359, Accuracy: 99.883%\n",
      "Epoch 098: Loss: 0.01342, Accuracy: 99.885%\n",
      "Epoch 099: Loss: 0.01325, Accuracy: 99.885%\n",
      "Epoch 100: Loss: 0.01309, Accuracy: 99.885%\n",
      "Epoch 101: Loss: 0.01293, Accuracy: 99.890%\n",
      "Epoch 102: Loss: 0.01277, Accuracy: 99.892%\n",
      "Epoch 103: Loss: 0.01261, Accuracy: 99.895%\n",
      "Epoch 104: Loss: 0.01246, Accuracy: 99.895%\n",
      "Epoch 105: Loss: 0.01231, Accuracy: 99.897%\n",
      "Epoch 106: Loss: 0.01216, Accuracy: 99.898%\n",
      "Epoch 107: Loss: 0.01201, Accuracy: 99.900%\n",
      "Epoch 108: Loss: 0.01187, Accuracy: 99.902%\n",
      "Epoch 109: Loss: 0.01173, Accuracy: 99.905%\n",
      "Epoch 110: Loss: 0.01159, Accuracy: 99.908%\n",
      "Epoch 111: Loss: 0.01146, Accuracy: 99.908%\n",
      "Epoch 112: Loss: 0.01133, Accuracy: 99.908%\n",
      "Epoch 113: Loss: 0.01120, Accuracy: 99.908%\n",
      "Epoch 114: Loss: 0.01107, Accuracy: 99.912%\n",
      "Epoch 115: Loss: 0.01094, Accuracy: 99.917%\n",
      "Epoch 116: Loss: 0.01082, Accuracy: 99.918%\n",
      "Epoch 117: Loss: 0.01070, Accuracy: 99.918%\n",
      "Epoch 118: Loss: 0.01058, Accuracy: 99.922%\n",
      "Epoch 119: Loss: 0.01046, Accuracy: 99.923%\n",
      "Epoch 120: Loss: 0.01034, Accuracy: 99.925%\n",
      "Epoch 121: Loss: 0.01023, Accuracy: 99.927%\n",
      "Epoch 122: Loss: 0.01012, Accuracy: 99.928%\n",
      "Epoch 123: Loss: 0.01001, Accuracy: 99.932%\n",
      "Epoch 124: Loss: 0.00990, Accuracy: 99.932%\n",
      "Epoch 125: Loss: 0.00979, Accuracy: 99.932%\n",
      "Epoch 126: Loss: 0.00969, Accuracy: 99.938%\n",
      "Epoch 127: Loss: 0.00958, Accuracy: 99.938%\n",
      "Epoch 128: Loss: 0.00948, Accuracy: 99.938%\n",
      "Epoch 129: Loss: 0.00938, Accuracy: 99.938%\n",
      "Epoch 130: Loss: 0.00928, Accuracy: 99.938%\n",
      "Epoch 131: Loss: 0.00918, Accuracy: 99.938%\n",
      "Epoch 132: Loss: 0.00909, Accuracy: 99.938%\n",
      "Epoch 133: Loss: 0.00899, Accuracy: 99.942%\n",
      "Epoch 134: Loss: 0.00890, Accuracy: 99.943%\n",
      "Epoch 135: Loss: 0.00881, Accuracy: 99.945%\n",
      "Epoch 136: Loss: 0.00872, Accuracy: 99.945%\n",
      "Epoch 137: Loss: 0.00863, Accuracy: 99.947%\n",
      "Epoch 138: Loss: 0.00855, Accuracy: 99.950%\n",
      "Epoch 139: Loss: 0.00846, Accuracy: 99.952%\n",
      "Epoch 140: Loss: 0.00838, Accuracy: 99.952%\n",
      "Epoch 141: Loss: 0.00829, Accuracy: 99.952%\n",
      "Epoch 142: Loss: 0.00821, Accuracy: 99.952%\n",
      "Epoch 143: Loss: 0.00813, Accuracy: 99.952%\n",
      "Epoch 144: Loss: 0.00805, Accuracy: 99.953%\n",
      "Epoch 145: Loss: 0.00797, Accuracy: 99.955%\n",
      "Epoch 146: Loss: 0.00790, Accuracy: 99.955%\n",
      "Epoch 147: Loss: 0.00782, Accuracy: 99.957%\n",
      "Epoch 148: Loss: 0.00775, Accuracy: 99.957%\n",
      "Epoch 149: Loss: 0.00767, Accuracy: 99.960%\n",
      "Epoch 150: Loss: 0.00760, Accuracy: 99.965%\n",
      "Epoch 151: Loss: 0.00753, Accuracy: 99.967%\n",
      "Epoch 152: Loss: 0.00746, Accuracy: 99.967%\n",
      "Epoch 153: Loss: 0.00739, Accuracy: 99.967%\n",
      "Epoch 154: Loss: 0.00732, Accuracy: 99.967%\n",
      "Epoch 155: Loss: 0.00725, Accuracy: 99.967%\n",
      "Epoch 156: Loss: 0.00718, Accuracy: 99.967%\n",
      "Epoch 157: Loss: 0.00712, Accuracy: 99.968%\n",
      "Epoch 158: Loss: 0.00705, Accuracy: 99.970%\n",
      "Epoch 159: Loss: 0.00699, Accuracy: 99.970%\n",
      "Epoch 160: Loss: 0.00693, Accuracy: 99.970%\n",
      "Epoch 161: Loss: 0.00686, Accuracy: 99.970%\n",
      "Epoch 162: Loss: 0.00680, Accuracy: 99.970%\n",
      "Epoch 163: Loss: 0.00674, Accuracy: 99.970%\n",
      "Epoch 164: Loss: 0.00668, Accuracy: 99.972%\n",
      "Epoch 165: Loss: 0.00663, Accuracy: 99.972%\n",
      "Epoch 166: Loss: 0.00657, Accuracy: 99.972%\n",
      "Epoch 167: Loss: 0.00651, Accuracy: 99.973%\n",
      "Epoch 168: Loss: 0.00645, Accuracy: 99.973%\n",
      "Epoch 169: Loss: 0.00640, Accuracy: 99.973%\n",
      "Epoch 170: Loss: 0.00634, Accuracy: 99.973%\n",
      "Epoch 171: Loss: 0.00629, Accuracy: 99.973%\n",
      "Epoch 172: Loss: 0.00624, Accuracy: 99.973%\n",
      "Epoch 173: Loss: 0.00618, Accuracy: 99.973%\n",
      "Epoch 174: Loss: 0.00613, Accuracy: 99.973%\n",
      "Epoch 175: Loss: 0.00608, Accuracy: 99.977%\n",
      "Epoch 176: Loss: 0.00603, Accuracy: 99.980%\n",
      "Epoch 177: Loss: 0.00598, Accuracy: 99.980%\n",
      "Epoch 178: Loss: 0.00593, Accuracy: 99.980%\n",
      "Epoch 179: Loss: 0.00588, Accuracy: 99.980%\n",
      "Epoch 180: Loss: 0.00584, Accuracy: 99.983%\n",
      "Epoch 181: Loss: 0.00579, Accuracy: 99.983%\n",
      "Epoch 182: Loss: 0.00574, Accuracy: 99.983%\n",
      "Epoch 183: Loss: 0.00570, Accuracy: 99.983%\n",
      "Epoch 184: Loss: 0.00565, Accuracy: 99.985%\n",
      "Epoch 185: Loss: 0.00561, Accuracy: 99.985%\n",
      "Epoch 186: Loss: 0.00556, Accuracy: 99.985%\n",
      "Epoch 187: Loss: 0.00552, Accuracy: 99.985%\n",
      "Epoch 188: Loss: 0.00547, Accuracy: 99.985%\n",
      "Epoch 189: Loss: 0.00543, Accuracy: 99.985%\n",
      "Epoch 190: Loss: 0.00539, Accuracy: 99.987%\n",
      "Epoch 191: Loss: 0.00535, Accuracy: 99.987%\n",
      "Epoch 192: Loss: 0.00531, Accuracy: 99.987%\n",
      "Epoch 193: Loss: 0.00527, Accuracy: 99.987%\n",
      "Epoch 194: Loss: 0.00523, Accuracy: 99.987%\n",
      "Epoch 195: Loss: 0.00519, Accuracy: 99.987%\n",
      "Epoch 196: Loss: 0.00515, Accuracy: 99.987%\n",
      "Epoch 197: Loss: 0.00511, Accuracy: 99.987%\n",
      "Epoch 198: Loss: 0.00507, Accuracy: 99.987%\n",
      "Epoch 199: Loss: 0.00503, Accuracy: 99.987%\n",
      "Epoch 200: Loss: 0.00500, Accuracy: 99.987%\n",
      "RMSProp\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9b70>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9b70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b81b33c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.39612, Accuracy: 92.187%\n",
      "Epoch 002: Loss: 0.18851, Accuracy: 96.528%\n",
      "Epoch 003: Loss: 0.13607, Accuracy: 97.843%\n",
      "Epoch 004: Loss: 0.10556, Accuracy: 98.552%\n",
      "Epoch 005: Loss: 0.08515, Accuracy: 98.967%\n",
      "Epoch 006: Loss: 0.07040, Accuracy: 99.265%\n",
      "Epoch 007: Loss: 0.05907, Accuracy: 99.450%\n",
      "Epoch 008: Loss: 0.05009, Accuracy: 99.605%\n",
      "Epoch 009: Loss: 0.04271, Accuracy: 99.740%\n",
      "Epoch 010: Loss: 0.03656, Accuracy: 99.807%\n",
      "Epoch 011: Loss: 0.03134, Accuracy: 99.863%\n",
      "Epoch 012: Loss: 0.02689, Accuracy: 99.905%\n",
      "Epoch 013: Loss: 0.02307, Accuracy: 99.933%\n",
      "Epoch 014: Loss: 0.01981, Accuracy: 99.950%\n",
      "Epoch 015: Loss: 0.01708, Accuracy: 99.968%\n",
      "Epoch 016: Loss: 0.01477, Accuracy: 99.980%\n",
      "Epoch 017: Loss: 0.01281, Accuracy: 99.983%\n",
      "Epoch 018: Loss: 0.01118, Accuracy: 99.985%\n",
      "Epoch 019: Loss: 0.00978, Accuracy: 99.985%\n",
      "Epoch 020: Loss: 0.00862, Accuracy: 99.987%\n",
      "Epoch 021: Loss: 0.00763, Accuracy: 99.990%\n",
      "Epoch 022: Loss: 0.00680, Accuracy: 99.992%\n",
      "Epoch 023: Loss: 0.00609, Accuracy: 99.992%\n",
      "Epoch 024: Loss: 0.00549, Accuracy: 99.993%\n",
      "Epoch 025: Loss: 0.00497, Accuracy: 99.995%\n",
      "Epoch 026: Loss: 0.00452, Accuracy: 99.997%\n",
      "Epoch 027: Loss: 0.00412, Accuracy: 99.997%\n",
      "Epoch 028: Loss: 0.00378, Accuracy: 100.000%\n",
      "Epoch 029: Loss: 0.00348, Accuracy: 100.000%\n",
      "Epoch 030: Loss: 0.00322, Accuracy: 100.000%\n",
      "Epoch 031: Loss: 0.00298, Accuracy: 100.000%\n",
      "Epoch 032: Loss: 0.00277, Accuracy: 100.000%\n",
      "Epoch 033: Loss: 0.00259, Accuracy: 100.000%\n",
      "Epoch 034: Loss: 0.00242, Accuracy: 100.000%\n",
      "Epoch 035: Loss: 0.00227, Accuracy: 100.000%\n",
      "Epoch 036: Loss: 0.00214, Accuracy: 100.000%\n",
      "Epoch 037: Loss: 0.00203, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00192, Accuracy: 100.000%\n",
      "Epoch 039: Loss: 0.00182, Accuracy: 100.000%\n",
      "Epoch 040: Loss: 0.00173, Accuracy: 100.000%\n",
      "Epoch 041: Loss: 0.00165, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00157, Accuracy: 100.000%\n",
      "Epoch 043: Loss: 0.00150, Accuracy: 100.000%\n",
      "Epoch 044: Loss: 0.00144, Accuracy: 100.000%\n",
      "Epoch 045: Loss: 0.00138, Accuracy: 100.000%\n",
      "Epoch 046: Loss: 0.00132, Accuracy: 100.000%\n",
      "Epoch 047: Loss: 0.00127, Accuracy: 100.000%\n",
      "Epoch 048: Loss: 0.00122, Accuracy: 100.000%\n",
      "Epoch 049: Loss: 0.00118, Accuracy: 100.000%\n",
      "Epoch 050: Loss: 0.00114, Accuracy: 100.000%\n",
      "Epoch 051: Loss: 0.00110, Accuracy: 100.000%\n",
      "Epoch 052: Loss: 0.00106, Accuracy: 100.000%\n",
      "Epoch 053: Loss: 0.00103, Accuracy: 100.000%\n",
      "Epoch 054: Loss: 0.00099, Accuracy: 100.000%\n",
      "Epoch 055: Loss: 0.00096, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00093, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00090, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00088, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00085, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00083, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00081, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00078, Accuracy: 100.000%\n",
      "Epoch 063: Loss: 0.00076, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00074, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00072, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00071, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00069, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00067, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00066, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00064, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00063, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00061, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00060, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00059, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00058, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00056, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00055, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00054, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00053, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00052, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00051, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00050, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00049, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00048, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00047, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00046, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 087: Loss: 0.00046, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00045, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00044, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00043, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00043, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00042, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00041, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00041, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00040, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00039, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00039, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00038, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00038, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00037, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 107: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 150: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 172: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 186: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00014, Accuracy: 100.000%\n",
      "Adam\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc598>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc598>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd40f0732e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.17390, Accuracy: 95.815%\n",
      "Epoch 002: Loss: 0.05781, Accuracy: 98.632%\n",
      "Epoch 003: Loss: 0.03268, Accuracy: 99.397%\n",
      "Epoch 004: Loss: 0.01947, Accuracy: 99.727%\n",
      "Epoch 005: Loss: 0.01174, Accuracy: 99.882%\n",
      "Epoch 006: Loss: 0.00752, Accuracy: 99.960%\n",
      "Epoch 007: Loss: 0.00729, Accuracy: 99.972%\n",
      "Epoch 008: Loss: 0.00493, Accuracy: 99.987%\n",
      "Epoch 009: Loss: 0.00559, Accuracy: 99.990%\n",
      "Epoch 010: Loss: 0.00551, Accuracy: 99.982%\n",
      "Epoch 011: Loss: 0.00581, Accuracy: 99.990%\n",
      "Epoch 012: Loss: 0.00640, Accuracy: 99.980%\n",
      "Epoch 013: Loss: 0.00373, Accuracy: 99.995%\n",
      "Epoch 014: Loss: 0.00348, Accuracy: 99.993%\n",
      "Epoch 015: Loss: 0.00398, Accuracy: 99.990%\n",
      "Epoch 016: Loss: 0.00376, Accuracy: 99.987%\n",
      "Epoch 017: Loss: 0.00240, Accuracy: 99.993%\n",
      "Epoch 018: Loss: 0.00711, Accuracy: 99.982%\n",
      "Epoch 019: Loss: 0.00231, Accuracy: 99.993%\n",
      "Epoch 020: Loss: 0.00370, Accuracy: 99.995%\n",
      "Epoch 021: Loss: 0.00366, Accuracy: 99.990%\n",
      "Epoch 022: Loss: 0.00216, Accuracy: 99.995%\n",
      "Epoch 023: Loss: 0.00235, Accuracy: 99.997%\n",
      "Epoch 024: Loss: 0.00237, Accuracy: 99.997%\n",
      "Epoch 025: Loss: 0.00301, Accuracy: 99.992%\n",
      "Epoch 026: Loss: 0.00347, Accuracy: 99.995%\n",
      "Epoch 027: Loss: 0.00329, Accuracy: 99.993%\n",
      "Epoch 028: Loss: 0.00214, Accuracy: 99.995%\n",
      "Epoch 029: Loss: 0.00357, Accuracy: 99.988%\n",
      "Epoch 030: Loss: 0.00273, Accuracy: 99.995%\n",
      "Epoch 031: Loss: 0.00101, Accuracy: 100.000%\n",
      "Epoch 032: Loss: 0.00435, Accuracy: 99.988%\n",
      "Epoch 033: Loss: 0.00220, Accuracy: 99.998%\n",
      "Epoch 034: Loss: 0.00131, Accuracy: 100.000%\n",
      "Epoch 035: Loss: 0.00119, Accuracy: 99.998%\n",
      "Epoch 036: Loss: 0.00067, Accuracy: 99.998%\n",
      "Epoch 037: Loss: 0.00007, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 039: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 040: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 041: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 043: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 044: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 045: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 046: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 047: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 048: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 049: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 050: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 051: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 052: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 053: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 054: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 055: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 063: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 087: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00000, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 150: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 172: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 186: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00000, Accuracy: 100.000%\n",
      "Trial 2\n",
      "SGD Nesterov\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5268>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5268>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 1.10081, Accuracy: 76.497%\n",
      "Epoch 002: Loss: 0.43949, Accuracy: 88.640%\n",
      "Epoch 003: Loss: 0.35074, Accuracy: 90.352%\n",
      "Epoch 004: Loss: 0.31086, Accuracy: 91.307%\n",
      "Epoch 005: Loss: 0.28512, Accuracy: 91.985%\n",
      "Epoch 006: Loss: 0.26573, Accuracy: 92.573%\n",
      "Epoch 007: Loss: 0.24991, Accuracy: 93.047%\n",
      "Epoch 008: Loss: 0.23640, Accuracy: 93.428%\n",
      "Epoch 009: Loss: 0.22451, Accuracy: 93.765%\n",
      "Epoch 010: Loss: 0.21386, Accuracy: 94.048%\n",
      "Epoch 011: Loss: 0.20421, Accuracy: 94.308%\n",
      "Epoch 012: Loss: 0.19538, Accuracy: 94.565%\n",
      "Epoch 013: Loss: 0.18724, Accuracy: 94.775%\n",
      "Epoch 014: Loss: 0.17971, Accuracy: 94.988%\n",
      "Epoch 015: Loss: 0.17272, Accuracy: 95.190%\n",
      "Epoch 016: Loss: 0.16621, Accuracy: 95.372%\n",
      "Epoch 017: Loss: 0.16013, Accuracy: 95.552%\n",
      "Epoch 018: Loss: 0.15443, Accuracy: 95.722%\n",
      "Epoch 019: Loss: 0.14910, Accuracy: 95.875%\n",
      "Epoch 020: Loss: 0.14408, Accuracy: 96.000%\n",
      "Epoch 021: Loss: 0.13936, Accuracy: 96.160%\n",
      "Epoch 022: Loss: 0.13490, Accuracy: 96.282%\n",
      "Epoch 023: Loss: 0.13068, Accuracy: 96.390%\n",
      "Epoch 024: Loss: 0.12669, Accuracy: 96.513%\n",
      "Epoch 025: Loss: 0.12290, Accuracy: 96.615%\n",
      "Epoch 026: Loss: 0.11931, Accuracy: 96.728%\n",
      "Epoch 027: Loss: 0.11589, Accuracy: 96.828%\n",
      "Epoch 028: Loss: 0.11263, Accuracy: 96.927%\n",
      "Epoch 029: Loss: 0.10952, Accuracy: 97.017%\n",
      "Epoch 030: Loss: 0.10654, Accuracy: 97.093%\n",
      "Epoch 031: Loss: 0.10370, Accuracy: 97.165%\n",
      "Epoch 032: Loss: 0.10097, Accuracy: 97.225%\n",
      "Epoch 033: Loss: 0.09835, Accuracy: 97.322%\n",
      "Epoch 034: Loss: 0.09583, Accuracy: 97.382%\n",
      "Epoch 035: Loss: 0.09341, Accuracy: 97.477%\n",
      "Epoch 036: Loss: 0.09108, Accuracy: 97.537%\n",
      "Epoch 037: Loss: 0.08885, Accuracy: 97.592%\n",
      "Epoch 038: Loss: 0.08669, Accuracy: 97.653%\n",
      "Epoch 039: Loss: 0.08461, Accuracy: 97.710%\n",
      "Epoch 040: Loss: 0.08261, Accuracy: 97.757%\n",
      "Epoch 041: Loss: 0.08067, Accuracy: 97.825%\n",
      "Epoch 042: Loss: 0.07879, Accuracy: 97.878%\n",
      "Epoch 043: Loss: 0.07698, Accuracy: 97.945%\n",
      "Epoch 044: Loss: 0.07523, Accuracy: 98.017%\n",
      "Epoch 045: Loss: 0.07354, Accuracy: 98.070%\n",
      "Epoch 046: Loss: 0.07190, Accuracy: 98.135%\n",
      "Epoch 047: Loss: 0.07032, Accuracy: 98.190%\n",
      "Epoch 048: Loss: 0.06878, Accuracy: 98.238%\n",
      "Epoch 049: Loss: 0.06729, Accuracy: 98.280%\n",
      "Epoch 050: Loss: 0.06585, Accuracy: 98.313%\n",
      "Epoch 051: Loss: 0.06445, Accuracy: 98.360%\n",
      "Epoch 052: Loss: 0.06309, Accuracy: 98.405%\n",
      "Epoch 053: Loss: 0.06177, Accuracy: 98.437%\n",
      "Epoch 054: Loss: 0.06050, Accuracy: 98.480%\n",
      "Epoch 055: Loss: 0.05925, Accuracy: 98.518%\n",
      "Epoch 056: Loss: 0.05805, Accuracy: 98.542%\n",
      "Epoch 057: Loss: 0.05687, Accuracy: 98.585%\n",
      "Epoch 058: Loss: 0.05573, Accuracy: 98.627%\n",
      "Epoch 059: Loss: 0.05462, Accuracy: 98.663%\n",
      "Epoch 060: Loss: 0.05353, Accuracy: 98.695%\n",
      "Epoch 061: Loss: 0.05248, Accuracy: 98.722%\n",
      "Epoch 062: Loss: 0.05145, Accuracy: 98.743%\n",
      "Epoch 063: Loss: 0.05045, Accuracy: 98.773%\n",
      "Epoch 064: Loss: 0.04948, Accuracy: 98.813%\n",
      "Epoch 065: Loss: 0.04853, Accuracy: 98.833%\n",
      "Epoch 066: Loss: 0.04760, Accuracy: 98.855%\n",
      "Epoch 067: Loss: 0.04670, Accuracy: 98.885%\n",
      "Epoch 068: Loss: 0.04582, Accuracy: 98.913%\n",
      "Epoch 069: Loss: 0.04496, Accuracy: 98.937%\n",
      "Epoch 070: Loss: 0.04412, Accuracy: 98.957%\n",
      "Epoch 071: Loss: 0.04330, Accuracy: 98.972%\n",
      "Epoch 072: Loss: 0.04250, Accuracy: 98.993%\n",
      "Epoch 073: Loss: 0.04172, Accuracy: 99.015%\n",
      "Epoch 074: Loss: 0.04096, Accuracy: 99.030%\n",
      "Epoch 075: Loss: 0.04021, Accuracy: 99.052%\n",
      "Epoch 076: Loss: 0.03948, Accuracy: 99.078%\n",
      "Epoch 077: Loss: 0.03877, Accuracy: 99.112%\n",
      "Epoch 078: Loss: 0.03807, Accuracy: 99.137%\n",
      "Epoch 079: Loss: 0.03738, Accuracy: 99.157%\n",
      "Epoch 080: Loss: 0.03672, Accuracy: 99.178%\n",
      "Epoch 081: Loss: 0.03606, Accuracy: 99.207%\n",
      "Epoch 082: Loss: 0.03542, Accuracy: 99.222%\n",
      "Epoch 083: Loss: 0.03480, Accuracy: 99.232%\n",
      "Epoch 084: Loss: 0.03418, Accuracy: 99.252%\n",
      "Epoch 085: Loss: 0.03359, Accuracy: 99.267%\n",
      "Epoch 086: Loss: 0.03300, Accuracy: 99.283%\n",
      "Epoch 087: Loss: 0.03243, Accuracy: 99.297%\n",
      "Epoch 088: Loss: 0.03186, Accuracy: 99.308%\n",
      "Epoch 089: Loss: 0.03131, Accuracy: 99.328%\n",
      "Epoch 090: Loss: 0.03077, Accuracy: 99.340%\n",
      "Epoch 091: Loss: 0.03024, Accuracy: 99.358%\n",
      "Epoch 092: Loss: 0.02973, Accuracy: 99.375%\n",
      "Epoch 093: Loss: 0.02922, Accuracy: 99.385%\n",
      "Epoch 094: Loss: 0.02872, Accuracy: 99.408%\n",
      "Epoch 095: Loss: 0.02823, Accuracy: 99.425%\n",
      "Epoch 096: Loss: 0.02776, Accuracy: 99.440%\n",
      "Epoch 097: Loss: 0.02729, Accuracy: 99.450%\n",
      "Epoch 098: Loss: 0.02683, Accuracy: 99.458%\n",
      "Epoch 099: Loss: 0.02639, Accuracy: 99.467%\n",
      "Epoch 100: Loss: 0.02595, Accuracy: 99.483%\n",
      "Epoch 101: Loss: 0.02552, Accuracy: 99.498%\n",
      "Epoch 102: Loss: 0.02509, Accuracy: 99.523%\n",
      "Epoch 103: Loss: 0.02468, Accuracy: 99.537%\n",
      "Epoch 104: Loss: 0.02427, Accuracy: 99.545%\n",
      "Epoch 105: Loss: 0.02388, Accuracy: 99.562%\n",
      "Epoch 106: Loss: 0.02349, Accuracy: 99.570%\n",
      "Epoch 107: Loss: 0.02310, Accuracy: 99.580%\n",
      "Epoch 108: Loss: 0.02273, Accuracy: 99.598%\n",
      "Epoch 109: Loss: 0.02236, Accuracy: 99.607%\n",
      "Epoch 110: Loss: 0.02200, Accuracy: 99.618%\n",
      "Epoch 111: Loss: 0.02165, Accuracy: 99.630%\n",
      "Epoch 112: Loss: 0.02130, Accuracy: 99.638%\n",
      "Epoch 113: Loss: 0.02096, Accuracy: 99.653%\n",
      "Epoch 114: Loss: 0.02063, Accuracy: 99.663%\n",
      "Epoch 115: Loss: 0.02030, Accuracy: 99.675%\n",
      "Epoch 116: Loss: 0.01998, Accuracy: 99.682%\n",
      "Epoch 117: Loss: 0.01967, Accuracy: 99.683%\n",
      "Epoch 118: Loss: 0.01936, Accuracy: 99.690%\n",
      "Epoch 119: Loss: 0.01906, Accuracy: 99.702%\n",
      "Epoch 120: Loss: 0.01876, Accuracy: 99.715%\n",
      "Epoch 121: Loss: 0.01847, Accuracy: 99.732%\n",
      "Epoch 122: Loss: 0.01819, Accuracy: 99.743%\n",
      "Epoch 123: Loss: 0.01791, Accuracy: 99.755%\n",
      "Epoch 124: Loss: 0.01764, Accuracy: 99.758%\n",
      "Epoch 125: Loss: 0.01737, Accuracy: 99.763%\n",
      "Epoch 126: Loss: 0.01711, Accuracy: 99.772%\n",
      "Epoch 127: Loss: 0.01685, Accuracy: 99.782%\n",
      "Epoch 128: Loss: 0.01659, Accuracy: 99.790%\n",
      "Epoch 129: Loss: 0.01635, Accuracy: 99.798%\n",
      "Epoch 130: Loss: 0.01610, Accuracy: 99.807%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131: Loss: 0.01586, Accuracy: 99.810%\n",
      "Epoch 132: Loss: 0.01563, Accuracy: 99.818%\n",
      "Epoch 133: Loss: 0.01540, Accuracy: 99.820%\n",
      "Epoch 134: Loss: 0.01517, Accuracy: 99.825%\n",
      "Epoch 135: Loss: 0.01495, Accuracy: 99.825%\n",
      "Epoch 136: Loss: 0.01473, Accuracy: 99.828%\n",
      "Epoch 137: Loss: 0.01452, Accuracy: 99.833%\n",
      "Epoch 138: Loss: 0.01431, Accuracy: 99.843%\n",
      "Epoch 139: Loss: 0.01411, Accuracy: 99.847%\n",
      "Epoch 140: Loss: 0.01390, Accuracy: 99.852%\n",
      "Epoch 141: Loss: 0.01371, Accuracy: 99.855%\n",
      "Epoch 142: Loss: 0.01351, Accuracy: 99.860%\n",
      "Epoch 143: Loss: 0.01332, Accuracy: 99.863%\n",
      "Epoch 144: Loss: 0.01314, Accuracy: 99.865%\n",
      "Epoch 145: Loss: 0.01295, Accuracy: 99.865%\n",
      "Epoch 146: Loss: 0.01277, Accuracy: 99.865%\n",
      "Epoch 147: Loss: 0.01260, Accuracy: 99.872%\n",
      "Epoch 148: Loss: 0.01242, Accuracy: 99.875%\n",
      "Epoch 149: Loss: 0.01225, Accuracy: 99.882%\n",
      "Epoch 150: Loss: 0.01209, Accuracy: 99.888%\n",
      "Epoch 151: Loss: 0.01192, Accuracy: 99.892%\n",
      "Epoch 152: Loss: 0.01176, Accuracy: 99.892%\n",
      "Epoch 153: Loss: 0.01160, Accuracy: 99.895%\n",
      "Epoch 154: Loss: 0.01145, Accuracy: 99.895%\n",
      "Epoch 155: Loss: 0.01129, Accuracy: 99.900%\n",
      "Epoch 156: Loss: 0.01114, Accuracy: 99.907%\n",
      "Epoch 157: Loss: 0.01100, Accuracy: 99.913%\n",
      "Epoch 158: Loss: 0.01085, Accuracy: 99.915%\n",
      "Epoch 159: Loss: 0.01071, Accuracy: 99.918%\n",
      "Epoch 160: Loss: 0.01057, Accuracy: 99.920%\n",
      "Epoch 161: Loss: 0.01043, Accuracy: 99.922%\n",
      "Epoch 162: Loss: 0.01030, Accuracy: 99.922%\n",
      "Epoch 163: Loss: 0.01017, Accuracy: 99.923%\n",
      "Epoch 164: Loss: 0.01004, Accuracy: 99.925%\n",
      "Epoch 165: Loss: 0.00991, Accuracy: 99.927%\n",
      "Epoch 166: Loss: 0.00978, Accuracy: 99.932%\n",
      "Epoch 167: Loss: 0.00966, Accuracy: 99.933%\n",
      "Epoch 168: Loss: 0.00954, Accuracy: 99.933%\n",
      "Epoch 169: Loss: 0.00942, Accuracy: 99.933%\n",
      "Epoch 170: Loss: 0.00930, Accuracy: 99.935%\n",
      "Epoch 171: Loss: 0.00919, Accuracy: 99.937%\n",
      "Epoch 172: Loss: 0.00908, Accuracy: 99.940%\n",
      "Epoch 173: Loss: 0.00897, Accuracy: 99.940%\n",
      "Epoch 174: Loss: 0.00886, Accuracy: 99.940%\n",
      "Epoch 175: Loss: 0.00875, Accuracy: 99.940%\n",
      "Epoch 176: Loss: 0.00864, Accuracy: 99.942%\n",
      "Epoch 177: Loss: 0.00854, Accuracy: 99.942%\n",
      "Epoch 178: Loss: 0.00844, Accuracy: 99.945%\n",
      "Epoch 179: Loss: 0.00834, Accuracy: 99.948%\n",
      "Epoch 180: Loss: 0.00824, Accuracy: 99.952%\n",
      "Epoch 181: Loss: 0.00814, Accuracy: 99.955%\n",
      "Epoch 182: Loss: 0.00805, Accuracy: 99.957%\n",
      "Epoch 183: Loss: 0.00795, Accuracy: 99.957%\n",
      "Epoch 184: Loss: 0.00786, Accuracy: 99.960%\n",
      "Epoch 185: Loss: 0.00777, Accuracy: 99.963%\n",
      "Epoch 186: Loss: 0.00768, Accuracy: 99.963%\n",
      "Epoch 187: Loss: 0.00759, Accuracy: 99.963%\n",
      "Epoch 188: Loss: 0.00751, Accuracy: 99.963%\n",
      "Epoch 189: Loss: 0.00742, Accuracy: 99.965%\n",
      "Epoch 190: Loss: 0.00734, Accuracy: 99.968%\n",
      "Epoch 191: Loss: 0.00725, Accuracy: 99.968%\n",
      "Epoch 192: Loss: 0.00717, Accuracy: 99.968%\n",
      "Epoch 193: Loss: 0.00709, Accuracy: 99.968%\n",
      "Epoch 194: Loss: 0.00701, Accuracy: 99.968%\n",
      "Epoch 195: Loss: 0.00694, Accuracy: 99.968%\n",
      "Epoch 196: Loss: 0.00686, Accuracy: 99.970%\n",
      "Epoch 197: Loss: 0.00679, Accuracy: 99.970%\n",
      "Epoch 198: Loss: 0.00671, Accuracy: 99.970%\n",
      "Epoch 199: Loss: 0.00664, Accuracy: 99.972%\n",
      "Epoch 200: Loss: 0.00657, Accuracy: 99.972%\n",
      "Adagrad\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035db70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035db70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035db70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035db70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.29897, Accuracy: 94.023%\n",
      "Epoch 002: Loss: 0.17748, Accuracy: 95.868%\n",
      "Epoch 003: Loss: 0.14841, Accuracy: 96.470%\n",
      "Epoch 004: Loss: 0.13095, Accuracy: 96.882%\n",
      "Epoch 005: Loss: 0.11862, Accuracy: 97.212%\n",
      "Epoch 006: Loss: 0.10915, Accuracy: 97.433%\n",
      "Epoch 007: Loss: 0.10152, Accuracy: 97.598%\n",
      "Epoch 008: Loss: 0.09520, Accuracy: 97.760%\n",
      "Epoch 009: Loss: 0.08980, Accuracy: 97.898%\n",
      "Epoch 010: Loss: 0.08512, Accuracy: 97.988%\n",
      "Epoch 011: Loss: 0.08098, Accuracy: 98.095%\n",
      "Epoch 012: Loss: 0.07730, Accuracy: 98.182%\n",
      "Epoch 013: Loss: 0.07398, Accuracy: 98.275%\n",
      "Epoch 014: Loss: 0.07097, Accuracy: 98.382%\n",
      "Epoch 015: Loss: 0.06823, Accuracy: 98.457%\n",
      "Epoch 016: Loss: 0.06571, Accuracy: 98.517%\n",
      "Epoch 017: Loss: 0.06339, Accuracy: 98.588%\n",
      "Epoch 018: Loss: 0.06124, Accuracy: 98.648%\n",
      "Epoch 019: Loss: 0.05924, Accuracy: 98.683%\n",
      "Epoch 020: Loss: 0.05737, Accuracy: 98.717%\n",
      "Epoch 021: Loss: 0.05562, Accuracy: 98.748%\n",
      "Epoch 022: Loss: 0.05397, Accuracy: 98.788%\n",
      "Epoch 023: Loss: 0.05242, Accuracy: 98.845%\n",
      "Epoch 024: Loss: 0.05095, Accuracy: 98.877%\n",
      "Epoch 025: Loss: 0.04956, Accuracy: 98.903%\n",
      "Epoch 026: Loss: 0.04825, Accuracy: 98.930%\n",
      "Epoch 027: Loss: 0.04699, Accuracy: 98.953%\n",
      "Epoch 028: Loss: 0.04580, Accuracy: 98.982%\n",
      "Epoch 029: Loss: 0.04466, Accuracy: 99.018%\n",
      "Epoch 030: Loss: 0.04357, Accuracy: 99.045%\n",
      "Epoch 031: Loss: 0.04253, Accuracy: 99.077%\n",
      "Epoch 032: Loss: 0.04153, Accuracy: 99.108%\n",
      "Epoch 033: Loss: 0.04058, Accuracy: 99.133%\n",
      "Epoch 034: Loss: 0.03965, Accuracy: 99.160%\n",
      "Epoch 035: Loss: 0.03877, Accuracy: 99.198%\n",
      "Epoch 036: Loss: 0.03792, Accuracy: 99.223%\n",
      "Epoch 037: Loss: 0.03710, Accuracy: 99.250%\n",
      "Epoch 038: Loss: 0.03631, Accuracy: 99.280%\n",
      "Epoch 039: Loss: 0.03555, Accuracy: 99.315%\n",
      "Epoch 040: Loss: 0.03482, Accuracy: 99.342%\n",
      "Epoch 041: Loss: 0.03411, Accuracy: 99.357%\n",
      "Epoch 042: Loss: 0.03343, Accuracy: 99.368%\n",
      "Epoch 043: Loss: 0.03276, Accuracy: 99.383%\n",
      "Epoch 044: Loss: 0.03213, Accuracy: 99.400%\n",
      "Epoch 045: Loss: 0.03151, Accuracy: 99.412%\n",
      "Epoch 046: Loss: 0.03091, Accuracy: 99.432%\n",
      "Epoch 047: Loss: 0.03033, Accuracy: 99.445%\n",
      "Epoch 048: Loss: 0.02977, Accuracy: 99.458%\n",
      "Epoch 049: Loss: 0.02922, Accuracy: 99.473%\n",
      "Epoch 050: Loss: 0.02869, Accuracy: 99.500%\n",
      "Epoch 051: Loss: 0.02818, Accuracy: 99.513%\n",
      "Epoch 052: Loss: 0.02768, Accuracy: 99.537%\n",
      "Epoch 053: Loss: 0.02719, Accuracy: 99.548%\n",
      "Epoch 054: Loss: 0.02672, Accuracy: 99.562%\n",
      "Epoch 055: Loss: 0.02627, Accuracy: 99.575%\n",
      "Epoch 056: Loss: 0.02582, Accuracy: 99.578%\n",
      "Epoch 057: Loss: 0.02539, Accuracy: 99.593%\n",
      "Epoch 058: Loss: 0.02497, Accuracy: 99.600%\n",
      "Epoch 059: Loss: 0.02456, Accuracy: 99.613%\n",
      "Epoch 060: Loss: 0.02416, Accuracy: 99.625%\n",
      "Epoch 061: Loss: 0.02377, Accuracy: 99.633%\n",
      "Epoch 062: Loss: 0.02339, Accuracy: 99.642%\n",
      "Epoch 063: Loss: 0.02302, Accuracy: 99.658%\n",
      "Epoch 064: Loss: 0.02266, Accuracy: 99.665%\n",
      "Epoch 065: Loss: 0.02230, Accuracy: 99.677%\n",
      "Epoch 066: Loss: 0.02196, Accuracy: 99.688%\n",
      "Epoch 067: Loss: 0.02162, Accuracy: 99.692%\n",
      "Epoch 068: Loss: 0.02130, Accuracy: 99.702%\n",
      "Epoch 069: Loss: 0.02098, Accuracy: 99.708%\n",
      "Epoch 070: Loss: 0.02067, Accuracy: 99.715%\n",
      "Epoch 071: Loss: 0.02036, Accuracy: 99.737%\n",
      "Epoch 072: Loss: 0.02006, Accuracy: 99.747%\n",
      "Epoch 073: Loss: 0.01977, Accuracy: 99.750%\n",
      "Epoch 074: Loss: 0.01948, Accuracy: 99.758%\n",
      "Epoch 075: Loss: 0.01920, Accuracy: 99.770%\n",
      "Epoch 076: Loss: 0.01893, Accuracy: 99.773%\n",
      "Epoch 077: Loss: 0.01866, Accuracy: 99.773%\n",
      "Epoch 078: Loss: 0.01840, Accuracy: 99.783%\n",
      "Epoch 079: Loss: 0.01815, Accuracy: 99.787%\n",
      "Epoch 080: Loss: 0.01790, Accuracy: 99.790%\n",
      "Epoch 081: Loss: 0.01765, Accuracy: 99.790%\n",
      "Epoch 082: Loss: 0.01741, Accuracy: 99.800%\n",
      "Epoch 083: Loss: 0.01718, Accuracy: 99.800%\n",
      "Epoch 084: Loss: 0.01695, Accuracy: 99.807%\n",
      "Epoch 085: Loss: 0.01672, Accuracy: 99.813%\n",
      "Epoch 086: Loss: 0.01650, Accuracy: 99.818%\n",
      "Epoch 087: Loss: 0.01629, Accuracy: 99.822%\n",
      "Epoch 088: Loss: 0.01608, Accuracy: 99.822%\n",
      "Epoch 089: Loss: 0.01587, Accuracy: 99.827%\n",
      "Epoch 090: Loss: 0.01566, Accuracy: 99.832%\n",
      "Epoch 091: Loss: 0.01546, Accuracy: 99.835%\n",
      "Epoch 092: Loss: 0.01527, Accuracy: 99.842%\n",
      "Epoch 093: Loss: 0.01508, Accuracy: 99.848%\n",
      "Epoch 094: Loss: 0.01489, Accuracy: 99.850%\n",
      "Epoch 095: Loss: 0.01470, Accuracy: 99.852%\n",
      "Epoch 096: Loss: 0.01452, Accuracy: 99.852%\n",
      "Epoch 097: Loss: 0.01435, Accuracy: 99.857%\n",
      "Epoch 098: Loss: 0.01417, Accuracy: 99.857%\n",
      "Epoch 099: Loss: 0.01400, Accuracy: 99.862%\n",
      "Epoch 100: Loss: 0.01383, Accuracy: 99.863%\n",
      "Epoch 101: Loss: 0.01367, Accuracy: 99.865%\n",
      "Epoch 102: Loss: 0.01350, Accuracy: 99.872%\n",
      "Epoch 103: Loss: 0.01334, Accuracy: 99.872%\n",
      "Epoch 104: Loss: 0.01319, Accuracy: 99.875%\n",
      "Epoch 105: Loss: 0.01303, Accuracy: 99.880%\n",
      "Epoch 106: Loss: 0.01288, Accuracy: 99.883%\n",
      "Epoch 107: Loss: 0.01274, Accuracy: 99.885%\n",
      "Epoch 108: Loss: 0.01259, Accuracy: 99.887%\n",
      "Epoch 109: Loss: 0.01245, Accuracy: 99.887%\n",
      "Epoch 110: Loss: 0.01231, Accuracy: 99.890%\n",
      "Epoch 111: Loss: 0.01217, Accuracy: 99.897%\n",
      "Epoch 112: Loss: 0.01203, Accuracy: 99.897%\n",
      "Epoch 113: Loss: 0.01190, Accuracy: 99.897%\n",
      "Epoch 114: Loss: 0.01176, Accuracy: 99.898%\n",
      "Epoch 115: Loss: 0.01164, Accuracy: 99.903%\n",
      "Epoch 116: Loss: 0.01151, Accuracy: 99.903%\n",
      "Epoch 117: Loss: 0.01138, Accuracy: 99.905%\n",
      "Epoch 118: Loss: 0.01126, Accuracy: 99.912%\n",
      "Epoch 119: Loss: 0.01114, Accuracy: 99.917%\n",
      "Epoch 120: Loss: 0.01102, Accuracy: 99.918%\n",
      "Epoch 121: Loss: 0.01090, Accuracy: 99.918%\n",
      "Epoch 122: Loss: 0.01079, Accuracy: 99.918%\n",
      "Epoch 123: Loss: 0.01067, Accuracy: 99.918%\n",
      "Epoch 124: Loss: 0.01056, Accuracy: 99.918%\n",
      "Epoch 125: Loss: 0.01045, Accuracy: 99.920%\n",
      "Epoch 126: Loss: 0.01034, Accuracy: 99.922%\n",
      "Epoch 127: Loss: 0.01024, Accuracy: 99.922%\n",
      "Epoch 128: Loss: 0.01013, Accuracy: 99.922%\n",
      "Epoch 129: Loss: 0.01003, Accuracy: 99.922%\n",
      "Epoch 130: Loss: 0.00993, Accuracy: 99.922%\n",
      "Epoch 131: Loss: 0.00983, Accuracy: 99.925%\n",
      "Epoch 132: Loss: 0.00973, Accuracy: 99.927%\n",
      "Epoch 133: Loss: 0.00963, Accuracy: 99.927%\n",
      "Epoch 134: Loss: 0.00954, Accuracy: 99.930%\n",
      "Epoch 135: Loss: 0.00944, Accuracy: 99.932%\n",
      "Epoch 136: Loss: 0.00935, Accuracy: 99.935%\n",
      "Epoch 137: Loss: 0.00926, Accuracy: 99.937%\n",
      "Epoch 138: Loss: 0.00917, Accuracy: 99.938%\n",
      "Epoch 139: Loss: 0.00908, Accuracy: 99.943%\n",
      "Epoch 140: Loss: 0.00899, Accuracy: 99.943%\n",
      "Epoch 141: Loss: 0.00891, Accuracy: 99.945%\n",
      "Epoch 142: Loss: 0.00882, Accuracy: 99.945%\n",
      "Epoch 143: Loss: 0.00874, Accuracy: 99.945%\n",
      "Epoch 144: Loss: 0.00865, Accuracy: 99.945%\n",
      "Epoch 145: Loss: 0.00857, Accuracy: 99.947%\n",
      "Epoch 146: Loss: 0.00849, Accuracy: 99.948%\n",
      "Epoch 147: Loss: 0.00841, Accuracy: 99.948%\n",
      "Epoch 148: Loss: 0.00834, Accuracy: 99.950%\n",
      "Epoch 149: Loss: 0.00826, Accuracy: 99.950%\n",
      "Epoch 150: Loss: 0.00818, Accuracy: 99.953%\n",
      "Epoch 151: Loss: 0.00811, Accuracy: 99.953%\n",
      "Epoch 152: Loss: 0.00803, Accuracy: 99.955%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153: Loss: 0.00796, Accuracy: 99.958%\n",
      "Epoch 154: Loss: 0.00789, Accuracy: 99.960%\n",
      "Epoch 155: Loss: 0.00782, Accuracy: 99.960%\n",
      "Epoch 156: Loss: 0.00775, Accuracy: 99.962%\n",
      "Epoch 157: Loss: 0.00768, Accuracy: 99.962%\n",
      "Epoch 158: Loss: 0.00761, Accuracy: 99.962%\n",
      "Epoch 159: Loss: 0.00754, Accuracy: 99.963%\n",
      "Epoch 160: Loss: 0.00748, Accuracy: 99.963%\n",
      "Epoch 161: Loss: 0.00741, Accuracy: 99.963%\n",
      "Epoch 162: Loss: 0.00735, Accuracy: 99.963%\n",
      "Epoch 163: Loss: 0.00728, Accuracy: 99.963%\n",
      "Epoch 164: Loss: 0.00722, Accuracy: 99.963%\n",
      "Epoch 165: Loss: 0.00716, Accuracy: 99.963%\n",
      "Epoch 166: Loss: 0.00710, Accuracy: 99.963%\n",
      "Epoch 167: Loss: 0.00704, Accuracy: 99.963%\n",
      "Epoch 168: Loss: 0.00698, Accuracy: 99.963%\n",
      "Epoch 169: Loss: 0.00692, Accuracy: 99.963%\n",
      "Epoch 170: Loss: 0.00686, Accuracy: 99.963%\n",
      "Epoch 171: Loss: 0.00680, Accuracy: 99.963%\n",
      "Epoch 172: Loss: 0.00675, Accuracy: 99.965%\n",
      "Epoch 173: Loss: 0.00669, Accuracy: 99.968%\n",
      "Epoch 174: Loss: 0.00664, Accuracy: 99.968%\n",
      "Epoch 175: Loss: 0.00658, Accuracy: 99.968%\n",
      "Epoch 176: Loss: 0.00653, Accuracy: 99.968%\n",
      "Epoch 177: Loss: 0.00648, Accuracy: 99.968%\n",
      "Epoch 178: Loss: 0.00642, Accuracy: 99.968%\n",
      "Epoch 179: Loss: 0.00637, Accuracy: 99.970%\n",
      "Epoch 180: Loss: 0.00632, Accuracy: 99.972%\n",
      "Epoch 181: Loss: 0.00627, Accuracy: 99.972%\n",
      "Epoch 182: Loss: 0.00622, Accuracy: 99.972%\n",
      "Epoch 183: Loss: 0.00617, Accuracy: 99.972%\n",
      "Epoch 184: Loss: 0.00612, Accuracy: 99.972%\n",
      "Epoch 185: Loss: 0.00607, Accuracy: 99.972%\n",
      "Epoch 186: Loss: 0.00602, Accuracy: 99.973%\n",
      "Epoch 187: Loss: 0.00598, Accuracy: 99.973%\n",
      "Epoch 188: Loss: 0.00593, Accuracy: 99.975%\n",
      "Epoch 189: Loss: 0.00589, Accuracy: 99.975%\n",
      "Epoch 190: Loss: 0.00584, Accuracy: 99.975%\n",
      "Epoch 191: Loss: 0.00579, Accuracy: 99.975%\n",
      "Epoch 192: Loss: 0.00575, Accuracy: 99.975%\n",
      "Epoch 193: Loss: 0.00571, Accuracy: 99.977%\n",
      "Epoch 194: Loss: 0.00566, Accuracy: 99.977%\n",
      "Epoch 195: Loss: 0.00562, Accuracy: 99.977%\n",
      "Epoch 196: Loss: 0.00558, Accuracy: 99.977%\n",
      "Epoch 197: Loss: 0.00554, Accuracy: 99.977%\n",
      "Epoch 198: Loss: 0.00549, Accuracy: 99.977%\n",
      "Epoch 199: Loss: 0.00545, Accuracy: 99.977%\n",
      "Epoch 200: Loss: 0.00541, Accuracy: 99.977%\n",
      "RMSProp\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc268>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc268>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.39347, Accuracy: 92.195%\n",
      "Epoch 002: Loss: 0.19054, Accuracy: 96.485%\n",
      "Epoch 003: Loss: 0.13796, Accuracy: 97.792%\n",
      "Epoch 004: Loss: 0.10729, Accuracy: 98.473%\n",
      "Epoch 005: Loss: 0.08659, Accuracy: 98.932%\n",
      "Epoch 006: Loss: 0.07148, Accuracy: 99.230%\n",
      "Epoch 007: Loss: 0.05998, Accuracy: 99.437%\n",
      "Epoch 008: Loss: 0.05088, Accuracy: 99.600%\n",
      "Epoch 009: Loss: 0.04345, Accuracy: 99.725%\n",
      "Epoch 010: Loss: 0.03729, Accuracy: 99.788%\n",
      "Epoch 011: Loss: 0.03207, Accuracy: 99.852%\n",
      "Epoch 012: Loss: 0.02764, Accuracy: 99.902%\n",
      "Epoch 013: Loss: 0.02384, Accuracy: 99.925%\n",
      "Epoch 014: Loss: 0.02060, Accuracy: 99.938%\n",
      "Epoch 015: Loss: 0.01781, Accuracy: 99.957%\n",
      "Epoch 016: Loss: 0.01545, Accuracy: 99.970%\n",
      "Epoch 017: Loss: 0.01343, Accuracy: 99.978%\n",
      "Epoch 018: Loss: 0.01173, Accuracy: 99.980%\n",
      "Epoch 019: Loss: 0.01029, Accuracy: 99.982%\n",
      "Epoch 020: Loss: 0.00909, Accuracy: 99.988%\n",
      "Epoch 021: Loss: 0.00806, Accuracy: 99.990%\n",
      "Epoch 022: Loss: 0.00718, Accuracy: 99.992%\n",
      "Epoch 023: Loss: 0.00643, Accuracy: 99.993%\n",
      "Epoch 024: Loss: 0.00578, Accuracy: 99.993%\n",
      "Epoch 025: Loss: 0.00522, Accuracy: 99.995%\n",
      "Epoch 026: Loss: 0.00474, Accuracy: 99.997%\n",
      "Epoch 027: Loss: 0.00432, Accuracy: 99.997%\n",
      "Epoch 028: Loss: 0.00395, Accuracy: 99.998%\n",
      "Epoch 029: Loss: 0.00363, Accuracy: 99.998%\n",
      "Epoch 030: Loss: 0.00333, Accuracy: 99.998%\n",
      "Epoch 031: Loss: 0.00308, Accuracy: 100.000%\n",
      "Epoch 032: Loss: 0.00286, Accuracy: 100.000%\n",
      "Epoch 033: Loss: 0.00266, Accuracy: 100.000%\n",
      "Epoch 034: Loss: 0.00249, Accuracy: 100.000%\n",
      "Epoch 035: Loss: 0.00233, Accuracy: 100.000%\n",
      "Epoch 036: Loss: 0.00220, Accuracy: 100.000%\n",
      "Epoch 037: Loss: 0.00207, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00196, Accuracy: 100.000%\n",
      "Epoch 039: Loss: 0.00186, Accuracy: 100.000%\n",
      "Epoch 040: Loss: 0.00177, Accuracy: 100.000%\n",
      "Epoch 041: Loss: 0.00168, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00160, Accuracy: 100.000%\n",
      "Epoch 043: Loss: 0.00153, Accuracy: 100.000%\n",
      "Epoch 044: Loss: 0.00147, Accuracy: 100.000%\n",
      "Epoch 045: Loss: 0.00140, Accuracy: 100.000%\n",
      "Epoch 046: Loss: 0.00135, Accuracy: 100.000%\n",
      "Epoch 047: Loss: 0.00129, Accuracy: 100.000%\n",
      "Epoch 048: Loss: 0.00124, Accuracy: 100.000%\n",
      "Epoch 049: Loss: 0.00120, Accuracy: 100.000%\n",
      "Epoch 050: Loss: 0.00115, Accuracy: 100.000%\n",
      "Epoch 051: Loss: 0.00111, Accuracy: 100.000%\n",
      "Epoch 052: Loss: 0.00108, Accuracy: 100.000%\n",
      "Epoch 053: Loss: 0.00104, Accuracy: 100.000%\n",
      "Epoch 054: Loss: 0.00101, Accuracy: 100.000%\n",
      "Epoch 055: Loss: 0.00097, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00094, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00092, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00089, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00086, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00084, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00082, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00079, Accuracy: 100.000%\n",
      "Epoch 063: Loss: 0.00077, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00075, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00073, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00072, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00070, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00068, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00066, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00065, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00063, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00062, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00061, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00059, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00058, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00057, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00056, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00055, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00054, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00053, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00052, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00051, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00050, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00049, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00048, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00047, Accuracy: 100.000%\n",
      "Epoch 087: Loss: 0.00046, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00045, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00045, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00044, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00043, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00042, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00042, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00041, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00040, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00040, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00039, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00039, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00038, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00037, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00037, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 107: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 150: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00017, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 186: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00014, Accuracy: 100.000%\n",
      "Adam\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d840>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d840>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f04a2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.17336, Accuracy: 95.868%\n",
      "Epoch 002: Loss: 0.05744, Accuracy: 98.712%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003: Loss: 0.03223, Accuracy: 99.410%\n",
      "Epoch 004: Loss: 0.01877, Accuracy: 99.732%\n",
      "Epoch 005: Loss: 0.01189, Accuracy: 99.887%\n",
      "Epoch 006: Loss: 0.00735, Accuracy: 99.958%\n",
      "Epoch 007: Loss: 0.00555, Accuracy: 99.982%\n",
      "Epoch 008: Loss: 0.00498, Accuracy: 99.990%\n",
      "Epoch 009: Loss: 0.00841, Accuracy: 99.973%\n",
      "Epoch 010: Loss: 0.00615, Accuracy: 99.980%\n",
      "Epoch 011: Loss: 0.00612, Accuracy: 99.975%\n",
      "Epoch 012: Loss: 0.00385, Accuracy: 99.988%\n",
      "Epoch 013: Loss: 0.00399, Accuracy: 99.988%\n",
      "Epoch 014: Loss: 0.00527, Accuracy: 99.993%\n",
      "Epoch 015: Loss: 0.00391, Accuracy: 99.987%\n",
      "Epoch 016: Loss: 0.00185, Accuracy: 100.000%\n",
      "Epoch 017: Loss: 0.00415, Accuracy: 99.983%\n",
      "Epoch 018: Loss: 0.00473, Accuracy: 99.993%\n",
      "Epoch 019: Loss: 0.00418, Accuracy: 99.987%\n",
      "Epoch 020: Loss: 0.00167, Accuracy: 100.000%\n",
      "Epoch 021: Loss: 0.00478, Accuracy: 99.993%\n",
      "Epoch 022: Loss: 0.00152, Accuracy: 99.997%\n",
      "Epoch 023: Loss: 0.00085, Accuracy: 100.000%\n",
      "Epoch 024: Loss: 0.00327, Accuracy: 99.997%\n",
      "Epoch 025: Loss: 0.00553, Accuracy: 99.982%\n",
      "Epoch 026: Loss: 0.00077, Accuracy: 99.998%\n",
      "Epoch 027: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 028: Loss: 0.00005, Accuracy: 100.000%\n",
      "Epoch 029: Loss: 0.00002, Accuracy: 100.000%\n",
      "Epoch 030: Loss: 0.00002, Accuracy: 100.000%\n",
      "Epoch 031: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 032: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 033: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 034: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 035: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 036: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 037: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 039: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 040: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 041: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 043: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 044: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 045: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 046: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 047: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 048: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 049: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 050: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 051: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 052: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 053: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 054: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 055: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 063: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 087: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 107: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 150: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 172: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00000, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00000, Accuracy: 100.000%\n",
      "Trial 3\n",
      "SGD Nesterov\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9a60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9a60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f01a2fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 1.07075, Accuracy: 77.328%\n",
      "Epoch 002: Loss: 0.43282, Accuracy: 88.730%\n",
      "Epoch 003: Loss: 0.34884, Accuracy: 90.350%\n",
      "Epoch 004: Loss: 0.31103, Accuracy: 91.277%\n",
      "Epoch 005: Loss: 0.28659, Accuracy: 91.910%\n",
      "Epoch 006: Loss: 0.26805, Accuracy: 92.473%\n",
      "Epoch 007: Loss: 0.25279, Accuracy: 92.887%\n",
      "Epoch 008: Loss: 0.23961, Accuracy: 93.263%\n",
      "Epoch 009: Loss: 0.22791, Accuracy: 93.603%\n",
      "Epoch 010: Loss: 0.21738, Accuracy: 93.933%\n",
      "Epoch 011: Loss: 0.20778, Accuracy: 94.207%\n",
      "Epoch 012: Loss: 0.19894, Accuracy: 94.458%\n",
      "Epoch 013: Loss: 0.19077, Accuracy: 94.665%\n",
      "Epoch 014: Loss: 0.18318, Accuracy: 94.875%\n",
      "Epoch 015: Loss: 0.17609, Accuracy: 95.092%\n",
      "Epoch 016: Loss: 0.16948, Accuracy: 95.273%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017: Loss: 0.16329, Accuracy: 95.443%\n",
      "Epoch 018: Loss: 0.15749, Accuracy: 95.612%\n",
      "Epoch 019: Loss: 0.15203, Accuracy: 95.767%\n",
      "Epoch 020: Loss: 0.14691, Accuracy: 95.908%\n",
      "Epoch 021: Loss: 0.14207, Accuracy: 96.065%\n",
      "Epoch 022: Loss: 0.13750, Accuracy: 96.198%\n",
      "Epoch 023: Loss: 0.13318, Accuracy: 96.350%\n",
      "Epoch 024: Loss: 0.12909, Accuracy: 96.495%\n",
      "Epoch 025: Loss: 0.12520, Accuracy: 96.618%\n",
      "Epoch 026: Loss: 0.12151, Accuracy: 96.700%\n",
      "Epoch 027: Loss: 0.11800, Accuracy: 96.775%\n",
      "Epoch 028: Loss: 0.11465, Accuracy: 96.872%\n",
      "Epoch 029: Loss: 0.11146, Accuracy: 96.975%\n",
      "Epoch 030: Loss: 0.10840, Accuracy: 97.085%\n",
      "Epoch 031: Loss: 0.10548, Accuracy: 97.162%\n",
      "Epoch 032: Loss: 0.10268, Accuracy: 97.248%\n",
      "Epoch 033: Loss: 0.10000, Accuracy: 97.308%\n",
      "Epoch 034: Loss: 0.09743, Accuracy: 97.375%\n",
      "Epoch 035: Loss: 0.09496, Accuracy: 97.420%\n",
      "Epoch 036: Loss: 0.09259, Accuracy: 97.488%\n",
      "Epoch 037: Loss: 0.09030, Accuracy: 97.553%\n",
      "Epoch 038: Loss: 0.08810, Accuracy: 97.633%\n",
      "Epoch 039: Loss: 0.08598, Accuracy: 97.682%\n",
      "Epoch 040: Loss: 0.08393, Accuracy: 97.735%\n",
      "Epoch 041: Loss: 0.08195, Accuracy: 97.802%\n",
      "Epoch 042: Loss: 0.08004, Accuracy: 97.860%\n",
      "Epoch 043: Loss: 0.07819, Accuracy: 97.925%\n",
      "Epoch 044: Loss: 0.07640, Accuracy: 97.975%\n",
      "Epoch 045: Loss: 0.07466, Accuracy: 98.027%\n",
      "Epoch 046: Loss: 0.07298, Accuracy: 98.073%\n",
      "Epoch 047: Loss: 0.07135, Accuracy: 98.112%\n",
      "Epoch 048: Loss: 0.06978, Accuracy: 98.170%\n",
      "Epoch 049: Loss: 0.06825, Accuracy: 98.222%\n",
      "Epoch 050: Loss: 0.06677, Accuracy: 98.268%\n",
      "Epoch 051: Loss: 0.06532, Accuracy: 98.323%\n",
      "Epoch 052: Loss: 0.06393, Accuracy: 98.368%\n",
      "Epoch 053: Loss: 0.06257, Accuracy: 98.412%\n",
      "Epoch 054: Loss: 0.06125, Accuracy: 98.455%\n",
      "Epoch 055: Loss: 0.05997, Accuracy: 98.490%\n",
      "Epoch 056: Loss: 0.05873, Accuracy: 98.528%\n",
      "Epoch 057: Loss: 0.05752, Accuracy: 98.560%\n",
      "Epoch 058: Loss: 0.05635, Accuracy: 98.600%\n",
      "Epoch 059: Loss: 0.05520, Accuracy: 98.633%\n",
      "Epoch 060: Loss: 0.05409, Accuracy: 98.653%\n",
      "Epoch 061: Loss: 0.05301, Accuracy: 98.692%\n",
      "Epoch 062: Loss: 0.05195, Accuracy: 98.715%\n",
      "Epoch 063: Loss: 0.05092, Accuracy: 98.742%\n",
      "Epoch 064: Loss: 0.04992, Accuracy: 98.778%\n",
      "Epoch 065: Loss: 0.04895, Accuracy: 98.835%\n",
      "Epoch 066: Loss: 0.04800, Accuracy: 98.857%\n",
      "Epoch 067: Loss: 0.04707, Accuracy: 98.875%\n",
      "Epoch 068: Loss: 0.04616, Accuracy: 98.898%\n",
      "Epoch 069: Loss: 0.04528, Accuracy: 98.918%\n",
      "Epoch 070: Loss: 0.04442, Accuracy: 98.935%\n",
      "Epoch 071: Loss: 0.04358, Accuracy: 98.962%\n",
      "Epoch 072: Loss: 0.04276, Accuracy: 98.983%\n",
      "Epoch 073: Loss: 0.04195, Accuracy: 99.015%\n",
      "Epoch 074: Loss: 0.04117, Accuracy: 99.033%\n",
      "Epoch 075: Loss: 0.04040, Accuracy: 99.047%\n",
      "Epoch 076: Loss: 0.03966, Accuracy: 99.067%\n",
      "Epoch 077: Loss: 0.03893, Accuracy: 99.090%\n",
      "Epoch 078: Loss: 0.03821, Accuracy: 99.107%\n",
      "Epoch 079: Loss: 0.03752, Accuracy: 99.137%\n",
      "Epoch 080: Loss: 0.03683, Accuracy: 99.158%\n",
      "Epoch 081: Loss: 0.03617, Accuracy: 99.188%\n",
      "Epoch 082: Loss: 0.03552, Accuracy: 99.208%\n",
      "Epoch 083: Loss: 0.03488, Accuracy: 99.225%\n",
      "Epoch 084: Loss: 0.03426, Accuracy: 99.242%\n",
      "Epoch 085: Loss: 0.03365, Accuracy: 99.255%\n",
      "Epoch 086: Loss: 0.03305, Accuracy: 99.277%\n",
      "Epoch 087: Loss: 0.03247, Accuracy: 99.288%\n",
      "Epoch 088: Loss: 0.03190, Accuracy: 99.308%\n",
      "Epoch 089: Loss: 0.03134, Accuracy: 99.323%\n",
      "Epoch 090: Loss: 0.03080, Accuracy: 99.343%\n",
      "Epoch 091: Loss: 0.03026, Accuracy: 99.362%\n",
      "Epoch 092: Loss: 0.02973, Accuracy: 99.383%\n",
      "Epoch 093: Loss: 0.02922, Accuracy: 99.398%\n",
      "Epoch 094: Loss: 0.02872, Accuracy: 99.417%\n",
      "Epoch 095: Loss: 0.02823, Accuracy: 99.432%\n",
      "Epoch 096: Loss: 0.02775, Accuracy: 99.463%\n",
      "Epoch 097: Loss: 0.02727, Accuracy: 99.478%\n",
      "Epoch 098: Loss: 0.02681, Accuracy: 99.488%\n",
      "Epoch 099: Loss: 0.02636, Accuracy: 99.502%\n",
      "Epoch 100: Loss: 0.02592, Accuracy: 99.510%\n",
      "Epoch 101: Loss: 0.02548, Accuracy: 99.518%\n",
      "Epoch 102: Loss: 0.02505, Accuracy: 99.528%\n",
      "Epoch 103: Loss: 0.02464, Accuracy: 99.540%\n",
      "Epoch 104: Loss: 0.02423, Accuracy: 99.557%\n",
      "Epoch 105: Loss: 0.02383, Accuracy: 99.573%\n",
      "Epoch 106: Loss: 0.02344, Accuracy: 99.585%\n",
      "Epoch 107: Loss: 0.02305, Accuracy: 99.603%\n",
      "Epoch 108: Loss: 0.02268, Accuracy: 99.612%\n",
      "Epoch 109: Loss: 0.02231, Accuracy: 99.627%\n",
      "Epoch 110: Loss: 0.02195, Accuracy: 99.638%\n",
      "Epoch 111: Loss: 0.02159, Accuracy: 99.645%\n",
      "Epoch 112: Loss: 0.02124, Accuracy: 99.665%\n",
      "Epoch 113: Loss: 0.02090, Accuracy: 99.672%\n",
      "Epoch 114: Loss: 0.02057, Accuracy: 99.677%\n",
      "Epoch 115: Loss: 0.02024, Accuracy: 99.687%\n",
      "Epoch 116: Loss: 0.01992, Accuracy: 99.705%\n",
      "Epoch 117: Loss: 0.01961, Accuracy: 99.713%\n",
      "Epoch 118: Loss: 0.01930, Accuracy: 99.723%\n",
      "Epoch 119: Loss: 0.01900, Accuracy: 99.730%\n",
      "Epoch 120: Loss: 0.01870, Accuracy: 99.735%\n",
      "Epoch 121: Loss: 0.01841, Accuracy: 99.747%\n",
      "Epoch 122: Loss: 0.01813, Accuracy: 99.752%\n",
      "Epoch 123: Loss: 0.01785, Accuracy: 99.755%\n",
      "Epoch 124: Loss: 0.01757, Accuracy: 99.760%\n",
      "Epoch 125: Loss: 0.01730, Accuracy: 99.768%\n",
      "Epoch 126: Loss: 0.01704, Accuracy: 99.773%\n",
      "Epoch 127: Loss: 0.01678, Accuracy: 99.783%\n",
      "Epoch 128: Loss: 0.01653, Accuracy: 99.788%\n",
      "Epoch 129: Loss: 0.01628, Accuracy: 99.795%\n",
      "Epoch 130: Loss: 0.01604, Accuracy: 99.805%\n",
      "Epoch 131: Loss: 0.01580, Accuracy: 99.812%\n",
      "Epoch 132: Loss: 0.01556, Accuracy: 99.815%\n",
      "Epoch 133: Loss: 0.01533, Accuracy: 99.817%\n",
      "Epoch 134: Loss: 0.01511, Accuracy: 99.827%\n",
      "Epoch 135: Loss: 0.01489, Accuracy: 99.832%\n",
      "Epoch 136: Loss: 0.01467, Accuracy: 99.838%\n",
      "Epoch 137: Loss: 0.01446, Accuracy: 99.843%\n",
      "Epoch 138: Loss: 0.01425, Accuracy: 99.848%\n",
      "Epoch 139: Loss: 0.01404, Accuracy: 99.850%\n",
      "Epoch 140: Loss: 0.01384, Accuracy: 99.853%\n",
      "Epoch 141: Loss: 0.01365, Accuracy: 99.858%\n",
      "Epoch 142: Loss: 0.01345, Accuracy: 99.863%\n",
      "Epoch 143: Loss: 0.01326, Accuracy: 99.865%\n",
      "Epoch 144: Loss: 0.01308, Accuracy: 99.867%\n",
      "Epoch 145: Loss: 0.01289, Accuracy: 99.873%\n",
      "Epoch 146: Loss: 0.01271, Accuracy: 99.875%\n",
      "Epoch 147: Loss: 0.01254, Accuracy: 99.880%\n",
      "Epoch 148: Loss: 0.01236, Accuracy: 99.883%\n",
      "Epoch 149: Loss: 0.01219, Accuracy: 99.887%\n",
      "Epoch 150: Loss: 0.01203, Accuracy: 99.890%\n",
      "Epoch 151: Loss: 0.01186, Accuracy: 99.890%\n",
      "Epoch 152: Loss: 0.01170, Accuracy: 99.897%\n",
      "Epoch 153: Loss: 0.01154, Accuracy: 99.898%\n",
      "Epoch 154: Loss: 0.01139, Accuracy: 99.900%\n",
      "Epoch 155: Loss: 0.01124, Accuracy: 99.902%\n",
      "Epoch 156: Loss: 0.01109, Accuracy: 99.905%\n",
      "Epoch 157: Loss: 0.01094, Accuracy: 99.905%\n",
      "Epoch 158: Loss: 0.01080, Accuracy: 99.908%\n",
      "Epoch 159: Loss: 0.01066, Accuracy: 99.917%\n",
      "Epoch 160: Loss: 0.01052, Accuracy: 99.918%\n",
      "Epoch 161: Loss: 0.01038, Accuracy: 99.922%\n",
      "Epoch 162: Loss: 0.01025, Accuracy: 99.925%\n",
      "Epoch 163: Loss: 0.01011, Accuracy: 99.927%\n",
      "Epoch 164: Loss: 0.00998, Accuracy: 99.928%\n",
      "Epoch 165: Loss: 0.00986, Accuracy: 99.930%\n",
      "Epoch 166: Loss: 0.00973, Accuracy: 99.933%\n",
      "Epoch 167: Loss: 0.00961, Accuracy: 99.935%\n",
      "Epoch 168: Loss: 0.00949, Accuracy: 99.938%\n",
      "Epoch 169: Loss: 0.00937, Accuracy: 99.940%\n",
      "Epoch 170: Loss: 0.00925, Accuracy: 99.940%\n",
      "Epoch 171: Loss: 0.00914, Accuracy: 99.940%\n",
      "Epoch 172: Loss: 0.00903, Accuracy: 99.943%\n",
      "Epoch 173: Loss: 0.00892, Accuracy: 99.943%\n",
      "Epoch 174: Loss: 0.00881, Accuracy: 99.943%\n",
      "Epoch 175: Loss: 0.00870, Accuracy: 99.945%\n",
      "Epoch 176: Loss: 0.00860, Accuracy: 99.945%\n",
      "Epoch 177: Loss: 0.00849, Accuracy: 99.948%\n",
      "Epoch 178: Loss: 0.00839, Accuracy: 99.950%\n",
      "Epoch 179: Loss: 0.00829, Accuracy: 99.950%\n",
      "Epoch 180: Loss: 0.00819, Accuracy: 99.950%\n",
      "Epoch 181: Loss: 0.00810, Accuracy: 99.952%\n",
      "Epoch 182: Loss: 0.00800, Accuracy: 99.953%\n",
      "Epoch 183: Loss: 0.00791, Accuracy: 99.955%\n",
      "Epoch 184: Loss: 0.00782, Accuracy: 99.958%\n",
      "Epoch 185: Loss: 0.00773, Accuracy: 99.958%\n",
      "Epoch 186: Loss: 0.00764, Accuracy: 99.958%\n",
      "Epoch 187: Loss: 0.00755, Accuracy: 99.958%\n",
      "Epoch 188: Loss: 0.00747, Accuracy: 99.958%\n",
      "Epoch 189: Loss: 0.00738, Accuracy: 99.958%\n",
      "Epoch 190: Loss: 0.00730, Accuracy: 99.960%\n",
      "Epoch 191: Loss: 0.00722, Accuracy: 99.960%\n",
      "Epoch 192: Loss: 0.00714, Accuracy: 99.962%\n",
      "Epoch 193: Loss: 0.00706, Accuracy: 99.965%\n",
      "Epoch 194: Loss: 0.00698, Accuracy: 99.965%\n",
      "Epoch 195: Loss: 0.00690, Accuracy: 99.965%\n",
      "Epoch 196: Loss: 0.00683, Accuracy: 99.968%\n",
      "Epoch 197: Loss: 0.00675, Accuracy: 99.970%\n",
      "Epoch 198: Loss: 0.00668, Accuracy: 99.970%\n",
      "Epoch 199: Loss: 0.00661, Accuracy: 99.970%\n",
      "Epoch 200: Loss: 0.00654, Accuracy: 99.972%\n",
      "Adagrad\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d950>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d950>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03a1828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.29740, Accuracy: 94.025%\n",
      "Epoch 002: Loss: 0.17763, Accuracy: 95.847%\n",
      "Epoch 003: Loss: 0.14868, Accuracy: 96.462%\n",
      "Epoch 004: Loss: 0.13104, Accuracy: 96.872%\n",
      "Epoch 005: Loss: 0.11856, Accuracy: 97.218%\n",
      "Epoch 006: Loss: 0.10900, Accuracy: 97.440%\n",
      "Epoch 007: Loss: 0.10133, Accuracy: 97.613%\n",
      "Epoch 008: Loss: 0.09495, Accuracy: 97.762%\n",
      "Epoch 009: Loss: 0.08952, Accuracy: 97.925%\n",
      "Epoch 010: Loss: 0.08480, Accuracy: 98.022%\n",
      "Epoch 011: Loss: 0.08065, Accuracy: 98.108%\n",
      "Epoch 012: Loss: 0.07695, Accuracy: 98.223%\n",
      "Epoch 013: Loss: 0.07363, Accuracy: 98.293%\n",
      "Epoch 014: Loss: 0.07062, Accuracy: 98.377%\n",
      "Epoch 015: Loss: 0.06788, Accuracy: 98.445%\n",
      "Epoch 016: Loss: 0.06536, Accuracy: 98.535%\n",
      "Epoch 017: Loss: 0.06303, Accuracy: 98.593%\n",
      "Epoch 018: Loss: 0.06088, Accuracy: 98.625%\n",
      "Epoch 019: Loss: 0.05887, Accuracy: 98.675%\n",
      "Epoch 020: Loss: 0.05699, Accuracy: 98.730%\n",
      "Epoch 021: Loss: 0.05524, Accuracy: 98.778%\n",
      "Epoch 022: Loss: 0.05358, Accuracy: 98.828%\n",
      "Epoch 023: Loss: 0.05202, Accuracy: 98.865%\n",
      "Epoch 024: Loss: 0.05054, Accuracy: 98.900%\n",
      "Epoch 025: Loss: 0.04915, Accuracy: 98.942%\n",
      "Epoch 026: Loss: 0.04783, Accuracy: 98.977%\n",
      "Epoch 027: Loss: 0.04658, Accuracy: 99.025%\n",
      "Epoch 028: Loss: 0.04538, Accuracy: 99.050%\n",
      "Epoch 029: Loss: 0.04424, Accuracy: 99.083%\n",
      "Epoch 030: Loss: 0.04315, Accuracy: 99.102%\n",
      "Epoch 031: Loss: 0.04211, Accuracy: 99.115%\n",
      "Epoch 032: Loss: 0.04112, Accuracy: 99.137%\n",
      "Epoch 033: Loss: 0.04016, Accuracy: 99.157%\n",
      "Epoch 034: Loss: 0.03925, Accuracy: 99.180%\n",
      "Epoch 035: Loss: 0.03837, Accuracy: 99.210%\n",
      "Epoch 036: Loss: 0.03752, Accuracy: 99.242%\n",
      "Epoch 037: Loss: 0.03671, Accuracy: 99.250%\n",
      "Epoch 038: Loss: 0.03592, Accuracy: 99.275%\n",
      "Epoch 039: Loss: 0.03516, Accuracy: 99.290%\n",
      "Epoch 040: Loss: 0.03443, Accuracy: 99.300%\n",
      "Epoch 041: Loss: 0.03373, Accuracy: 99.320%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042: Loss: 0.03305, Accuracy: 99.332%\n",
      "Epoch 043: Loss: 0.03239, Accuracy: 99.343%\n",
      "Epoch 044: Loss: 0.03175, Accuracy: 99.358%\n",
      "Epoch 045: Loss: 0.03114, Accuracy: 99.375%\n",
      "Epoch 046: Loss: 0.03054, Accuracy: 99.395%\n",
      "Epoch 047: Loss: 0.02996, Accuracy: 99.408%\n",
      "Epoch 048: Loss: 0.02940, Accuracy: 99.425%\n",
      "Epoch 049: Loss: 0.02886, Accuracy: 99.435%\n",
      "Epoch 050: Loss: 0.02833, Accuracy: 99.452%\n",
      "Epoch 051: Loss: 0.02782, Accuracy: 99.467%\n",
      "Epoch 052: Loss: 0.02732, Accuracy: 99.482%\n",
      "Epoch 053: Loss: 0.02684, Accuracy: 99.503%\n",
      "Epoch 054: Loss: 0.02637, Accuracy: 99.520%\n",
      "Epoch 055: Loss: 0.02591, Accuracy: 99.538%\n",
      "Epoch 056: Loss: 0.02547, Accuracy: 99.553%\n",
      "Epoch 057: Loss: 0.02504, Accuracy: 99.560%\n",
      "Epoch 058: Loss: 0.02462, Accuracy: 99.568%\n",
      "Epoch 059: Loss: 0.02421, Accuracy: 99.588%\n",
      "Epoch 060: Loss: 0.02381, Accuracy: 99.610%\n",
      "Epoch 061: Loss: 0.02342, Accuracy: 99.620%\n",
      "Epoch 062: Loss: 0.02304, Accuracy: 99.633%\n",
      "Epoch 063: Loss: 0.02267, Accuracy: 99.647%\n",
      "Epoch 064: Loss: 0.02231, Accuracy: 99.657%\n",
      "Epoch 065: Loss: 0.02196, Accuracy: 99.668%\n",
      "Epoch 066: Loss: 0.02162, Accuracy: 99.678%\n",
      "Epoch 067: Loss: 0.02129, Accuracy: 99.685%\n",
      "Epoch 068: Loss: 0.02096, Accuracy: 99.692%\n",
      "Epoch 069: Loss: 0.02064, Accuracy: 99.703%\n",
      "Epoch 070: Loss: 0.02033, Accuracy: 99.718%\n",
      "Epoch 071: Loss: 0.02003, Accuracy: 99.725%\n",
      "Epoch 072: Loss: 0.01973, Accuracy: 99.727%\n",
      "Epoch 073: Loss: 0.01944, Accuracy: 99.738%\n",
      "Epoch 074: Loss: 0.01915, Accuracy: 99.742%\n",
      "Epoch 075: Loss: 0.01888, Accuracy: 99.748%\n",
      "Epoch 076: Loss: 0.01861, Accuracy: 99.760%\n",
      "Epoch 077: Loss: 0.01834, Accuracy: 99.767%\n",
      "Epoch 078: Loss: 0.01808, Accuracy: 99.772%\n",
      "Epoch 079: Loss: 0.01783, Accuracy: 99.778%\n",
      "Epoch 080: Loss: 0.01758, Accuracy: 99.782%\n",
      "Epoch 081: Loss: 0.01733, Accuracy: 99.787%\n",
      "Epoch 082: Loss: 0.01709, Accuracy: 99.790%\n",
      "Epoch 083: Loss: 0.01686, Accuracy: 99.797%\n",
      "Epoch 084: Loss: 0.01663, Accuracy: 99.798%\n",
      "Epoch 085: Loss: 0.01641, Accuracy: 99.807%\n",
      "Epoch 086: Loss: 0.01619, Accuracy: 99.808%\n",
      "Epoch 087: Loss: 0.01597, Accuracy: 99.812%\n",
      "Epoch 088: Loss: 0.01576, Accuracy: 99.815%\n",
      "Epoch 089: Loss: 0.01555, Accuracy: 99.822%\n",
      "Epoch 090: Loss: 0.01535, Accuracy: 99.827%\n",
      "Epoch 091: Loss: 0.01515, Accuracy: 99.832%\n",
      "Epoch 092: Loss: 0.01496, Accuracy: 99.838%\n",
      "Epoch 093: Loss: 0.01477, Accuracy: 99.843%\n",
      "Epoch 094: Loss: 0.01458, Accuracy: 99.845%\n",
      "Epoch 095: Loss: 0.01439, Accuracy: 99.848%\n",
      "Epoch 096: Loss: 0.01421, Accuracy: 99.852%\n",
      "Epoch 097: Loss: 0.01404, Accuracy: 99.853%\n",
      "Epoch 098: Loss: 0.01386, Accuracy: 99.855%\n",
      "Epoch 099: Loss: 0.01369, Accuracy: 99.857%\n",
      "Epoch 100: Loss: 0.01352, Accuracy: 99.860%\n",
      "Epoch 101: Loss: 0.01336, Accuracy: 99.862%\n",
      "Epoch 102: Loss: 0.01320, Accuracy: 99.867%\n",
      "Epoch 103: Loss: 0.01304, Accuracy: 99.867%\n",
      "Epoch 104: Loss: 0.01288, Accuracy: 99.868%\n",
      "Epoch 105: Loss: 0.01273, Accuracy: 99.873%\n",
      "Epoch 106: Loss: 0.01258, Accuracy: 99.878%\n",
      "Epoch 107: Loss: 0.01243, Accuracy: 99.878%\n",
      "Epoch 108: Loss: 0.01228, Accuracy: 99.887%\n",
      "Epoch 109: Loss: 0.01214, Accuracy: 99.890%\n",
      "Epoch 110: Loss: 0.01200, Accuracy: 99.897%\n",
      "Epoch 111: Loss: 0.01186, Accuracy: 99.902%\n",
      "Epoch 112: Loss: 0.01173, Accuracy: 99.902%\n",
      "Epoch 113: Loss: 0.01160, Accuracy: 99.902%\n",
      "Epoch 114: Loss: 0.01147, Accuracy: 99.902%\n",
      "Epoch 115: Loss: 0.01134, Accuracy: 99.902%\n",
      "Epoch 116: Loss: 0.01121, Accuracy: 99.903%\n",
      "Epoch 117: Loss: 0.01109, Accuracy: 99.903%\n",
      "Epoch 118: Loss: 0.01096, Accuracy: 99.905%\n",
      "Epoch 119: Loss: 0.01084, Accuracy: 99.905%\n",
      "Epoch 120: Loss: 0.01073, Accuracy: 99.910%\n",
      "Epoch 121: Loss: 0.01061, Accuracy: 99.913%\n",
      "Epoch 122: Loss: 0.01049, Accuracy: 99.918%\n",
      "Epoch 123: Loss: 0.01038, Accuracy: 99.920%\n",
      "Epoch 124: Loss: 0.01027, Accuracy: 99.922%\n",
      "Epoch 125: Loss: 0.01016, Accuracy: 99.922%\n",
      "Epoch 126: Loss: 0.01006, Accuracy: 99.927%\n",
      "Epoch 127: Loss: 0.00995, Accuracy: 99.930%\n",
      "Epoch 128: Loss: 0.00985, Accuracy: 99.930%\n",
      "Epoch 129: Loss: 0.00974, Accuracy: 99.930%\n",
      "Epoch 130: Loss: 0.00964, Accuracy: 99.932%\n",
      "Epoch 131: Loss: 0.00954, Accuracy: 99.933%\n",
      "Epoch 132: Loss: 0.00945, Accuracy: 99.937%\n",
      "Epoch 133: Loss: 0.00935, Accuracy: 99.938%\n",
      "Epoch 134: Loss: 0.00926, Accuracy: 99.938%\n",
      "Epoch 135: Loss: 0.00916, Accuracy: 99.940%\n",
      "Epoch 136: Loss: 0.00907, Accuracy: 99.942%\n",
      "Epoch 137: Loss: 0.00898, Accuracy: 99.945%\n",
      "Epoch 138: Loss: 0.00889, Accuracy: 99.948%\n",
      "Epoch 139: Loss: 0.00880, Accuracy: 99.948%\n",
      "Epoch 140: Loss: 0.00872, Accuracy: 99.948%\n",
      "Epoch 141: Loss: 0.00863, Accuracy: 99.948%\n",
      "Epoch 142: Loss: 0.00855, Accuracy: 99.948%\n",
      "Epoch 143: Loss: 0.00847, Accuracy: 99.948%\n",
      "Epoch 144: Loss: 0.00838, Accuracy: 99.948%\n",
      "Epoch 145: Loss: 0.00830, Accuracy: 99.948%\n",
      "Epoch 146: Loss: 0.00822, Accuracy: 99.950%\n",
      "Epoch 147: Loss: 0.00815, Accuracy: 99.950%\n",
      "Epoch 148: Loss: 0.00807, Accuracy: 99.950%\n",
      "Epoch 149: Loss: 0.00799, Accuracy: 99.950%\n",
      "Epoch 150: Loss: 0.00792, Accuracy: 99.950%\n",
      "Epoch 151: Loss: 0.00785, Accuracy: 99.953%\n",
      "Epoch 152: Loss: 0.00777, Accuracy: 99.955%\n",
      "Epoch 153: Loss: 0.00770, Accuracy: 99.955%\n",
      "Epoch 154: Loss: 0.00763, Accuracy: 99.955%\n",
      "Epoch 155: Loss: 0.00756, Accuracy: 99.957%\n",
      "Epoch 156: Loss: 0.00749, Accuracy: 99.960%\n",
      "Epoch 157: Loss: 0.00743, Accuracy: 99.963%\n",
      "Epoch 158: Loss: 0.00736, Accuracy: 99.963%\n",
      "Epoch 159: Loss: 0.00729, Accuracy: 99.963%\n",
      "Epoch 160: Loss: 0.00723, Accuracy: 99.963%\n",
      "Epoch 161: Loss: 0.00716, Accuracy: 99.965%\n",
      "Epoch 162: Loss: 0.00710, Accuracy: 99.965%\n",
      "Epoch 163: Loss: 0.00704, Accuracy: 99.965%\n",
      "Epoch 164: Loss: 0.00698, Accuracy: 99.965%\n",
      "Epoch 165: Loss: 0.00692, Accuracy: 99.967%\n",
      "Epoch 166: Loss: 0.00686, Accuracy: 99.967%\n",
      "Epoch 167: Loss: 0.00680, Accuracy: 99.968%\n",
      "Epoch 168: Loss: 0.00674, Accuracy: 99.968%\n",
      "Epoch 169: Loss: 0.00668, Accuracy: 99.968%\n",
      "Epoch 170: Loss: 0.00663, Accuracy: 99.968%\n",
      "Epoch 171: Loss: 0.00657, Accuracy: 99.970%\n",
      "Epoch 172: Loss: 0.00651, Accuracy: 99.970%\n",
      "Epoch 173: Loss: 0.00646, Accuracy: 99.972%\n",
      "Epoch 174: Loss: 0.00641, Accuracy: 99.972%\n",
      "Epoch 175: Loss: 0.00635, Accuracy: 99.972%\n",
      "Epoch 176: Loss: 0.00630, Accuracy: 99.972%\n",
      "Epoch 177: Loss: 0.00625, Accuracy: 99.973%\n",
      "Epoch 178: Loss: 0.00620, Accuracy: 99.975%\n",
      "Epoch 179: Loss: 0.00615, Accuracy: 99.977%\n",
      "Epoch 180: Loss: 0.00610, Accuracy: 99.977%\n",
      "Epoch 181: Loss: 0.00605, Accuracy: 99.977%\n",
      "Epoch 182: Loss: 0.00600, Accuracy: 99.977%\n",
      "Epoch 183: Loss: 0.00595, Accuracy: 99.977%\n",
      "Epoch 184: Loss: 0.00590, Accuracy: 99.977%\n",
      "Epoch 185: Loss: 0.00586, Accuracy: 99.977%\n",
      "Epoch 186: Loss: 0.00581, Accuracy: 99.977%\n",
      "Epoch 187: Loss: 0.00577, Accuracy: 99.977%\n",
      "Epoch 188: Loss: 0.00572, Accuracy: 99.977%\n",
      "Epoch 189: Loss: 0.00568, Accuracy: 99.977%\n",
      "Epoch 190: Loss: 0.00563, Accuracy: 99.977%\n",
      "Epoch 191: Loss: 0.00559, Accuracy: 99.977%\n",
      "Epoch 192: Loss: 0.00555, Accuracy: 99.978%\n",
      "Epoch 193: Loss: 0.00550, Accuracy: 99.980%\n",
      "Epoch 194: Loss: 0.00546, Accuracy: 99.980%\n",
      "Epoch 195: Loss: 0.00542, Accuracy: 99.980%\n",
      "Epoch 196: Loss: 0.00538, Accuracy: 99.980%\n",
      "Epoch 197: Loss: 0.00534, Accuracy: 99.980%\n",
      "Epoch 198: Loss: 0.00530, Accuracy: 99.983%\n",
      "Epoch 199: Loss: 0.00526, Accuracy: 99.983%\n",
      "Epoch 200: Loss: 0.00522, Accuracy: 99.983%\n",
      "RMSProp\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd35aec3ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd35aec3ea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd35aec3ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd35aec3ea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f00f6be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.39641, Accuracy: 92.115%\n",
      "Epoch 002: Loss: 0.18985, Accuracy: 96.475%\n",
      "Epoch 003: Loss: 0.13881, Accuracy: 97.742%\n",
      "Epoch 004: Loss: 0.10871, Accuracy: 98.447%\n",
      "Epoch 005: Loss: 0.08825, Accuracy: 98.892%\n",
      "Epoch 006: Loss: 0.07312, Accuracy: 99.192%\n",
      "Epoch 007: Loss: 0.06145, Accuracy: 99.412%\n",
      "Epoch 008: Loss: 0.05212, Accuracy: 99.575%\n",
      "Epoch 009: Loss: 0.04449, Accuracy: 99.693%\n",
      "Epoch 010: Loss: 0.03814, Accuracy: 99.788%\n",
      "Epoch 011: Loss: 0.03275, Accuracy: 99.835%\n",
      "Epoch 012: Loss: 0.02816, Accuracy: 99.885%\n",
      "Epoch 013: Loss: 0.02424, Accuracy: 99.915%\n",
      "Epoch 014: Loss: 0.02089, Accuracy: 99.942%\n",
      "Epoch 015: Loss: 0.01802, Accuracy: 99.957%\n",
      "Epoch 016: Loss: 0.01559, Accuracy: 99.970%\n",
      "Epoch 017: Loss: 0.01350, Accuracy: 99.977%\n",
      "Epoch 018: Loss: 0.01173, Accuracy: 99.985%\n",
      "Epoch 019: Loss: 0.01025, Accuracy: 99.987%\n",
      "Epoch 020: Loss: 0.00899, Accuracy: 99.992%\n",
      "Epoch 021: Loss: 0.00794, Accuracy: 99.992%\n",
      "Epoch 022: Loss: 0.00705, Accuracy: 99.993%\n",
      "Epoch 023: Loss: 0.00630, Accuracy: 99.995%\n",
      "Epoch 024: Loss: 0.00566, Accuracy: 99.997%\n",
      "Epoch 025: Loss: 0.00511, Accuracy: 99.998%\n",
      "Epoch 026: Loss: 0.00464, Accuracy: 100.000%\n",
      "Epoch 027: Loss: 0.00423, Accuracy: 100.000%\n",
      "Epoch 028: Loss: 0.00387, Accuracy: 100.000%\n",
      "Epoch 029: Loss: 0.00355, Accuracy: 100.000%\n",
      "Epoch 030: Loss: 0.00327, Accuracy: 100.000%\n",
      "Epoch 031: Loss: 0.00302, Accuracy: 100.000%\n",
      "Epoch 032: Loss: 0.00281, Accuracy: 100.000%\n",
      "Epoch 033: Loss: 0.00262, Accuracy: 100.000%\n",
      "Epoch 034: Loss: 0.00246, Accuracy: 100.000%\n",
      "Epoch 035: Loss: 0.00231, Accuracy: 100.000%\n",
      "Epoch 036: Loss: 0.00218, Accuracy: 100.000%\n",
      "Epoch 037: Loss: 0.00206, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00195, Accuracy: 100.000%\n",
      "Epoch 039: Loss: 0.00185, Accuracy: 100.000%\n",
      "Epoch 040: Loss: 0.00176, Accuracy: 100.000%\n",
      "Epoch 041: Loss: 0.00167, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00160, Accuracy: 100.000%\n",
      "Epoch 043: Loss: 0.00153, Accuracy: 100.000%\n",
      "Epoch 044: Loss: 0.00146, Accuracy: 100.000%\n",
      "Epoch 045: Loss: 0.00140, Accuracy: 100.000%\n",
      "Epoch 046: Loss: 0.00134, Accuracy: 100.000%\n",
      "Epoch 047: Loss: 0.00129, Accuracy: 100.000%\n",
      "Epoch 048: Loss: 0.00124, Accuracy: 100.000%\n",
      "Epoch 049: Loss: 0.00120, Accuracy: 100.000%\n",
      "Epoch 050: Loss: 0.00115, Accuracy: 100.000%\n",
      "Epoch 051: Loss: 0.00111, Accuracy: 100.000%\n",
      "Epoch 052: Loss: 0.00108, Accuracy: 100.000%\n",
      "Epoch 053: Loss: 0.00104, Accuracy: 100.000%\n",
      "Epoch 054: Loss: 0.00101, Accuracy: 100.000%\n",
      "Epoch 055: Loss: 0.00098, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00095, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00092, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00089, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00086, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00084, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00082, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00079, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 063: Loss: 0.00077, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00075, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00073, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00072, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00070, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00068, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00067, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00065, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00064, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00062, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00061, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00060, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00058, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00057, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00056, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00055, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00054, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00053, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00052, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00051, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00050, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00049, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00048, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00047, Accuracy: 100.000%\n",
      "Epoch 087: Loss: 0.00046, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00045, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00045, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00044, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00043, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00042, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00042, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00041, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00040, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00040, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00039, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00039, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00038, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00037, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00037, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 107: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 150: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 172: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 186: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00014, Accuracy: 100.000%\n",
      "Adam\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d08>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d08>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f02becc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.17353, Accuracy: 95.883%\n",
      "Epoch 002: Loss: 0.05727, Accuracy: 98.655%\n",
      "Epoch 003: Loss: 0.03191, Accuracy: 99.412%\n",
      "Epoch 004: Loss: 0.01841, Accuracy: 99.750%\n",
      "Epoch 005: Loss: 0.01145, Accuracy: 99.882%\n",
      "Epoch 006: Loss: 0.00648, Accuracy: 99.965%\n",
      "Epoch 007: Loss: 0.00571, Accuracy: 99.970%\n",
      "Epoch 008: Loss: 0.00654, Accuracy: 99.977%\n",
      "Epoch 009: Loss: 0.00699, Accuracy: 99.982%\n",
      "Epoch 010: Loss: 0.00671, Accuracy: 99.987%\n",
      "Epoch 011: Loss: 0.00536, Accuracy: 99.990%\n",
      "Epoch 012: Loss: 0.00364, Accuracy: 99.993%\n",
      "Epoch 013: Loss: 0.00277, Accuracy: 99.993%\n",
      "Epoch 014: Loss: 0.00447, Accuracy: 99.988%\n",
      "Epoch 015: Loss: 0.00460, Accuracy: 99.990%\n",
      "Epoch 016: Loss: 0.00310, Accuracy: 99.997%\n",
      "Epoch 017: Loss: 0.00609, Accuracy: 99.982%\n",
      "Epoch 018: Loss: 0.00262, Accuracy: 99.995%\n",
      "Epoch 019: Loss: 0.00184, Accuracy: 99.993%\n",
      "Epoch 020: Loss: 0.00097, Accuracy: 99.998%\n",
      "Epoch 021: Loss: 0.00143, Accuracy: 99.997%\n",
      "Epoch 022: Loss: 0.00967, Accuracy: 99.970%\n",
      "Epoch 023: Loss: 0.00288, Accuracy: 99.993%\n",
      "Epoch 024: Loss: 0.00045, Accuracy: 100.000%\n",
      "Epoch 025: Loss: 0.00037, Accuracy: 99.998%\n",
      "Epoch 026: Loss: 0.00178, Accuracy: 99.998%\n",
      "Epoch 027: Loss: 0.00884, Accuracy: 99.967%\n",
      "Epoch 028: Loss: 0.00406, Accuracy: 99.990%\n",
      "Epoch 029: Loss: 0.00167, Accuracy: 99.995%\n",
      "Epoch 030: Loss: 0.00082, Accuracy: 100.000%\n",
      "Epoch 031: Loss: 0.00421, Accuracy: 99.985%\n",
      "Epoch 032: Loss: 0.00457, Accuracy: 99.983%\n",
      "Epoch 033: Loss: 0.00320, Accuracy: 99.995%\n",
      "Epoch 034: Loss: 0.00179, Accuracy: 99.997%\n",
      "Epoch 035: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 036: Loss: 0.00006, Accuracy: 100.000%\n",
      "Epoch 037: Loss: 0.00002, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 039: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 040: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 041: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 043: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 044: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 045: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 046: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 047: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 048: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 049: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 050: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 051: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 052: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 053: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 054: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 055: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 063: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00000, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 087: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 107: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 150: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 172: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 186: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00000, Accuracy: 100.000%\n",
      "Trial 4\n",
      "SGD Nesterov\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d1e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d1e0>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d1e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f035d1e0>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35aec44e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 1.10936, Accuracy: 75.970%\n",
      "Epoch 002: Loss: 0.43900, Accuracy: 88.650%\n",
      "Epoch 003: Loss: 0.34847, Accuracy: 90.402%\n",
      "Epoch 004: Loss: 0.30800, Accuracy: 91.358%\n",
      "Epoch 005: Loss: 0.28205, Accuracy: 92.093%\n",
      "Epoch 006: Loss: 0.26262, Accuracy: 92.638%\n",
      "Epoch 007: Loss: 0.24681, Accuracy: 93.110%\n",
      "Epoch 008: Loss: 0.23334, Accuracy: 93.488%\n",
      "Epoch 009: Loss: 0.22152, Accuracy: 93.827%\n",
      "Epoch 010: Loss: 0.21096, Accuracy: 94.125%\n",
      "Epoch 011: Loss: 0.20137, Accuracy: 94.408%\n",
      "Epoch 012: Loss: 0.19260, Accuracy: 94.668%\n",
      "Epoch 013: Loss: 0.18455, Accuracy: 94.890%\n",
      "Epoch 014: Loss: 0.17712, Accuracy: 95.092%\n",
      "Epoch 015: Loss: 0.17022, Accuracy: 95.260%\n",
      "Epoch 016: Loss: 0.16380, Accuracy: 95.437%\n",
      "Epoch 017: Loss: 0.15782, Accuracy: 95.610%\n",
      "Epoch 018: Loss: 0.15222, Accuracy: 95.757%\n",
      "Epoch 019: Loss: 0.14695, Accuracy: 95.922%\n",
      "Epoch 020: Loss: 0.14199, Accuracy: 96.063%\n",
      "Epoch 021: Loss: 0.13731, Accuracy: 96.215%\n",
      "Epoch 022: Loss: 0.13290, Accuracy: 96.345%\n",
      "Epoch 023: Loss: 0.12872, Accuracy: 96.467%\n",
      "Epoch 024: Loss: 0.12477, Accuracy: 96.593%\n",
      "Epoch 025: Loss: 0.12101, Accuracy: 96.712%\n",
      "Epoch 026: Loss: 0.11744, Accuracy: 96.823%\n",
      "Epoch 027: Loss: 0.11403, Accuracy: 96.905%\n",
      "Epoch 028: Loss: 0.11078, Accuracy: 96.995%\n",
      "Epoch 029: Loss: 0.10768, Accuracy: 97.067%\n",
      "Epoch 030: Loss: 0.10471, Accuracy: 97.143%\n",
      "Epoch 031: Loss: 0.10187, Accuracy: 97.235%\n",
      "Epoch 032: Loss: 0.09915, Accuracy: 97.313%\n",
      "Epoch 033: Loss: 0.09654, Accuracy: 97.390%\n",
      "Epoch 034: Loss: 0.09404, Accuracy: 97.452%\n",
      "Epoch 035: Loss: 0.09163, Accuracy: 97.538%\n",
      "Epoch 036: Loss: 0.08932, Accuracy: 97.595%\n",
      "Epoch 037: Loss: 0.08709, Accuracy: 97.670%\n",
      "Epoch 038: Loss: 0.08495, Accuracy: 97.745%\n",
      "Epoch 039: Loss: 0.08288, Accuracy: 97.815%\n",
      "Epoch 040: Loss: 0.08088, Accuracy: 97.875%\n",
      "Epoch 041: Loss: 0.07896, Accuracy: 97.937%\n",
      "Epoch 042: Loss: 0.07710, Accuracy: 97.993%\n",
      "Epoch 043: Loss: 0.07531, Accuracy: 98.060%\n",
      "Epoch 044: Loss: 0.07357, Accuracy: 98.100%\n",
      "Epoch 045: Loss: 0.07189, Accuracy: 98.148%\n",
      "Epoch 046: Loss: 0.07027, Accuracy: 98.190%\n",
      "Epoch 047: Loss: 0.06870, Accuracy: 98.235%\n",
      "Epoch 048: Loss: 0.06718, Accuracy: 98.282%\n",
      "Epoch 049: Loss: 0.06570, Accuracy: 98.327%\n",
      "Epoch 050: Loss: 0.06427, Accuracy: 98.348%\n",
      "Epoch 051: Loss: 0.06289, Accuracy: 98.392%\n",
      "Epoch 052: Loss: 0.06155, Accuracy: 98.423%\n",
      "Epoch 053: Loss: 0.06024, Accuracy: 98.463%\n",
      "Epoch 054: Loss: 0.05898, Accuracy: 98.493%\n",
      "Epoch 055: Loss: 0.05775, Accuracy: 98.528%\n",
      "Epoch 056: Loss: 0.05656, Accuracy: 98.557%\n",
      "Epoch 057: Loss: 0.05540, Accuracy: 98.580%\n",
      "Epoch 058: Loss: 0.05428, Accuracy: 98.610%\n",
      "Epoch 059: Loss: 0.05318, Accuracy: 98.647%\n",
      "Epoch 060: Loss: 0.05212, Accuracy: 98.682%\n",
      "Epoch 061: Loss: 0.05108, Accuracy: 98.715%\n",
      "Epoch 062: Loss: 0.05007, Accuracy: 98.745%\n",
      "Epoch 063: Loss: 0.04909, Accuracy: 98.773%\n",
      "Epoch 064: Loss: 0.04813, Accuracy: 98.802%\n",
      "Epoch 065: Loss: 0.04720, Accuracy: 98.820%\n",
      "Epoch 066: Loss: 0.04629, Accuracy: 98.852%\n",
      "Epoch 067: Loss: 0.04541, Accuracy: 98.875%\n",
      "Epoch 068: Loss: 0.04454, Accuracy: 98.910%\n",
      "Epoch 069: Loss: 0.04370, Accuracy: 98.943%\n",
      "Epoch 070: Loss: 0.04288, Accuracy: 98.967%\n",
      "Epoch 071: Loss: 0.04208, Accuracy: 98.980%\n",
      "Epoch 072: Loss: 0.04129, Accuracy: 99.003%\n",
      "Epoch 073: Loss: 0.04053, Accuracy: 99.027%\n",
      "Epoch 074: Loss: 0.03978, Accuracy: 99.052%\n",
      "Epoch 075: Loss: 0.03905, Accuracy: 99.078%\n",
      "Epoch 076: Loss: 0.03834, Accuracy: 99.102%\n",
      "Epoch 077: Loss: 0.03764, Accuracy: 99.132%\n",
      "Epoch 078: Loss: 0.03696, Accuracy: 99.147%\n",
      "Epoch 079: Loss: 0.03630, Accuracy: 99.173%\n",
      "Epoch 080: Loss: 0.03565, Accuracy: 99.197%\n",
      "Epoch 081: Loss: 0.03501, Accuracy: 99.203%\n",
      "Epoch 082: Loss: 0.03439, Accuracy: 99.218%\n",
      "Epoch 083: Loss: 0.03378, Accuracy: 99.233%\n",
      "Epoch 084: Loss: 0.03318, Accuracy: 99.253%\n",
      "Epoch 085: Loss: 0.03260, Accuracy: 99.267%\n",
      "Epoch 086: Loss: 0.03203, Accuracy: 99.292%\n",
      "Epoch 087: Loss: 0.03147, Accuracy: 99.305%\n",
      "Epoch 088: Loss: 0.03093, Accuracy: 99.318%\n",
      "Epoch 089: Loss: 0.03039, Accuracy: 99.332%\n",
      "Epoch 090: Loss: 0.02987, Accuracy: 99.358%\n",
      "Epoch 091: Loss: 0.02936, Accuracy: 99.367%\n",
      "Epoch 092: Loss: 0.02886, Accuracy: 99.382%\n",
      "Epoch 093: Loss: 0.02837, Accuracy: 99.390%\n",
      "Epoch 094: Loss: 0.02788, Accuracy: 99.398%\n",
      "Epoch 095: Loss: 0.02742, Accuracy: 99.413%\n",
      "Epoch 096: Loss: 0.02695, Accuracy: 99.430%\n",
      "Epoch 097: Loss: 0.02650, Accuracy: 99.443%\n",
      "Epoch 098: Loss: 0.02606, Accuracy: 99.458%\n",
      "Epoch 099: Loss: 0.02563, Accuracy: 99.482%\n",
      "Epoch 100: Loss: 0.02520, Accuracy: 99.500%\n",
      "Epoch 101: Loss: 0.02479, Accuracy: 99.520%\n",
      "Epoch 102: Loss: 0.02438, Accuracy: 99.532%\n",
      "Epoch 103: Loss: 0.02398, Accuracy: 99.548%\n",
      "Epoch 104: Loss: 0.02359, Accuracy: 99.565%\n",
      "Epoch 105: Loss: 0.02321, Accuracy: 99.580%\n",
      "Epoch 106: Loss: 0.02284, Accuracy: 99.600%\n",
      "Epoch 107: Loss: 0.02247, Accuracy: 99.613%\n",
      "Epoch 108: Loss: 0.02211, Accuracy: 99.627%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109: Loss: 0.02175, Accuracy: 99.647%\n",
      "Epoch 110: Loss: 0.02141, Accuracy: 99.655%\n",
      "Epoch 111: Loss: 0.02107, Accuracy: 99.668%\n",
      "Epoch 112: Loss: 0.02074, Accuracy: 99.675%\n",
      "Epoch 113: Loss: 0.02041, Accuracy: 99.695%\n",
      "Epoch 114: Loss: 0.02009, Accuracy: 99.703%\n",
      "Epoch 115: Loss: 0.01978, Accuracy: 99.710%\n",
      "Epoch 116: Loss: 0.01947, Accuracy: 99.715%\n",
      "Epoch 117: Loss: 0.01917, Accuracy: 99.723%\n",
      "Epoch 118: Loss: 0.01887, Accuracy: 99.728%\n",
      "Epoch 119: Loss: 0.01858, Accuracy: 99.740%\n",
      "Epoch 120: Loss: 0.01829, Accuracy: 99.743%\n",
      "Epoch 121: Loss: 0.01802, Accuracy: 99.748%\n",
      "Epoch 122: Loss: 0.01774, Accuracy: 99.752%\n",
      "Epoch 123: Loss: 0.01747, Accuracy: 99.757%\n",
      "Epoch 124: Loss: 0.01721, Accuracy: 99.760%\n",
      "Epoch 125: Loss: 0.01695, Accuracy: 99.765%\n",
      "Epoch 126: Loss: 0.01670, Accuracy: 99.767%\n",
      "Epoch 127: Loss: 0.01645, Accuracy: 99.773%\n",
      "Epoch 128: Loss: 0.01621, Accuracy: 99.787%\n",
      "Epoch 129: Loss: 0.01597, Accuracy: 99.788%\n",
      "Epoch 130: Loss: 0.01573, Accuracy: 99.793%\n",
      "Epoch 131: Loss: 0.01550, Accuracy: 99.795%\n",
      "Epoch 132: Loss: 0.01528, Accuracy: 99.802%\n",
      "Epoch 133: Loss: 0.01505, Accuracy: 99.808%\n",
      "Epoch 134: Loss: 0.01484, Accuracy: 99.815%\n",
      "Epoch 135: Loss: 0.01462, Accuracy: 99.828%\n",
      "Epoch 136: Loss: 0.01441, Accuracy: 99.835%\n",
      "Epoch 137: Loss: 0.01421, Accuracy: 99.840%\n",
      "Epoch 138: Loss: 0.01401, Accuracy: 99.848%\n",
      "Epoch 139: Loss: 0.01381, Accuracy: 99.852%\n",
      "Epoch 140: Loss: 0.01362, Accuracy: 99.852%\n",
      "Epoch 141: Loss: 0.01342, Accuracy: 99.853%\n",
      "Epoch 142: Loss: 0.01324, Accuracy: 99.858%\n",
      "Epoch 143: Loss: 0.01305, Accuracy: 99.865%\n",
      "Epoch 144: Loss: 0.01287, Accuracy: 99.870%\n",
      "Epoch 145: Loss: 0.01269, Accuracy: 99.873%\n",
      "Epoch 146: Loss: 0.01252, Accuracy: 99.877%\n",
      "Epoch 147: Loss: 0.01235, Accuracy: 99.880%\n",
      "Epoch 148: Loss: 0.01218, Accuracy: 99.883%\n",
      "Epoch 149: Loss: 0.01202, Accuracy: 99.885%\n",
      "Epoch 150: Loss: 0.01185, Accuracy: 99.888%\n",
      "Epoch 151: Loss: 0.01170, Accuracy: 99.888%\n",
      "Epoch 152: Loss: 0.01154, Accuracy: 99.893%\n",
      "Epoch 153: Loss: 0.01139, Accuracy: 99.898%\n",
      "Epoch 154: Loss: 0.01124, Accuracy: 99.898%\n",
      "Epoch 155: Loss: 0.01109, Accuracy: 99.902%\n",
      "Epoch 156: Loss: 0.01094, Accuracy: 99.903%\n",
      "Epoch 157: Loss: 0.01080, Accuracy: 99.905%\n",
      "Epoch 158: Loss: 0.01066, Accuracy: 99.907%\n",
      "Epoch 159: Loss: 0.01052, Accuracy: 99.908%\n",
      "Epoch 160: Loss: 0.01039, Accuracy: 99.908%\n",
      "Epoch 161: Loss: 0.01025, Accuracy: 99.910%\n",
      "Epoch 162: Loss: 0.01012, Accuracy: 99.913%\n",
      "Epoch 163: Loss: 0.00999, Accuracy: 99.917%\n",
      "Epoch 164: Loss: 0.00987, Accuracy: 99.917%\n",
      "Epoch 165: Loss: 0.00974, Accuracy: 99.923%\n",
      "Epoch 166: Loss: 0.00962, Accuracy: 99.925%\n",
      "Epoch 167: Loss: 0.00950, Accuracy: 99.930%\n",
      "Epoch 168: Loss: 0.00939, Accuracy: 99.930%\n",
      "Epoch 169: Loss: 0.00927, Accuracy: 99.932%\n",
      "Epoch 170: Loss: 0.00916, Accuracy: 99.937%\n",
      "Epoch 171: Loss: 0.00905, Accuracy: 99.940%\n",
      "Epoch 172: Loss: 0.00894, Accuracy: 99.943%\n",
      "Epoch 173: Loss: 0.00883, Accuracy: 99.943%\n",
      "Epoch 174: Loss: 0.00872, Accuracy: 99.945%\n",
      "Epoch 175: Loss: 0.00862, Accuracy: 99.948%\n",
      "Epoch 176: Loss: 0.00852, Accuracy: 99.950%\n",
      "Epoch 177: Loss: 0.00842, Accuracy: 99.952%\n",
      "Epoch 178: Loss: 0.00832, Accuracy: 99.953%\n",
      "Epoch 179: Loss: 0.00822, Accuracy: 99.955%\n",
      "Epoch 180: Loss: 0.00813, Accuracy: 99.957%\n",
      "Epoch 181: Loss: 0.00803, Accuracy: 99.957%\n",
      "Epoch 182: Loss: 0.00794, Accuracy: 99.958%\n",
      "Epoch 183: Loss: 0.00785, Accuracy: 99.958%\n",
      "Epoch 184: Loss: 0.00776, Accuracy: 99.958%\n",
      "Epoch 185: Loss: 0.00767, Accuracy: 99.958%\n",
      "Epoch 186: Loss: 0.00759, Accuracy: 99.958%\n",
      "Epoch 187: Loss: 0.00750, Accuracy: 99.960%\n",
      "Epoch 188: Loss: 0.00742, Accuracy: 99.960%\n",
      "Epoch 189: Loss: 0.00733, Accuracy: 99.960%\n",
      "Epoch 190: Loss: 0.00725, Accuracy: 99.962%\n",
      "Epoch 191: Loss: 0.00717, Accuracy: 99.962%\n",
      "Epoch 192: Loss: 0.00709, Accuracy: 99.963%\n",
      "Epoch 193: Loss: 0.00702, Accuracy: 99.965%\n",
      "Epoch 194: Loss: 0.00694, Accuracy: 99.967%\n",
      "Epoch 195: Loss: 0.00687, Accuracy: 99.968%\n",
      "Epoch 196: Loss: 0.00679, Accuracy: 99.970%\n",
      "Epoch 197: Loss: 0.00672, Accuracy: 99.972%\n",
      "Epoch 198: Loss: 0.00665, Accuracy: 99.973%\n",
      "Epoch 199: Loss: 0.00658, Accuracy: 99.973%\n",
      "Epoch 200: Loss: 0.00651, Accuracy: 99.973%\n",
      "Adagrad\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc598>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3f00fc598>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46daef780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.29848, Accuracy: 94.017%\n",
      "Epoch 002: Loss: 0.17847, Accuracy: 95.805%\n",
      "Epoch 003: Loss: 0.14981, Accuracy: 96.430%\n",
      "Epoch 004: Loss: 0.13245, Accuracy: 96.838%\n",
      "Epoch 005: Loss: 0.12015, Accuracy: 97.132%\n",
      "Epoch 006: Loss: 0.11069, Accuracy: 97.343%\n",
      "Epoch 007: Loss: 0.10303, Accuracy: 97.545%\n",
      "Epoch 008: Loss: 0.09663, Accuracy: 97.682%\n",
      "Epoch 009: Loss: 0.09115, Accuracy: 97.840%\n",
      "Epoch 010: Loss: 0.08638, Accuracy: 97.960%\n",
      "Epoch 011: Loss: 0.08217, Accuracy: 98.050%\n",
      "Epoch 012: Loss: 0.07842, Accuracy: 98.132%\n",
      "Epoch 013: Loss: 0.07504, Accuracy: 98.208%\n",
      "Epoch 014: Loss: 0.07197, Accuracy: 98.288%\n",
      "Epoch 015: Loss: 0.06916, Accuracy: 98.372%\n",
      "Epoch 016: Loss: 0.06658, Accuracy: 98.440%\n",
      "Epoch 017: Loss: 0.06419, Accuracy: 98.512%\n",
      "Epoch 018: Loss: 0.06198, Accuracy: 98.568%\n",
      "Epoch 019: Loss: 0.05993, Accuracy: 98.620%\n",
      "Epoch 020: Loss: 0.05800, Accuracy: 98.677%\n",
      "Epoch 021: Loss: 0.05620, Accuracy: 98.742%\n",
      "Epoch 022: Loss: 0.05452, Accuracy: 98.780%\n",
      "Epoch 023: Loss: 0.05292, Accuracy: 98.817%\n",
      "Epoch 024: Loss: 0.05142, Accuracy: 98.862%\n",
      "Epoch 025: Loss: 0.04999, Accuracy: 98.893%\n",
      "Epoch 026: Loss: 0.04864, Accuracy: 98.930%\n",
      "Epoch 027: Loss: 0.04736, Accuracy: 98.957%\n",
      "Epoch 028: Loss: 0.04613, Accuracy: 99.000%\n",
      "Epoch 029: Loss: 0.04497, Accuracy: 99.030%\n",
      "Epoch 030: Loss: 0.04386, Accuracy: 99.058%\n",
      "Epoch 031: Loss: 0.04279, Accuracy: 99.097%\n",
      "Epoch 032: Loss: 0.04178, Accuracy: 99.130%\n",
      "Epoch 033: Loss: 0.04081, Accuracy: 99.160%\n",
      "Epoch 034: Loss: 0.03988, Accuracy: 99.193%\n",
      "Epoch 035: Loss: 0.03898, Accuracy: 99.213%\n",
      "Epoch 036: Loss: 0.03812, Accuracy: 99.235%\n",
      "Epoch 037: Loss: 0.03729, Accuracy: 99.248%\n",
      "Epoch 038: Loss: 0.03649, Accuracy: 99.272%\n",
      "Epoch 039: Loss: 0.03572, Accuracy: 99.293%\n",
      "Epoch 040: Loss: 0.03497, Accuracy: 99.308%\n",
      "Epoch 041: Loss: 0.03425, Accuracy: 99.320%\n",
      "Epoch 042: Loss: 0.03356, Accuracy: 99.342%\n",
      "Epoch 043: Loss: 0.03289, Accuracy: 99.365%\n",
      "Epoch 044: Loss: 0.03224, Accuracy: 99.385%\n",
      "Epoch 045: Loss: 0.03162, Accuracy: 99.403%\n",
      "Epoch 046: Loss: 0.03101, Accuracy: 99.417%\n",
      "Epoch 047: Loss: 0.03042, Accuracy: 99.432%\n",
      "Epoch 048: Loss: 0.02985, Accuracy: 99.453%\n",
      "Epoch 049: Loss: 0.02930, Accuracy: 99.468%\n",
      "Epoch 050: Loss: 0.02877, Accuracy: 99.477%\n",
      "Epoch 051: Loss: 0.02824, Accuracy: 99.492%\n",
      "Epoch 052: Loss: 0.02774, Accuracy: 99.500%\n",
      "Epoch 053: Loss: 0.02725, Accuracy: 99.515%\n",
      "Epoch 054: Loss: 0.02677, Accuracy: 99.533%\n",
      "Epoch 055: Loss: 0.02631, Accuracy: 99.542%\n",
      "Epoch 056: Loss: 0.02586, Accuracy: 99.550%\n",
      "Epoch 057: Loss: 0.02542, Accuracy: 99.560%\n",
      "Epoch 058: Loss: 0.02499, Accuracy: 99.570%\n",
      "Epoch 059: Loss: 0.02458, Accuracy: 99.588%\n",
      "Epoch 060: Loss: 0.02418, Accuracy: 99.603%\n",
      "Epoch 061: Loss: 0.02378, Accuracy: 99.615%\n",
      "Epoch 062: Loss: 0.02340, Accuracy: 99.633%\n",
      "Epoch 063: Loss: 0.02303, Accuracy: 99.643%\n",
      "Epoch 064: Loss: 0.02266, Accuracy: 99.660%\n",
      "Epoch 065: Loss: 0.02231, Accuracy: 99.673%\n",
      "Epoch 066: Loss: 0.02196, Accuracy: 99.683%\n",
      "Epoch 067: Loss: 0.02162, Accuracy: 99.695%\n",
      "Epoch 068: Loss: 0.02129, Accuracy: 99.703%\n",
      "Epoch 069: Loss: 0.02097, Accuracy: 99.708%\n",
      "Epoch 070: Loss: 0.02066, Accuracy: 99.720%\n",
      "Epoch 071: Loss: 0.02035, Accuracy: 99.735%\n",
      "Epoch 072: Loss: 0.02005, Accuracy: 99.743%\n",
      "Epoch 073: Loss: 0.01975, Accuracy: 99.745%\n",
      "Epoch 074: Loss: 0.01947, Accuracy: 99.753%\n",
      "Epoch 075: Loss: 0.01919, Accuracy: 99.760%\n",
      "Epoch 076: Loss: 0.01891, Accuracy: 99.760%\n",
      "Epoch 077: Loss: 0.01865, Accuracy: 99.767%\n",
      "Epoch 078: Loss: 0.01838, Accuracy: 99.770%\n",
      "Epoch 079: Loss: 0.01813, Accuracy: 99.775%\n",
      "Epoch 080: Loss: 0.01787, Accuracy: 99.782%\n",
      "Epoch 081: Loss: 0.01763, Accuracy: 99.787%\n",
      "Epoch 082: Loss: 0.01739, Accuracy: 99.788%\n",
      "Epoch 083: Loss: 0.01715, Accuracy: 99.798%\n",
      "Epoch 084: Loss: 0.01692, Accuracy: 99.803%\n",
      "Epoch 085: Loss: 0.01669, Accuracy: 99.805%\n",
      "Epoch 086: Loss: 0.01647, Accuracy: 99.810%\n",
      "Epoch 087: Loss: 0.01625, Accuracy: 99.817%\n",
      "Epoch 088: Loss: 0.01604, Accuracy: 99.820%\n",
      "Epoch 089: Loss: 0.01583, Accuracy: 99.822%\n",
      "Epoch 090: Loss: 0.01563, Accuracy: 99.825%\n",
      "Epoch 091: Loss: 0.01543, Accuracy: 99.835%\n",
      "Epoch 092: Loss: 0.01523, Accuracy: 99.835%\n",
      "Epoch 093: Loss: 0.01503, Accuracy: 99.840%\n",
      "Epoch 094: Loss: 0.01484, Accuracy: 99.843%\n",
      "Epoch 095: Loss: 0.01466, Accuracy: 99.845%\n",
      "Epoch 096: Loss: 0.01448, Accuracy: 99.847%\n",
      "Epoch 097: Loss: 0.01430, Accuracy: 99.850%\n",
      "Epoch 098: Loss: 0.01412, Accuracy: 99.855%\n",
      "Epoch 099: Loss: 0.01395, Accuracy: 99.855%\n",
      "Epoch 100: Loss: 0.01378, Accuracy: 99.857%\n",
      "Epoch 101: Loss: 0.01361, Accuracy: 99.863%\n",
      "Epoch 102: Loss: 0.01345, Accuracy: 99.863%\n",
      "Epoch 103: Loss: 0.01329, Accuracy: 99.867%\n",
      "Epoch 104: Loss: 0.01313, Accuracy: 99.872%\n",
      "Epoch 105: Loss: 0.01297, Accuracy: 99.872%\n",
      "Epoch 106: Loss: 0.01282, Accuracy: 99.875%\n",
      "Epoch 107: Loss: 0.01267, Accuracy: 99.882%\n",
      "Epoch 108: Loss: 0.01253, Accuracy: 99.882%\n",
      "Epoch 109: Loss: 0.01238, Accuracy: 99.888%\n",
      "Epoch 110: Loss: 0.01224, Accuracy: 99.890%\n",
      "Epoch 111: Loss: 0.01210, Accuracy: 99.895%\n",
      "Epoch 112: Loss: 0.01196, Accuracy: 99.895%\n",
      "Epoch 113: Loss: 0.01183, Accuracy: 99.897%\n",
      "Epoch 114: Loss: 0.01170, Accuracy: 99.898%\n",
      "Epoch 115: Loss: 0.01156, Accuracy: 99.903%\n",
      "Epoch 116: Loss: 0.01144, Accuracy: 99.907%\n",
      "Epoch 117: Loss: 0.01131, Accuracy: 99.907%\n",
      "Epoch 118: Loss: 0.01119, Accuracy: 99.908%\n",
      "Epoch 119: Loss: 0.01106, Accuracy: 99.910%\n",
      "Epoch 120: Loss: 0.01094, Accuracy: 99.912%\n",
      "Epoch 121: Loss: 0.01083, Accuracy: 99.915%\n",
      "Epoch 122: Loss: 0.01071, Accuracy: 99.917%\n",
      "Epoch 123: Loss: 0.01060, Accuracy: 99.920%\n",
      "Epoch 124: Loss: 0.01048, Accuracy: 99.920%\n",
      "Epoch 125: Loss: 0.01037, Accuracy: 99.922%\n",
      "Epoch 126: Loss: 0.01026, Accuracy: 99.923%\n",
      "Epoch 127: Loss: 0.01016, Accuracy: 99.923%\n",
      "Epoch 128: Loss: 0.01005, Accuracy: 99.930%\n",
      "Epoch 129: Loss: 0.00995, Accuracy: 99.932%\n",
      "Epoch 130: Loss: 0.00984, Accuracy: 99.933%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131: Loss: 0.00974, Accuracy: 99.935%\n",
      "Epoch 132: Loss: 0.00965, Accuracy: 99.935%\n",
      "Epoch 133: Loss: 0.00955, Accuracy: 99.937%\n",
      "Epoch 134: Loss: 0.00945, Accuracy: 99.942%\n",
      "Epoch 135: Loss: 0.00936, Accuracy: 99.945%\n",
      "Epoch 136: Loss: 0.00926, Accuracy: 99.947%\n",
      "Epoch 137: Loss: 0.00917, Accuracy: 99.947%\n",
      "Epoch 138: Loss: 0.00908, Accuracy: 99.947%\n",
      "Epoch 139: Loss: 0.00899, Accuracy: 99.947%\n",
      "Epoch 140: Loss: 0.00890, Accuracy: 99.950%\n",
      "Epoch 141: Loss: 0.00882, Accuracy: 99.952%\n",
      "Epoch 142: Loss: 0.00873, Accuracy: 99.953%\n",
      "Epoch 143: Loss: 0.00865, Accuracy: 99.953%\n",
      "Epoch 144: Loss: 0.00857, Accuracy: 99.957%\n",
      "Epoch 145: Loss: 0.00848, Accuracy: 99.957%\n",
      "Epoch 146: Loss: 0.00840, Accuracy: 99.957%\n",
      "Epoch 147: Loss: 0.00832, Accuracy: 99.957%\n",
      "Epoch 148: Loss: 0.00825, Accuracy: 99.957%\n",
      "Epoch 149: Loss: 0.00817, Accuracy: 99.957%\n",
      "Epoch 150: Loss: 0.00809, Accuracy: 99.957%\n",
      "Epoch 151: Loss: 0.00802, Accuracy: 99.960%\n",
      "Epoch 152: Loss: 0.00795, Accuracy: 99.962%\n",
      "Epoch 153: Loss: 0.00787, Accuracy: 99.963%\n",
      "Epoch 154: Loss: 0.00780, Accuracy: 99.965%\n",
      "Epoch 155: Loss: 0.00773, Accuracy: 99.967%\n",
      "Epoch 156: Loss: 0.00766, Accuracy: 99.967%\n",
      "Epoch 157: Loss: 0.00759, Accuracy: 99.967%\n",
      "Epoch 158: Loss: 0.00752, Accuracy: 99.967%\n",
      "Epoch 159: Loss: 0.00746, Accuracy: 99.967%\n",
      "Epoch 160: Loss: 0.00739, Accuracy: 99.967%\n",
      "Epoch 161: Loss: 0.00732, Accuracy: 99.968%\n",
      "Epoch 162: Loss: 0.00726, Accuracy: 99.968%\n",
      "Epoch 163: Loss: 0.00720, Accuracy: 99.970%\n",
      "Epoch 164: Loss: 0.00713, Accuracy: 99.970%\n",
      "Epoch 165: Loss: 0.00707, Accuracy: 99.970%\n",
      "Epoch 166: Loss: 0.00701, Accuracy: 99.972%\n",
      "Epoch 167: Loss: 0.00695, Accuracy: 99.972%\n",
      "Epoch 168: Loss: 0.00689, Accuracy: 99.972%\n",
      "Epoch 169: Loss: 0.00683, Accuracy: 99.972%\n",
      "Epoch 170: Loss: 0.00677, Accuracy: 99.972%\n",
      "Epoch 171: Loss: 0.00672, Accuracy: 99.973%\n",
      "Epoch 172: Loss: 0.00666, Accuracy: 99.973%\n",
      "Epoch 173: Loss: 0.00661, Accuracy: 99.973%\n",
      "Epoch 174: Loss: 0.00655, Accuracy: 99.973%\n",
      "Epoch 175: Loss: 0.00650, Accuracy: 99.975%\n",
      "Epoch 176: Loss: 0.00644, Accuracy: 99.975%\n",
      "Epoch 177: Loss: 0.00639, Accuracy: 99.975%\n",
      "Epoch 178: Loss: 0.00634, Accuracy: 99.977%\n",
      "Epoch 179: Loss: 0.00629, Accuracy: 99.977%\n",
      "Epoch 180: Loss: 0.00623, Accuracy: 99.977%\n",
      "Epoch 181: Loss: 0.00618, Accuracy: 99.977%\n",
      "Epoch 182: Loss: 0.00613, Accuracy: 99.977%\n",
      "Epoch 183: Loss: 0.00609, Accuracy: 99.978%\n",
      "Epoch 184: Loss: 0.00604, Accuracy: 99.978%\n",
      "Epoch 185: Loss: 0.00599, Accuracy: 99.978%\n",
      "Epoch 186: Loss: 0.00594, Accuracy: 99.978%\n",
      "Epoch 187: Loss: 0.00589, Accuracy: 99.978%\n",
      "Epoch 188: Loss: 0.00585, Accuracy: 99.978%\n",
      "Epoch 189: Loss: 0.00580, Accuracy: 99.978%\n",
      "Epoch 190: Loss: 0.00576, Accuracy: 99.978%\n",
      "Epoch 191: Loss: 0.00571, Accuracy: 99.978%\n",
      "Epoch 192: Loss: 0.00567, Accuracy: 99.978%\n",
      "Epoch 193: Loss: 0.00563, Accuracy: 99.980%\n",
      "Epoch 194: Loss: 0.00558, Accuracy: 99.982%\n",
      "Epoch 195: Loss: 0.00554, Accuracy: 99.982%\n",
      "Epoch 196: Loss: 0.00550, Accuracy: 99.982%\n",
      "Epoch 197: Loss: 0.00546, Accuracy: 99.982%\n",
      "Epoch 198: Loss: 0.00542, Accuracy: 99.982%\n",
      "Epoch 199: Loss: 0.00538, Accuracy: 99.982%\n",
      "Epoch 200: Loss: 0.00534, Accuracy: 99.982%\n",
      "RMSProp\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b866ad08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b866ad08>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b866ad08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b866ad08>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3b86273c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.39921, Accuracy: 92.075%\n",
      "Epoch 002: Loss: 0.19011, Accuracy: 96.497%\n",
      "Epoch 003: Loss: 0.13794, Accuracy: 97.758%\n",
      "Epoch 004: Loss: 0.10765, Accuracy: 98.468%\n",
      "Epoch 005: Loss: 0.08712, Accuracy: 98.945%\n",
      "Epoch 006: Loss: 0.07203, Accuracy: 99.217%\n",
      "Epoch 007: Loss: 0.06045, Accuracy: 99.427%\n",
      "Epoch 008: Loss: 0.05125, Accuracy: 99.573%\n",
      "Epoch 009: Loss: 0.04377, Accuracy: 99.700%\n",
      "Epoch 010: Loss: 0.03752, Accuracy: 99.783%\n",
      "Epoch 011: Loss: 0.03227, Accuracy: 99.855%\n",
      "Epoch 012: Loss: 0.02778, Accuracy: 99.898%\n",
      "Epoch 013: Loss: 0.02392, Accuracy: 99.923%\n",
      "Epoch 014: Loss: 0.02063, Accuracy: 99.942%\n",
      "Epoch 015: Loss: 0.01781, Accuracy: 99.955%\n",
      "Epoch 016: Loss: 0.01541, Accuracy: 99.980%\n",
      "Epoch 017: Loss: 0.01339, Accuracy: 99.982%\n",
      "Epoch 018: Loss: 0.01170, Accuracy: 99.985%\n",
      "Epoch 019: Loss: 0.01025, Accuracy: 99.987%\n",
      "Epoch 020: Loss: 0.00901, Accuracy: 99.987%\n",
      "Epoch 021: Loss: 0.00797, Accuracy: 99.988%\n",
      "Epoch 022: Loss: 0.00708, Accuracy: 99.993%\n",
      "Epoch 023: Loss: 0.00631, Accuracy: 99.995%\n",
      "Epoch 024: Loss: 0.00566, Accuracy: 99.995%\n",
      "Epoch 025: Loss: 0.00510, Accuracy: 99.998%\n",
      "Epoch 026: Loss: 0.00462, Accuracy: 99.998%\n",
      "Epoch 027: Loss: 0.00420, Accuracy: 99.998%\n",
      "Epoch 028: Loss: 0.00384, Accuracy: 99.998%\n",
      "Epoch 029: Loss: 0.00352, Accuracy: 100.000%\n",
      "Epoch 030: Loss: 0.00325, Accuracy: 100.000%\n",
      "Epoch 031: Loss: 0.00301, Accuracy: 100.000%\n",
      "Epoch 032: Loss: 0.00280, Accuracy: 100.000%\n",
      "Epoch 033: Loss: 0.00262, Accuracy: 100.000%\n",
      "Epoch 034: Loss: 0.00246, Accuracy: 100.000%\n",
      "Epoch 035: Loss: 0.00231, Accuracy: 100.000%\n",
      "Epoch 036: Loss: 0.00218, Accuracy: 100.000%\n",
      "Epoch 037: Loss: 0.00206, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00195, Accuracy: 100.000%\n",
      "Epoch 039: Loss: 0.00185, Accuracy: 100.000%\n",
      "Epoch 040: Loss: 0.00176, Accuracy: 100.000%\n",
      "Epoch 041: Loss: 0.00168, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00160, Accuracy: 100.000%\n",
      "Epoch 043: Loss: 0.00153, Accuracy: 100.000%\n",
      "Epoch 044: Loss: 0.00146, Accuracy: 100.000%\n",
      "Epoch 045: Loss: 0.00140, Accuracy: 100.000%\n",
      "Epoch 046: Loss: 0.00135, Accuracy: 100.000%\n",
      "Epoch 047: Loss: 0.00130, Accuracy: 100.000%\n",
      "Epoch 048: Loss: 0.00125, Accuracy: 100.000%\n",
      "Epoch 049: Loss: 0.00120, Accuracy: 100.000%\n",
      "Epoch 050: Loss: 0.00116, Accuracy: 100.000%\n",
      "Epoch 051: Loss: 0.00112, Accuracy: 100.000%\n",
      "Epoch 052: Loss: 0.00108, Accuracy: 100.000%\n",
      "Epoch 053: Loss: 0.00104, Accuracy: 100.000%\n",
      "Epoch 054: Loss: 0.00101, Accuracy: 100.000%\n",
      "Epoch 055: Loss: 0.00098, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00095, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00092, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00089, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00087, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00084, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00082, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00080, Accuracy: 100.000%\n",
      "Epoch 063: Loss: 0.00078, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00076, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00074, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00072, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00070, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00069, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00067, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00065, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00064, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00063, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00061, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00060, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00059, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00057, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00056, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00055, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00054, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00053, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00052, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00051, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00050, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00049, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00048, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00047, Accuracy: 100.000%\n",
      "Epoch 087: Loss: 0.00046, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00046, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00045, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00044, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00043, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00043, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00042, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00041, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00041, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00040, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00039, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00039, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00038, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00038, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00037, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00037, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 107: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00021, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 172: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 186: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00014, Accuracy: 100.000%\n",
      "Adam\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d90>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d90>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46dae7f60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.17093, Accuracy: 95.943%\n",
      "Epoch 002: Loss: 0.05662, Accuracy: 98.665%\n",
      "Epoch 003: Loss: 0.03189, Accuracy: 99.423%\n",
      "Epoch 004: Loss: 0.01881, Accuracy: 99.740%\n",
      "Epoch 005: Loss: 0.01171, Accuracy: 99.895%\n",
      "Epoch 006: Loss: 0.00741, Accuracy: 99.957%\n",
      "Epoch 007: Loss: 0.00520, Accuracy: 99.978%\n",
      "Epoch 008: Loss: 0.00642, Accuracy: 99.982%\n",
      "Epoch 009: Loss: 0.00726, Accuracy: 99.982%\n",
      "Epoch 010: Loss: 0.00675, Accuracy: 99.975%\n",
      "Epoch 011: Loss: 0.00445, Accuracy: 99.988%\n",
      "Epoch 012: Loss: 0.00233, Accuracy: 99.998%\n",
      "Epoch 013: Loss: 0.00307, Accuracy: 99.993%\n",
      "Epoch 014: Loss: 0.00893, Accuracy: 99.975%\n",
      "Epoch 015: Loss: 0.00437, Accuracy: 99.988%\n",
      "Epoch 016: Loss: 0.00233, Accuracy: 99.997%\n",
      "Epoch 017: Loss: 0.00405, Accuracy: 99.987%\n",
      "Epoch 018: Loss: 0.00178, Accuracy: 100.000%\n",
      "Epoch 019: Loss: 0.00371, Accuracy: 99.987%\n",
      "Epoch 020: Loss: 0.00560, Accuracy: 99.983%\n",
      "Epoch 021: Loss: 0.00231, Accuracy: 99.997%\n",
      "Epoch 022: Loss: 0.00260, Accuracy: 99.997%\n",
      "Epoch 023: Loss: 0.00213, Accuracy: 99.997%\n",
      "Epoch 024: Loss: 0.00633, Accuracy: 99.985%\n",
      "Epoch 025: Loss: 0.00223, Accuracy: 99.995%\n",
      "Epoch 026: Loss: 0.00284, Accuracy: 99.995%\n",
      "Epoch 027: Loss: 0.00193, Accuracy: 99.997%\n",
      "Epoch 028: Loss: 0.00265, Accuracy: 99.992%\n",
      "Epoch 029: Loss: 0.00320, Accuracy: 99.995%\n",
      "Epoch 030: Loss: 0.00235, Accuracy: 99.993%\n",
      "Epoch 031: Loss: 0.00147, Accuracy: 99.997%\n",
      "Epoch 032: Loss: 0.00278, Accuracy: 99.990%\n",
      "Epoch 033: Loss: 0.00383, Accuracy: 99.987%\n",
      "Epoch 034: Loss: 0.00350, Accuracy: 99.997%\n",
      "Epoch 035: Loss: 0.00151, Accuracy: 99.998%\n",
      "Epoch 036: Loss: 0.00053, Accuracy: 99.998%\n",
      "Epoch 037: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00341, Accuracy: 99.993%\n",
      "Epoch 039: Loss: 0.00381, Accuracy: 99.995%\n",
      "Epoch 040: Loss: 0.00245, Accuracy: 99.997%\n",
      "Epoch 041: Loss: 0.00171, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00166, Accuracy: 99.998%\n",
      "Epoch 043: Loss: 0.00229, Accuracy: 99.992%\n",
      "Epoch 044: Loss: 0.00197, Accuracy: 99.992%\n",
      "Epoch 045: Loss: 0.00249, Accuracy: 99.998%\n",
      "Epoch 046: Loss: 0.00304, Accuracy: 99.992%\n",
      "Epoch 047: Loss: 0.00149, Accuracy: 99.995%\n",
      "Epoch 048: Loss: 0.00116, Accuracy: 99.997%\n",
      "Epoch 049: Loss: 0.00132, Accuracy: 99.998%\n",
      "Epoch 050: Loss: 0.00157, Accuracy: 99.998%\n",
      "Epoch 051: Loss: 0.00246, Accuracy: 99.995%\n",
      "Epoch 052: Loss: 0.00181, Accuracy: 99.998%\n",
      "Epoch 053: Loss: 0.00148, Accuracy: 99.998%\n",
      "Epoch 054: Loss: 0.00129, Accuracy: 99.998%\n",
      "Epoch 055: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00002, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 063: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 087: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 107: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 150: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00000, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 186: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00000, Accuracy: 100.000%\n",
      "Trial 5\n",
      "SGD Nesterov\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5f28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5f28>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5f28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5f28>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd35afee908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 1.07676, Accuracy: 77.735%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002: Loss: 0.43468, Accuracy: 88.718%\n",
      "Epoch 003: Loss: 0.34892, Accuracy: 90.403%\n",
      "Epoch 004: Loss: 0.30975, Accuracy: 91.345%\n",
      "Epoch 005: Loss: 0.28432, Accuracy: 92.118%\n",
      "Epoch 006: Loss: 0.26518, Accuracy: 92.593%\n",
      "Epoch 007: Loss: 0.24960, Accuracy: 93.027%\n",
      "Epoch 008: Loss: 0.23631, Accuracy: 93.418%\n",
      "Epoch 009: Loss: 0.22465, Accuracy: 93.762%\n",
      "Epoch 010: Loss: 0.21419, Accuracy: 94.035%\n",
      "Epoch 011: Loss: 0.20470, Accuracy: 94.328%\n",
      "Epoch 012: Loss: 0.19600, Accuracy: 94.565%\n",
      "Epoch 013: Loss: 0.18797, Accuracy: 94.797%\n",
      "Epoch 014: Loss: 0.18050, Accuracy: 94.985%\n",
      "Epoch 015: Loss: 0.17355, Accuracy: 95.177%\n",
      "Epoch 016: Loss: 0.16707, Accuracy: 95.375%\n",
      "Epoch 017: Loss: 0.16099, Accuracy: 95.515%\n",
      "Epoch 018: Loss: 0.15529, Accuracy: 95.658%\n",
      "Epoch 019: Loss: 0.14992, Accuracy: 95.785%\n",
      "Epoch 020: Loss: 0.14488, Accuracy: 95.962%\n",
      "Epoch 021: Loss: 0.14013, Accuracy: 96.120%\n",
      "Epoch 022: Loss: 0.13564, Accuracy: 96.258%\n",
      "Epoch 023: Loss: 0.13139, Accuracy: 96.387%\n",
      "Epoch 024: Loss: 0.12737, Accuracy: 96.503%\n",
      "Epoch 025: Loss: 0.12355, Accuracy: 96.635%\n",
      "Epoch 026: Loss: 0.11991, Accuracy: 96.733%\n",
      "Epoch 027: Loss: 0.11645, Accuracy: 96.842%\n",
      "Epoch 028: Loss: 0.11316, Accuracy: 96.950%\n",
      "Epoch 029: Loss: 0.11001, Accuracy: 97.042%\n",
      "Epoch 030: Loss: 0.10700, Accuracy: 97.133%\n",
      "Epoch 031: Loss: 0.10412, Accuracy: 97.217%\n",
      "Epoch 032: Loss: 0.10135, Accuracy: 97.288%\n",
      "Epoch 033: Loss: 0.09870, Accuracy: 97.338%\n",
      "Epoch 034: Loss: 0.09615, Accuracy: 97.412%\n",
      "Epoch 035: Loss: 0.09370, Accuracy: 97.472%\n",
      "Epoch 036: Loss: 0.09135, Accuracy: 97.538%\n",
      "Epoch 037: Loss: 0.08908, Accuracy: 97.618%\n",
      "Epoch 038: Loss: 0.08689, Accuracy: 97.687%\n",
      "Epoch 039: Loss: 0.08479, Accuracy: 97.745%\n",
      "Epoch 040: Loss: 0.08276, Accuracy: 97.800%\n",
      "Epoch 041: Loss: 0.08079, Accuracy: 97.855%\n",
      "Epoch 042: Loss: 0.07890, Accuracy: 97.913%\n",
      "Epoch 043: Loss: 0.07706, Accuracy: 97.972%\n",
      "Epoch 044: Loss: 0.07529, Accuracy: 98.022%\n",
      "Epoch 045: Loss: 0.07357, Accuracy: 98.075%\n",
      "Epoch 046: Loss: 0.07191, Accuracy: 98.122%\n",
      "Epoch 047: Loss: 0.07030, Accuracy: 98.177%\n",
      "Epoch 048: Loss: 0.06875, Accuracy: 98.215%\n",
      "Epoch 049: Loss: 0.06724, Accuracy: 98.258%\n",
      "Epoch 050: Loss: 0.06577, Accuracy: 98.292%\n",
      "Epoch 051: Loss: 0.06435, Accuracy: 98.347%\n",
      "Epoch 052: Loss: 0.06297, Accuracy: 98.387%\n",
      "Epoch 053: Loss: 0.06163, Accuracy: 98.437%\n",
      "Epoch 054: Loss: 0.06033, Accuracy: 98.488%\n",
      "Epoch 055: Loss: 0.05907, Accuracy: 98.528%\n",
      "Epoch 056: Loss: 0.05784, Accuracy: 98.557%\n",
      "Epoch 057: Loss: 0.05664, Accuracy: 98.585%\n",
      "Epoch 058: Loss: 0.05548, Accuracy: 98.620%\n",
      "Epoch 059: Loss: 0.05435, Accuracy: 98.662%\n",
      "Epoch 060: Loss: 0.05324, Accuracy: 98.682%\n",
      "Epoch 061: Loss: 0.05217, Accuracy: 98.720%\n",
      "Epoch 062: Loss: 0.05113, Accuracy: 98.752%\n",
      "Epoch 063: Loss: 0.05011, Accuracy: 98.778%\n",
      "Epoch 064: Loss: 0.04912, Accuracy: 98.803%\n",
      "Epoch 065: Loss: 0.04816, Accuracy: 98.830%\n",
      "Epoch 066: Loss: 0.04722, Accuracy: 98.852%\n",
      "Epoch 067: Loss: 0.04630, Accuracy: 98.875%\n",
      "Epoch 068: Loss: 0.04541, Accuracy: 98.900%\n",
      "Epoch 069: Loss: 0.04454, Accuracy: 98.927%\n",
      "Epoch 070: Loss: 0.04369, Accuracy: 98.955%\n",
      "Epoch 071: Loss: 0.04286, Accuracy: 98.988%\n",
      "Epoch 072: Loss: 0.04205, Accuracy: 99.007%\n",
      "Epoch 073: Loss: 0.04127, Accuracy: 99.028%\n",
      "Epoch 074: Loss: 0.04050, Accuracy: 99.047%\n",
      "Epoch 075: Loss: 0.03975, Accuracy: 99.075%\n",
      "Epoch 076: Loss: 0.03901, Accuracy: 99.090%\n",
      "Epoch 077: Loss: 0.03830, Accuracy: 99.110%\n",
      "Epoch 078: Loss: 0.03760, Accuracy: 99.140%\n",
      "Epoch 079: Loss: 0.03691, Accuracy: 99.167%\n",
      "Epoch 080: Loss: 0.03625, Accuracy: 99.193%\n",
      "Epoch 081: Loss: 0.03559, Accuracy: 99.218%\n",
      "Epoch 082: Loss: 0.03495, Accuracy: 99.230%\n",
      "Epoch 083: Loss: 0.03433, Accuracy: 99.250%\n",
      "Epoch 084: Loss: 0.03372, Accuracy: 99.263%\n",
      "Epoch 085: Loss: 0.03312, Accuracy: 99.282%\n",
      "Epoch 086: Loss: 0.03253, Accuracy: 99.307%\n",
      "Epoch 087: Loss: 0.03196, Accuracy: 99.313%\n",
      "Epoch 088: Loss: 0.03140, Accuracy: 99.332%\n",
      "Epoch 089: Loss: 0.03085, Accuracy: 99.342%\n",
      "Epoch 090: Loss: 0.03031, Accuracy: 99.355%\n",
      "Epoch 091: Loss: 0.02979, Accuracy: 99.367%\n",
      "Epoch 092: Loss: 0.02927, Accuracy: 99.380%\n",
      "Epoch 093: Loss: 0.02877, Accuracy: 99.408%\n",
      "Epoch 094: Loss: 0.02827, Accuracy: 99.425%\n",
      "Epoch 095: Loss: 0.02779, Accuracy: 99.433%\n",
      "Epoch 096: Loss: 0.02732, Accuracy: 99.457%\n",
      "Epoch 097: Loss: 0.02685, Accuracy: 99.465%\n",
      "Epoch 098: Loss: 0.02640, Accuracy: 99.475%\n",
      "Epoch 099: Loss: 0.02595, Accuracy: 99.483%\n",
      "Epoch 100: Loss: 0.02552, Accuracy: 99.498%\n",
      "Epoch 101: Loss: 0.02509, Accuracy: 99.508%\n",
      "Epoch 102: Loss: 0.02467, Accuracy: 99.523%\n",
      "Epoch 103: Loss: 0.02426, Accuracy: 99.538%\n",
      "Epoch 104: Loss: 0.02386, Accuracy: 99.560%\n",
      "Epoch 105: Loss: 0.02346, Accuracy: 99.573%\n",
      "Epoch 106: Loss: 0.02308, Accuracy: 99.593%\n",
      "Epoch 107: Loss: 0.02270, Accuracy: 99.610%\n",
      "Epoch 108: Loss: 0.02233, Accuracy: 99.627%\n",
      "Epoch 109: Loss: 0.02197, Accuracy: 99.643%\n",
      "Epoch 110: Loss: 0.02161, Accuracy: 99.662%\n",
      "Epoch 111: Loss: 0.02126, Accuracy: 99.670%\n",
      "Epoch 112: Loss: 0.02092, Accuracy: 99.677%\n",
      "Epoch 113: Loss: 0.02058, Accuracy: 99.687%\n",
      "Epoch 114: Loss: 0.02026, Accuracy: 99.695%\n",
      "Epoch 115: Loss: 0.01993, Accuracy: 99.705%\n",
      "Epoch 116: Loss: 0.01962, Accuracy: 99.707%\n",
      "Epoch 117: Loss: 0.01931, Accuracy: 99.713%\n",
      "Epoch 118: Loss: 0.01900, Accuracy: 99.717%\n",
      "Epoch 119: Loss: 0.01870, Accuracy: 99.725%\n",
      "Epoch 120: Loss: 0.01841, Accuracy: 99.728%\n",
      "Epoch 121: Loss: 0.01813, Accuracy: 99.737%\n",
      "Epoch 122: Loss: 0.01785, Accuracy: 99.740%\n",
      "Epoch 123: Loss: 0.01757, Accuracy: 99.747%\n",
      "Epoch 124: Loss: 0.01730, Accuracy: 99.755%\n",
      "Epoch 125: Loss: 0.01704, Accuracy: 99.762%\n",
      "Epoch 126: Loss: 0.01678, Accuracy: 99.773%\n",
      "Epoch 127: Loss: 0.01652, Accuracy: 99.785%\n",
      "Epoch 128: Loss: 0.01627, Accuracy: 99.790%\n",
      "Epoch 129: Loss: 0.01603, Accuracy: 99.793%\n",
      "Epoch 130: Loss: 0.01579, Accuracy: 99.803%\n",
      "Epoch 131: Loss: 0.01555, Accuracy: 99.815%\n",
      "Epoch 132: Loss: 0.01532, Accuracy: 99.823%\n",
      "Epoch 133: Loss: 0.01510, Accuracy: 99.823%\n",
      "Epoch 134: Loss: 0.01488, Accuracy: 99.828%\n",
      "Epoch 135: Loss: 0.01466, Accuracy: 99.830%\n",
      "Epoch 136: Loss: 0.01445, Accuracy: 99.835%\n",
      "Epoch 137: Loss: 0.01424, Accuracy: 99.838%\n",
      "Epoch 138: Loss: 0.01403, Accuracy: 99.838%\n",
      "Epoch 139: Loss: 0.01383, Accuracy: 99.840%\n",
      "Epoch 140: Loss: 0.01363, Accuracy: 99.850%\n",
      "Epoch 141: Loss: 0.01344, Accuracy: 99.855%\n",
      "Epoch 142: Loss: 0.01325, Accuracy: 99.858%\n",
      "Epoch 143: Loss: 0.01306, Accuracy: 99.862%\n",
      "Epoch 144: Loss: 0.01288, Accuracy: 99.872%\n",
      "Epoch 145: Loss: 0.01270, Accuracy: 99.875%\n",
      "Epoch 146: Loss: 0.01252, Accuracy: 99.880%\n",
      "Epoch 147: Loss: 0.01235, Accuracy: 99.882%\n",
      "Epoch 148: Loss: 0.01218, Accuracy: 99.885%\n",
      "Epoch 149: Loss: 0.01201, Accuracy: 99.885%\n",
      "Epoch 150: Loss: 0.01185, Accuracy: 99.888%\n",
      "Epoch 151: Loss: 0.01169, Accuracy: 99.892%\n",
      "Epoch 152: Loss: 0.01153, Accuracy: 99.895%\n",
      "Epoch 153: Loss: 0.01138, Accuracy: 99.895%\n",
      "Epoch 154: Loss: 0.01123, Accuracy: 99.900%\n",
      "Epoch 155: Loss: 0.01108, Accuracy: 99.905%\n",
      "Epoch 156: Loss: 0.01093, Accuracy: 99.908%\n",
      "Epoch 157: Loss: 0.01079, Accuracy: 99.913%\n",
      "Epoch 158: Loss: 0.01064, Accuracy: 99.917%\n",
      "Epoch 159: Loss: 0.01051, Accuracy: 99.922%\n",
      "Epoch 160: Loss: 0.01037, Accuracy: 99.923%\n",
      "Epoch 161: Loss: 0.01024, Accuracy: 99.927%\n",
      "Epoch 162: Loss: 0.01010, Accuracy: 99.928%\n",
      "Epoch 163: Loss: 0.00998, Accuracy: 99.928%\n",
      "Epoch 164: Loss: 0.00985, Accuracy: 99.930%\n",
      "Epoch 165: Loss: 0.00972, Accuracy: 99.932%\n",
      "Epoch 166: Loss: 0.00960, Accuracy: 99.933%\n",
      "Epoch 167: Loss: 0.00948, Accuracy: 99.938%\n",
      "Epoch 168: Loss: 0.00936, Accuracy: 99.938%\n",
      "Epoch 169: Loss: 0.00925, Accuracy: 99.938%\n",
      "Epoch 170: Loss: 0.00913, Accuracy: 99.938%\n",
      "Epoch 171: Loss: 0.00902, Accuracy: 99.940%\n",
      "Epoch 172: Loss: 0.00891, Accuracy: 99.943%\n",
      "Epoch 173: Loss: 0.00880, Accuracy: 99.945%\n",
      "Epoch 174: Loss: 0.00870, Accuracy: 99.945%\n",
      "Epoch 175: Loss: 0.00859, Accuracy: 99.945%\n",
      "Epoch 176: Loss: 0.00849, Accuracy: 99.947%\n",
      "Epoch 177: Loss: 0.00839, Accuracy: 99.948%\n",
      "Epoch 178: Loss: 0.00829, Accuracy: 99.952%\n",
      "Epoch 179: Loss: 0.00819, Accuracy: 99.955%\n",
      "Epoch 180: Loss: 0.00809, Accuracy: 99.955%\n",
      "Epoch 181: Loss: 0.00800, Accuracy: 99.957%\n",
      "Epoch 182: Loss: 0.00791, Accuracy: 99.958%\n",
      "Epoch 183: Loss: 0.00781, Accuracy: 99.960%\n",
      "Epoch 184: Loss: 0.00772, Accuracy: 99.963%\n",
      "Epoch 185: Loss: 0.00763, Accuracy: 99.963%\n",
      "Epoch 186: Loss: 0.00755, Accuracy: 99.963%\n",
      "Epoch 187: Loss: 0.00746, Accuracy: 99.963%\n",
      "Epoch 188: Loss: 0.00738, Accuracy: 99.963%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189: Loss: 0.00729, Accuracy: 99.965%\n",
      "Epoch 190: Loss: 0.00721, Accuracy: 99.965%\n",
      "Epoch 191: Loss: 0.00713, Accuracy: 99.965%\n",
      "Epoch 192: Loss: 0.00705, Accuracy: 99.965%\n",
      "Epoch 193: Loss: 0.00697, Accuracy: 99.965%\n",
      "Epoch 194: Loss: 0.00690, Accuracy: 99.965%\n",
      "Epoch 195: Loss: 0.00682, Accuracy: 99.965%\n",
      "Epoch 196: Loss: 0.00675, Accuracy: 99.967%\n",
      "Epoch 197: Loss: 0.00667, Accuracy: 99.967%\n",
      "Epoch 198: Loss: 0.00660, Accuracy: 99.967%\n",
      "Epoch 199: Loss: 0.00653, Accuracy: 99.968%\n",
      "Epoch 200: Loss: 0.00646, Accuracy: 99.970%\n",
      "Adagrad\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a59d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a59d8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a59d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a59d8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd46db20d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.29729, Accuracy: 94.088%\n",
      "Epoch 002: Loss: 0.17624, Accuracy: 95.865%\n",
      "Epoch 003: Loss: 0.14712, Accuracy: 96.498%\n",
      "Epoch 004: Loss: 0.12969, Accuracy: 96.937%\n",
      "Epoch 005: Loss: 0.11745, Accuracy: 97.207%\n",
      "Epoch 006: Loss: 0.10809, Accuracy: 97.455%\n",
      "Epoch 007: Loss: 0.10058, Accuracy: 97.582%\n",
      "Epoch 008: Loss: 0.09434, Accuracy: 97.753%\n",
      "Epoch 009: Loss: 0.08902, Accuracy: 97.905%\n",
      "Epoch 010: Loss: 0.08440, Accuracy: 98.035%\n",
      "Epoch 011: Loss: 0.08033, Accuracy: 98.143%\n",
      "Epoch 012: Loss: 0.07670, Accuracy: 98.257%\n",
      "Epoch 013: Loss: 0.07344, Accuracy: 98.332%\n",
      "Epoch 014: Loss: 0.07048, Accuracy: 98.398%\n",
      "Epoch 015: Loss: 0.06778, Accuracy: 98.465%\n",
      "Epoch 016: Loss: 0.06529, Accuracy: 98.523%\n",
      "Epoch 017: Loss: 0.06299, Accuracy: 98.585%\n",
      "Epoch 018: Loss: 0.06086, Accuracy: 98.633%\n",
      "Epoch 019: Loss: 0.05887, Accuracy: 98.688%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020: Loss: 0.05700, Accuracy: 98.722%\n",
      "Epoch 021: Loss: 0.05526, Accuracy: 98.760%\n",
      "Epoch 022: Loss: 0.05361, Accuracy: 98.795%\n",
      "Epoch 023: Loss: 0.05205, Accuracy: 98.847%\n",
      "Epoch 024: Loss: 0.05058, Accuracy: 98.882%\n",
      "Epoch 025: Loss: 0.04918, Accuracy: 98.923%\n",
      "Epoch 026: Loss: 0.04786, Accuracy: 98.952%\n",
      "Epoch 027: Loss: 0.04660, Accuracy: 98.980%\n",
      "Epoch 028: Loss: 0.04540, Accuracy: 99.020%\n",
      "Epoch 029: Loss: 0.04425, Accuracy: 99.037%\n",
      "Epoch 030: Loss: 0.04316, Accuracy: 99.052%\n",
      "Epoch 031: Loss: 0.04211, Accuracy: 99.073%\n",
      "Epoch 032: Loss: 0.04111, Accuracy: 99.113%\n",
      "Epoch 033: Loss: 0.04015, Accuracy: 99.148%\n",
      "Epoch 034: Loss: 0.03923, Accuracy: 99.173%\n",
      "Epoch 035: Loss: 0.03835, Accuracy: 99.187%\n",
      "Epoch 036: Loss: 0.03749, Accuracy: 99.215%\n",
      "Epoch 037: Loss: 0.03667, Accuracy: 99.237%\n",
      "Epoch 038: Loss: 0.03588, Accuracy: 99.267%\n",
      "Epoch 039: Loss: 0.03512, Accuracy: 99.295%\n",
      "Epoch 040: Loss: 0.03438, Accuracy: 99.313%\n",
      "Epoch 041: Loss: 0.03367, Accuracy: 99.340%\n",
      "Epoch 042: Loss: 0.03298, Accuracy: 99.353%\n",
      "Epoch 043: Loss: 0.03232, Accuracy: 99.367%\n",
      "Epoch 044: Loss: 0.03168, Accuracy: 99.385%\n",
      "Epoch 045: Loss: 0.03106, Accuracy: 99.403%\n",
      "Epoch 046: Loss: 0.03046, Accuracy: 99.420%\n",
      "Epoch 047: Loss: 0.02988, Accuracy: 99.437%\n",
      "Epoch 048: Loss: 0.02931, Accuracy: 99.455%\n",
      "Epoch 049: Loss: 0.02876, Accuracy: 99.470%\n",
      "Epoch 050: Loss: 0.02823, Accuracy: 99.488%\n",
      "Epoch 051: Loss: 0.02772, Accuracy: 99.502%\n",
      "Epoch 052: Loss: 0.02722, Accuracy: 99.525%\n",
      "Epoch 053: Loss: 0.02673, Accuracy: 99.540%\n",
      "Epoch 054: Loss: 0.02626, Accuracy: 99.553%\n",
      "Epoch 055: Loss: 0.02580, Accuracy: 99.558%\n",
      "Epoch 056: Loss: 0.02535, Accuracy: 99.575%\n",
      "Epoch 057: Loss: 0.02492, Accuracy: 99.582%\n",
      "Epoch 058: Loss: 0.02449, Accuracy: 99.600%\n",
      "Epoch 059: Loss: 0.02408, Accuracy: 99.610%\n",
      "Epoch 060: Loss: 0.02368, Accuracy: 99.623%\n",
      "Epoch 061: Loss: 0.02329, Accuracy: 99.632%\n",
      "Epoch 062: Loss: 0.02291, Accuracy: 99.645%\n",
      "Epoch 063: Loss: 0.02254, Accuracy: 99.658%\n",
      "Epoch 064: Loss: 0.02218, Accuracy: 99.667%\n",
      "Epoch 065: Loss: 0.02183, Accuracy: 99.678%\n",
      "Epoch 066: Loss: 0.02148, Accuracy: 99.683%\n",
      "Epoch 067: Loss: 0.02115, Accuracy: 99.693%\n",
      "Epoch 068: Loss: 0.02082, Accuracy: 99.702%\n",
      "Epoch 069: Loss: 0.02050, Accuracy: 99.710%\n",
      "Epoch 070: Loss: 0.02019, Accuracy: 99.725%\n",
      "Epoch 071: Loss: 0.01988, Accuracy: 99.737%\n",
      "Epoch 072: Loss: 0.01959, Accuracy: 99.748%\n",
      "Epoch 073: Loss: 0.01930, Accuracy: 99.757%\n",
      "Epoch 074: Loss: 0.01901, Accuracy: 99.760%\n",
      "Epoch 075: Loss: 0.01873, Accuracy: 99.760%\n",
      "Epoch 076: Loss: 0.01846, Accuracy: 99.762%\n",
      "Epoch 077: Loss: 0.01820, Accuracy: 99.770%\n",
      "Epoch 078: Loss: 0.01794, Accuracy: 99.780%\n",
      "Epoch 079: Loss: 0.01769, Accuracy: 99.790%\n",
      "Epoch 080: Loss: 0.01744, Accuracy: 99.797%\n",
      "Epoch 081: Loss: 0.01719, Accuracy: 99.802%\n",
      "Epoch 082: Loss: 0.01696, Accuracy: 99.807%\n",
      "Epoch 083: Loss: 0.01672, Accuracy: 99.820%\n",
      "Epoch 084: Loss: 0.01649, Accuracy: 99.820%\n",
      "Epoch 085: Loss: 0.01627, Accuracy: 99.828%\n",
      "Epoch 086: Loss: 0.01605, Accuracy: 99.833%\n",
      "Epoch 087: Loss: 0.01584, Accuracy: 99.840%\n",
      "Epoch 088: Loss: 0.01563, Accuracy: 99.847%\n",
      "Epoch 089: Loss: 0.01542, Accuracy: 99.850%\n",
      "Epoch 090: Loss: 0.01522, Accuracy: 99.855%\n",
      "Epoch 091: Loss: 0.01502, Accuracy: 99.857%\n",
      "Epoch 092: Loss: 0.01483, Accuracy: 99.860%\n",
      "Epoch 093: Loss: 0.01464, Accuracy: 99.863%\n",
      "Epoch 094: Loss: 0.01445, Accuracy: 99.867%\n",
      "Epoch 095: Loss: 0.01427, Accuracy: 99.868%\n",
      "Epoch 096: Loss: 0.01409, Accuracy: 99.872%\n",
      "Epoch 097: Loss: 0.01392, Accuracy: 99.872%\n",
      "Epoch 098: Loss: 0.01374, Accuracy: 99.872%\n",
      "Epoch 099: Loss: 0.01358, Accuracy: 99.875%\n",
      "Epoch 100: Loss: 0.01341, Accuracy: 99.877%\n",
      "Epoch 101: Loss: 0.01325, Accuracy: 99.878%\n",
      "Epoch 102: Loss: 0.01309, Accuracy: 99.880%\n",
      "Epoch 103: Loss: 0.01293, Accuracy: 99.883%\n",
      "Epoch 104: Loss: 0.01277, Accuracy: 99.885%\n",
      "Epoch 105: Loss: 0.01262, Accuracy: 99.888%\n",
      "Epoch 106: Loss: 0.01247, Accuracy: 99.888%\n",
      "Epoch 107: Loss: 0.01233, Accuracy: 99.888%\n",
      "Epoch 108: Loss: 0.01218, Accuracy: 99.892%\n",
      "Epoch 109: Loss: 0.01204, Accuracy: 99.893%\n",
      "Epoch 110: Loss: 0.01190, Accuracy: 99.895%\n",
      "Epoch 111: Loss: 0.01177, Accuracy: 99.898%\n",
      "Epoch 112: Loss: 0.01163, Accuracy: 99.900%\n",
      "Epoch 113: Loss: 0.01150, Accuracy: 99.903%\n",
      "Epoch 114: Loss: 0.01137, Accuracy: 99.910%\n",
      "Epoch 115: Loss: 0.01124, Accuracy: 99.912%\n",
      "Epoch 116: Loss: 0.01112, Accuracy: 99.912%\n",
      "Epoch 117: Loss: 0.01099, Accuracy: 99.915%\n",
      "Epoch 118: Loss: 0.01087, Accuracy: 99.918%\n",
      "Epoch 119: Loss: 0.01075, Accuracy: 99.920%\n",
      "Epoch 120: Loss: 0.01064, Accuracy: 99.923%\n",
      "Epoch 121: Loss: 0.01052, Accuracy: 99.923%\n",
      "Epoch 122: Loss: 0.01041, Accuracy: 99.925%\n",
      "Epoch 123: Loss: 0.01030, Accuracy: 99.927%\n",
      "Epoch 124: Loss: 0.01018, Accuracy: 99.927%\n",
      "Epoch 125: Loss: 0.01008, Accuracy: 99.927%\n",
      "Epoch 126: Loss: 0.00997, Accuracy: 99.930%\n",
      "Epoch 127: Loss: 0.00987, Accuracy: 99.930%\n",
      "Epoch 128: Loss: 0.00976, Accuracy: 99.930%\n",
      "Epoch 129: Loss: 0.00966, Accuracy: 99.930%\n",
      "Epoch 130: Loss: 0.00956, Accuracy: 99.932%\n",
      "Epoch 131: Loss: 0.00946, Accuracy: 99.935%\n",
      "Epoch 132: Loss: 0.00936, Accuracy: 99.935%\n",
      "Epoch 133: Loss: 0.00927, Accuracy: 99.938%\n",
      "Epoch 134: Loss: 0.00917, Accuracy: 99.938%\n",
      "Epoch 135: Loss: 0.00908, Accuracy: 99.942%\n",
      "Epoch 136: Loss: 0.00899, Accuracy: 99.943%\n",
      "Epoch 137: Loss: 0.00890, Accuracy: 99.945%\n",
      "Epoch 138: Loss: 0.00881, Accuracy: 99.948%\n",
      "Epoch 139: Loss: 0.00873, Accuracy: 99.952%\n",
      "Epoch 140: Loss: 0.00864, Accuracy: 99.955%\n",
      "Epoch 141: Loss: 0.00856, Accuracy: 99.957%\n",
      "Epoch 142: Loss: 0.00847, Accuracy: 99.957%\n",
      "Epoch 143: Loss: 0.00839, Accuracy: 99.957%\n",
      "Epoch 144: Loss: 0.00831, Accuracy: 99.958%\n",
      "Epoch 145: Loss: 0.00823, Accuracy: 99.958%\n",
      "Epoch 146: Loss: 0.00815, Accuracy: 99.962%\n",
      "Epoch 147: Loss: 0.00807, Accuracy: 99.962%\n",
      "Epoch 148: Loss: 0.00800, Accuracy: 99.962%\n",
      "Epoch 149: Loss: 0.00792, Accuracy: 99.962%\n",
      "Epoch 150: Loss: 0.00785, Accuracy: 99.962%\n",
      "Epoch 151: Loss: 0.00777, Accuracy: 99.962%\n",
      "Epoch 152: Loss: 0.00770, Accuracy: 99.963%\n",
      "Epoch 153: Loss: 0.00763, Accuracy: 99.963%\n",
      "Epoch 154: Loss: 0.00756, Accuracy: 99.963%\n",
      "Epoch 155: Loss: 0.00749, Accuracy: 99.963%\n",
      "Epoch 156: Loss: 0.00742, Accuracy: 99.963%\n",
      "Epoch 157: Loss: 0.00735, Accuracy: 99.965%\n",
      "Epoch 158: Loss: 0.00729, Accuracy: 99.965%\n",
      "Epoch 159: Loss: 0.00722, Accuracy: 99.967%\n",
      "Epoch 160: Loss: 0.00716, Accuracy: 99.968%\n",
      "Epoch 161: Loss: 0.00709, Accuracy: 99.968%\n",
      "Epoch 162: Loss: 0.00703, Accuracy: 99.970%\n",
      "Epoch 163: Loss: 0.00697, Accuracy: 99.970%\n",
      "Epoch 164: Loss: 0.00691, Accuracy: 99.970%\n",
      "Epoch 165: Loss: 0.00684, Accuracy: 99.972%\n",
      "Epoch 166: Loss: 0.00678, Accuracy: 99.972%\n",
      "Epoch 167: Loss: 0.00673, Accuracy: 99.972%\n",
      "Epoch 168: Loss: 0.00667, Accuracy: 99.972%\n",
      "Epoch 169: Loss: 0.00661, Accuracy: 99.972%\n",
      "Epoch 170: Loss: 0.00655, Accuracy: 99.972%\n",
      "Epoch 171: Loss: 0.00650, Accuracy: 99.973%\n",
      "Epoch 172: Loss: 0.00644, Accuracy: 99.973%\n",
      "Epoch 173: Loss: 0.00639, Accuracy: 99.973%\n",
      "Epoch 174: Loss: 0.00633, Accuracy: 99.973%\n",
      "Epoch 175: Loss: 0.00628, Accuracy: 99.973%\n",
      "Epoch 176: Loss: 0.00623, Accuracy: 99.973%\n",
      "Epoch 177: Loss: 0.00618, Accuracy: 99.973%\n",
      "Epoch 178: Loss: 0.00613, Accuracy: 99.975%\n",
      "Epoch 179: Loss: 0.00607, Accuracy: 99.975%\n",
      "Epoch 180: Loss: 0.00602, Accuracy: 99.975%\n",
      "Epoch 181: Loss: 0.00598, Accuracy: 99.977%\n",
      "Epoch 182: Loss: 0.00593, Accuracy: 99.978%\n",
      "Epoch 183: Loss: 0.00588, Accuracy: 99.978%\n",
      "Epoch 184: Loss: 0.00583, Accuracy: 99.978%\n",
      "Epoch 185: Loss: 0.00579, Accuracy: 99.978%\n",
      "Epoch 186: Loss: 0.00574, Accuracy: 99.978%\n",
      "Epoch 187: Loss: 0.00569, Accuracy: 99.978%\n",
      "Epoch 188: Loss: 0.00565, Accuracy: 99.978%\n",
      "Epoch 189: Loss: 0.00560, Accuracy: 99.978%\n",
      "Epoch 190: Loss: 0.00556, Accuracy: 99.978%\n",
      "Epoch 191: Loss: 0.00552, Accuracy: 99.978%\n",
      "Epoch 192: Loss: 0.00547, Accuracy: 99.978%\n",
      "Epoch 193: Loss: 0.00543, Accuracy: 99.978%\n",
      "Epoch 194: Loss: 0.00539, Accuracy: 99.978%\n",
      "Epoch 195: Loss: 0.00535, Accuracy: 99.978%\n",
      "Epoch 196: Loss: 0.00531, Accuracy: 99.980%\n",
      "Epoch 197: Loss: 0.00527, Accuracy: 99.982%\n",
      "Epoch 198: Loss: 0.00523, Accuracy: 99.982%\n",
      "Epoch 199: Loss: 0.00519, Accuracy: 99.983%\n",
      "Epoch 200: Loss: 0.00515, Accuracy: 99.983%\n",
      "RMSProp\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5a60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b82a5a60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f0168470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.39586, Accuracy: 92.120%\n",
      "Epoch 002: Loss: 0.18904, Accuracy: 96.552%\n",
      "Epoch 003: Loss: 0.13712, Accuracy: 97.815%\n",
      "Epoch 004: Loss: 0.10686, Accuracy: 98.487%\n",
      "Epoch 005: Loss: 0.08635, Accuracy: 98.967%\n",
      "Epoch 006: Loss: 0.07133, Accuracy: 99.255%\n",
      "Epoch 007: Loss: 0.05979, Accuracy: 99.458%\n",
      "Epoch 008: Loss: 0.05064, Accuracy: 99.598%\n",
      "Epoch 009: Loss: 0.04320, Accuracy: 99.722%\n",
      "Epoch 010: Loss: 0.03700, Accuracy: 99.792%\n",
      "Epoch 011: Loss: 0.03178, Accuracy: 99.852%\n",
      "Epoch 012: Loss: 0.02734, Accuracy: 99.897%\n",
      "Epoch 013: Loss: 0.02356, Accuracy: 99.925%\n",
      "Epoch 014: Loss: 0.02032, Accuracy: 99.942%\n",
      "Epoch 015: Loss: 0.01757, Accuracy: 99.958%\n",
      "Epoch 016: Loss: 0.01522, Accuracy: 99.972%\n",
      "Epoch 017: Loss: 0.01322, Accuracy: 99.980%\n",
      "Epoch 018: Loss: 0.01153, Accuracy: 99.983%\n",
      "Epoch 019: Loss: 0.01011, Accuracy: 99.983%\n",
      "Epoch 020: Loss: 0.00891, Accuracy: 99.988%\n",
      "Epoch 021: Loss: 0.00789, Accuracy: 99.988%\n",
      "Epoch 022: Loss: 0.00703, Accuracy: 99.988%\n",
      "Epoch 023: Loss: 0.00629, Accuracy: 99.992%\n",
      "Epoch 024: Loss: 0.00566, Accuracy: 99.992%\n",
      "Epoch 025: Loss: 0.00512, Accuracy: 99.998%\n",
      "Epoch 026: Loss: 0.00466, Accuracy: 99.998%\n",
      "Epoch 027: Loss: 0.00425, Accuracy: 99.998%\n",
      "Epoch 028: Loss: 0.00389, Accuracy: 99.998%\n",
      "Epoch 029: Loss: 0.00358, Accuracy: 99.998%\n",
      "Epoch 030: Loss: 0.00329, Accuracy: 100.000%\n",
      "Epoch 031: Loss: 0.00304, Accuracy: 100.000%\n",
      "Epoch 032: Loss: 0.00283, Accuracy: 100.000%\n",
      "Epoch 033: Loss: 0.00264, Accuracy: 100.000%\n",
      "Epoch 034: Loss: 0.00247, Accuracy: 100.000%\n",
      "Epoch 035: Loss: 0.00232, Accuracy: 100.000%\n",
      "Epoch 036: Loss: 0.00218, Accuracy: 100.000%\n",
      "Epoch 037: Loss: 0.00206, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00195, Accuracy: 100.000%\n",
      "Epoch 039: Loss: 0.00185, Accuracy: 100.000%\n",
      "Epoch 040: Loss: 0.00176, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 041: Loss: 0.00168, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00160, Accuracy: 100.000%\n",
      "Epoch 043: Loss: 0.00153, Accuracy: 100.000%\n",
      "Epoch 044: Loss: 0.00146, Accuracy: 100.000%\n",
      "Epoch 045: Loss: 0.00140, Accuracy: 100.000%\n",
      "Epoch 046: Loss: 0.00134, Accuracy: 100.000%\n",
      "Epoch 047: Loss: 0.00129, Accuracy: 100.000%\n",
      "Epoch 048: Loss: 0.00124, Accuracy: 100.000%\n",
      "Epoch 049: Loss: 0.00120, Accuracy: 100.000%\n",
      "Epoch 050: Loss: 0.00115, Accuracy: 100.000%\n",
      "Epoch 051: Loss: 0.00111, Accuracy: 100.000%\n",
      "Epoch 052: Loss: 0.00107, Accuracy: 100.000%\n",
      "Epoch 053: Loss: 0.00104, Accuracy: 100.000%\n",
      "Epoch 054: Loss: 0.00100, Accuracy: 100.000%\n",
      "Epoch 055: Loss: 0.00097, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00094, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00091, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00089, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00086, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00084, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00081, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00079, Accuracy: 100.000%\n",
      "Epoch 063: Loss: 0.00077, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00075, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00073, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00071, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00070, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00068, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00066, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00065, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00063, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00062, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00061, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00059, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00058, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00057, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00056, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00055, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00053, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00052, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00051, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00050, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00050, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00049, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00048, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00047, Accuracy: 100.000%\n",
      "Epoch 087: Loss: 0.00046, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00045, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00044, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00044, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00043, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00042, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00042, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00041, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00040, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00040, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00039, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00038, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00038, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00037, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00037, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00036, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00035, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 107: Loss: 0.00034, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00033, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00032, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00031, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00030, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00029, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00028, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00027, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00026, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00025, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00024, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00023, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00022, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 150: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00021, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00020, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00019, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00018, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 172: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00017, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 186: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00015, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00014, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00014, Accuracy: 100.000%\n",
      "Adam\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d90>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd3b86d9d90>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function calculate_grad at 0x7fd3b86740d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calculate_grad at 0x7fd3b86740d0>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method return_Model.<locals>.Model_MNIST.call of <__main__.return_Model.<locals>.Model_MNIST object at 0x7fd3f03e3898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 001: Loss: 0.17137, Accuracy: 95.912%\n",
      "Epoch 002: Loss: 0.05718, Accuracy: 98.697%\n",
      "Epoch 003: Loss: 0.03215, Accuracy: 99.413%\n",
      "Epoch 004: Loss: 0.01938, Accuracy: 99.732%\n",
      "Epoch 005: Loss: 0.01117, Accuracy: 99.892%\n",
      "Epoch 006: Loss: 0.00715, Accuracy: 99.950%\n",
      "Epoch 007: Loss: 0.00520, Accuracy: 99.982%\n",
      "Epoch 008: Loss: 0.00549, Accuracy: 99.988%\n",
      "Epoch 009: Loss: 0.00736, Accuracy: 99.980%\n",
      "Epoch 010: Loss: 0.00602, Accuracy: 99.988%\n",
      "Epoch 011: Loss: 0.00713, Accuracy: 99.977%\n",
      "Epoch 012: Loss: 0.00402, Accuracy: 99.990%\n",
      "Epoch 013: Loss: 0.00436, Accuracy: 99.985%\n",
      "Epoch 014: Loss: 0.00312, Accuracy: 99.997%\n",
      "Epoch 015: Loss: 0.00633, Accuracy: 99.975%\n",
      "Epoch 016: Loss: 0.00278, Accuracy: 99.992%\n",
      "Epoch 017: Loss: 0.00439, Accuracy: 99.988%\n",
      "Epoch 018: Loss: 0.00412, Accuracy: 99.988%\n",
      "Epoch 019: Loss: 0.00152, Accuracy: 99.995%\n",
      "Epoch 020: Loss: 0.00016, Accuracy: 100.000%\n",
      "Epoch 021: Loss: 0.00006, Accuracy: 100.000%\n",
      "Epoch 022: Loss: 0.00004, Accuracy: 100.000%\n",
      "Epoch 023: Loss: 0.00003, Accuracy: 100.000%\n",
      "Epoch 024: Loss: 0.00002, Accuracy: 100.000%\n",
      "Epoch 025: Loss: 0.00002, Accuracy: 100.000%\n",
      "Epoch 026: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 027: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 028: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 029: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 030: Loss: 0.00001, Accuracy: 100.000%\n",
      "Epoch 031: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 032: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 033: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 034: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 035: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 036: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 037: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 038: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 039: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 040: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 041: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 042: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 043: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 044: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 045: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 046: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 047: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 048: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 049: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 050: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 051: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 052: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 053: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 054: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 055: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 056: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 057: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 058: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 059: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 060: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 061: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 062: Loss: 0.00000, Accuracy: 100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 063: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 064: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 065: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 066: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 067: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 068: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 069: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 070: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 071: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 072: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 073: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 074: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 075: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 076: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 077: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 078: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 079: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 080: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 081: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 082: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 083: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 084: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 085: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 086: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 087: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 088: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 089: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 090: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 091: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 092: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 093: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 094: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 095: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 096: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 097: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 098: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 099: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 100: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 101: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 102: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 103: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 104: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 105: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 106: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 107: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 108: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 109: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 110: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 111: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 112: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 113: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 114: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 115: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 116: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 117: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 118: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 119: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 120: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 121: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 122: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 123: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 124: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 125: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 126: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 127: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 128: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 129: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 130: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 131: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 132: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 133: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 134: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 135: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 136: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 137: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 138: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 139: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 140: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 141: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 142: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 143: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 144: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 145: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 146: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 147: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 148: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 149: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 150: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 151: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 152: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 153: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 154: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 155: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 156: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 157: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 158: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 159: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 160: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 161: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 162: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 163: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 164: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 165: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 166: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 167: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 168: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 169: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 170: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 171: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 172: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 173: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 174: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 175: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 176: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 177: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 178: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 179: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 180: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 181: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 182: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 183: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 184: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 185: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 186: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 187: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 188: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 189: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 190: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 191: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 192: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 193: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 194: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 195: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 196: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 197: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 198: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 199: Loss: 0.00000, Accuracy: 100.000%\n",
      "Epoch 200: Loss: 0.00000, Accuracy: 100.000%\n"
     ]
    }
   ],
   "source": [
    "n_trials = 5\n",
    "\n",
    "# Number of epochs & batch size have been taken as 200 and 128 respectively, as given in the paper.\n",
    "n_epochs = 200\n",
    "batch_size = 128\n",
    "# Since learning rate was not mention, it was observed, that 1e-3 is a good choice, and was taken.\n",
    "lr = 1e-3\n",
    "\n",
    "loss_sgdNesterov = numpy.zeros((n_trials,n_epochs))\n",
    "accuracy_sgdNesterov = numpy.zeros((n_trials,n_epochs))\n",
    "\n",
    "loss_adagrad = numpy.zeros((n_trials,n_epochs))\n",
    "accuracy_adagrad = numpy.zeros((n_trials,n_epochs))\n",
    "\n",
    "loss_rmsprop = numpy.zeros((n_trials,n_epochs))\n",
    "accuracy_rmsprop = numpy.zeros((n_trials,n_epochs))\n",
    "\n",
    "loss_adam = numpy.zeros((n_trials,n_epochs))\n",
    "accuracy_adam = numpy.zeros((n_trials,n_epochs))\n",
    "\n",
    "# The train dataset iterator was created.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(batch_size)\n",
    "# The loss was defined.\n",
    "loss_criterion = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# The optimizers were declared.\n",
    "optimizerObj_SGDNesterov = MySGDNesterov(learning_rate=lr)\n",
    "optimizerObj_Adagrad = MyAdagrad(learning_rate=lr)\n",
    "optimizerObj_RMSProp = MyRMSProp(learning_rate=lr, beta = 0.95)\n",
    "optimizerObj_Adam = MyAdam(learning_rate=lr, gamma=0.9, beta = 0.999)\n",
    "\n",
    "# Performance of each optimizer was observed for each trial\n",
    "for i in range(n_trials):\n",
    "  \n",
    "  print('Trial ' + str(i+1))\n",
    "\n",
    "  #SGDNesterov\n",
    "  print('SGD Nesterov')\n",
    "  model = return_Model()\n",
    "  global_step = tf.Variable(0)\n",
    "  loss_per_epoch, accuracy_per_epoch = train_model(model,optimizerObj_SGDNesterov,train_dataset,loss_criterion,global_step,n_epochs,batch_size)\n",
    "  loss_sgdNesterov[i] = numpy.array(loss_per_epoch)\n",
    "  accuracy_sgdNesterov[i] = numpy.array(accuracy_per_epoch)\n",
    "\n",
    "  #Adagrad\n",
    "  print('Adagrad')\n",
    "  model = return_Model()\n",
    "  global_step = tf.Variable(0)\n",
    "  loss_per_epoch, accuracy_per_epoch = train_model(model,optimizerObj_Adagrad,train_dataset,loss_criterion,global_step,n_epochs,batch_size)\n",
    "  loss_adagrad[i] = numpy.array(loss_per_epoch)\n",
    "  accuracy_adagrad[i] = numpy.array(accuracy_per_epoch)\n",
    "\n",
    "  #RMSProp\n",
    "  print('RMSProp')\n",
    "  model = return_Model()\n",
    "  global_step = tf.Variable(0)\n",
    "  loss_per_epoch, accuracy_per_epoch = train_model(model,optimizerObj_RMSProp,train_dataset,loss_criterion,global_step,n_epochs,batch_size)\n",
    "  loss_rmsprop[i] = numpy.array(loss_per_epoch)\n",
    "  accuracy_rmsprop[i] = numpy.array(accuracy_per_epoch)\n",
    "\n",
    "  #Adam\n",
    "  print('Adam')\n",
    "  model = return_Model()\n",
    "  global_step = tf.Variable(0)\n",
    "  loss_per_epoch, accuracy_per_epoch = train_model(model,optimizerObj_Adam,train_dataset,loss_criterion,global_step,n_epochs,batch_size)\n",
    "  loss_adam[i] = numpy.array(loss_per_epoch)\n",
    "  accuracy_adam[i] = numpy.array(accuracy_per_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SepRVRE3aVgZ"
   },
   "source": [
    "**After gathering the training losses, it is time to plot. The losses per epoch for all the trials were averaged first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIgBw9mqhsNL"
   },
   "outputs": [],
   "source": [
    "loss_sgdNesterov = numpy.mean(loss_sgdNesterov,axis=0)\n",
    "accuracy_sgdNesterov = numpy.mean(accuracy_sgdNesterov,axis=0)\n",
    "\n",
    "loss_adagrad = numpy.mean(loss_adagrad,axis=0)\n",
    "accuracy_adagrad = numpy.mean(accuracy_adagrad,axis=0)\n",
    "\n",
    "loss_rmsprop = numpy.mean(loss_rmsprop,axis=0)\n",
    "accuracy_rmsprop = numpy.mean(accuracy_rmsprop,axis=0)\n",
    "\n",
    "loss_adam = numpy.mean(loss_adam,axis=0)\n",
    "accuracy_adam = numpy.mean(accuracy_adam,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIvVEcclarE8"
   },
   "source": [
    "**Plot for loss for different optimizers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nf2JlG2Cijbb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis = numpy.array(list(range(1,n_epochs+1)))\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.plot(x_axis, loss_sgdNesterov, color='red', label='SGD Nesterov')\n",
    "plt.plot(x_axis, loss_adagrad, color='blue', label='AdaGrad')\n",
    "plt.plot(x_axis, loss_rmsprop, color='green', label='RMSProp')\n",
    "plt.plot(x_axis, loss_adam, color='pink', label='Adam')\n",
    "plt.legend(loc=1)\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-8,0.1)\n",
    "plt.xlabel('Iterations over entire dataset')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qj0_a5gXaz6z"
   },
   "source": [
    "**Plot for accuracies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yz0fS7FMy_aU"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = numpy.array(list(range(1,n_epochs+1)))\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.plot(x_axis, accuracy_sgdNesterov, color='red', label='SGD Nesterov')\n",
    "plt.plot(x_axis, accuracy_adagrad, color='blue', label='AdaGrad')\n",
    "plt.plot(x_axis, accuracy_rmsprop, color='green', label='RMSProp')\n",
    "plt.plot(x_axis, accuracy_adam, color='pink', label='Adam')\n",
    "plt.legend(loc=1)\n",
    "plt.ylim(0.999,1)\n",
    "plt.xlabel('Iterations over entire dataset')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pq3fX_ka6r_"
   },
   "source": [
    "# **The results and findings are discussed in the PDF.**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Part2_MNIST_Dataset.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
